content/blog/longhorn-v1.1.0.md:41:Currently, the RWX feature is still considered experimented in Longhorn. One thing we're still working on is ensuring better availability. At the moment, if the server that is running the NFS server for the RWX is down, we will restart the server. Based on the user's setting of [Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly](https://longhorn.io/docs/1.1.0/references/settings/#automatically-delete-workload-pod-when-the-volume-is-detached-unexpectedly), we’ll delete the workload pods that are using the volume to trigger the pod remount process. It will incur workload downtime during the process. We're working on a better way to reduce the downtime in the node failure cases.
content/blog/longhorn-v1.1.0.md:72:Once you’ve installed the Prometheus operator, simply add the [ServiceMonitor YAML](https://longhorn.io/docs/1.1.0/monitoring/prometheus_and_grafana_setup/#install-longhorn-servicemonitor) to enable Prometheus support in Longhorn. If you're using Grafana, you can import the [Longhorn v1.1.0 example dashboard](https://grafana.com/grafana/dashboards/13032) to get a quick overview of the metrics exported by Longhorn.
content/blog/longhorn-v1.1.0.md:87:As you may know, Kubernetes has a few limitations regarding the node down with stateful workload, as we explained [here](https://longhorn.io/docs/1.1.0/high-availability/node-failure/#what-to-expect-when-a-kubernetes-node-fails). This results in the stateful workload Pods not being recreated successfully on other nodes in the cluster (which is different from the stateless workload case). Manual intervention is required in those cases.
content/blog/longhorn-v1.1.0.md:91:Now in Longhorn v1.1, we've moved one step further to allow automatic recovery for both StatefulSet and Deployment Pods if they're using Longhorn volumes. Based on the setting [Pod Deletion Policy When the Node Is Down](https://longhorn.io/docs/1.1.0/references/settings/#pod-deletion-policy-when-node-is-down), Longhorn can automatically detect the node down scenario and delete the affected Pod to trigger the Kubernetes recreate mechanism.
content/blog/longhorn-v1.1.0.md:100:In Longhorn v1.1, we've introduced a way of rebuilding with existing replicas. The new [Replica Replenish Wait Interval](https://longhorn.io/docs/1.1.0/references/settings/#replica-replenishment-wait-interval) setting specifies how long Longhorn will wait before trying to fully rebuild a new replica instead of reusing the existing replica for rebuilding. Also, if we failed to rebuild using an existing replica three times, we will give up this replica and continue with other existing replicas or fully rebuild a new replica. This process will cut the storage consumption as well as speed up the rebuilding process.
content/blog/longhorn-v1.1.0.md:115:Starting with Longhorn v1.1, we recommend reserving 25 percent of your CPU resources on the node for Longhorn engines. That means in the production environment, we recommend setting the [Guaranteed Engine CPU](https://longhorn.io/docs/1.1.0/references/settings/#guaranteed-engine-cpu) option to 12.5 percent of your total CPU numbers on the node. It's half of 25 percent since the setting is applied to both Engine Instance Manager and Replica Instance Manager on the node. See the updated [Best Practice Guide](https://longhorn.io/docs/1.1.0/best-practices) for details. 
content/blog/longhorn-v1.1.0.md:132:3. The [Disable Replica Rebuild](https://longhorn.io/docs/1.1.0/references/settings/#disable-replica-rebuild) setting stops replica rebuild cluster-wide for maintenance.
content/blog/longhorn-v1.1.0.md:134:    1. Longhorn will stop the Drain process if it contains the last healthy replica of any volume by default to prevent unintentionally service outage during the Drain process. The behavior can be changed by [Allow Node Drain with the Last Healthy Replica](https://longhorn.io/docs/1.1.0/references/settings/#allow-node-drain-with-the-last-healthy-replica) setting.
content/docs/1.4.0/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.4.0/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.4.0/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.0/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.0/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.4.0/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.0/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:113:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:114:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.0/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.4.0/deploy/important-notes/index.md:15:After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/1.4.0/deploy/important-notes/index.md:22:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.0/deploy/important-notes/index.md:62:### Add StorageClass Parameter `mkfsParams` and [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) Setting Deprecated
content/docs/1.4.0/deploy/important-notes/index.md:64:The [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) will be deprecated and replaced by a new parameter `mkfsParams` as a per-StorageClass mkfs option (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`).
content/docs/1.4.0/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.4.0/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.0/deploy/install/install-with-helm.md:13:  - 2. If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/1.4.0/deploy/install/install-with-helm.md:21:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.0/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.0/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.4.0/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.0/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.0/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.4.0/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.4.0/deploy/upgrade/longhorn-manager.md:97:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/1.4.0/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.4.0/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.4.0/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.4.0/references/settings.md:378:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.4.0/references/settings.md:494:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.4.0/references/settings.md:504:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/1.4.0/references/settings.md:658:This deprecated setting is replaced by the new setting [`mkfsParams` in the StorageClass](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) and planned removed from v1.5.0.
content/docs/1.4.0/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.0/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.0/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.0/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.0/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.0/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.0/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.4.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.4.0/terminology.md:69:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.4.0/terminology.md:182:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.4.0/terminology.md:188:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.4.0/terminology.md:202:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.4.0/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/1.4.0/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/1.4.0/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.4.0/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.4.0/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.4.0/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.4.0/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.4.0/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.4.0/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.4.0/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.4.0/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:40:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.4.0/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.4.0/best-practices.md:83:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.4.0/best-practices.md:113:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/1.4.0/best-practices.md:117:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.4.0/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.4.0/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.4.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.4.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.4.1/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.4.1/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.4.1/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.1/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.1/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.4.1/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.1/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.1/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.4.1/deploy/important-notes/index.md:15:After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/1.4.1/deploy/important-notes/index.md:22:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.1/deploy/important-notes/index.md:62:### Add StorageClass Parameter `mkfsParams` and [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) Setting Deprecated
content/docs/1.4.1/deploy/important-notes/index.md:64:The [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) will be deprecated and replaced by a new parameter `mkfsParams` as a per-StorageClass mkfs option (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`).
content/docs/1.4.1/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.4.1/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.4.1/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.1/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.1/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.4.1/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.1/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.1/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.4.1/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.4.1/deploy/upgrade/longhorn-manager.md:97:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/1.4.1/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.4.1/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.4.1/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.4.1/references/settings.md:378:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.4.1/references/settings.md:494:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.4.1/references/settings.md:504:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/1.4.1/references/settings.md:658:This deprecated setting is replaced by the new setting [`mkfsParams` in the StorageClass](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) and planned removed from v1.5.0.
content/docs/1.4.1/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.1/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.1/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.1/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.1/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.1/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.1/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.4.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.4.1/terminology.md:69:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.4.1/terminology.md:182:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.4.1/terminology.md:188:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.4.1/terminology.md:202:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.4.1/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/1.4.1/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/1.4.1/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.4.1/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.4.1/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.4.1/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.4.1/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.4.1/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.4.1/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.4.1/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.4.1/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:39:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.4.1/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.4.1/best-practices.md:83:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.4.1/best-practices.md:113:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/1.4.1/best-practices.md:117:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.4.1/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.4.1/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.4.1/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.4.1/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.4.2/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.4.2/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.4.2/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.2/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.2/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.4.2/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.2/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.2/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.4.2/deploy/important-notes/index.md:15:After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/1.4.2/deploy/important-notes/index.md:24:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.2/deploy/important-notes/index.md:64:### Add StorageClass Parameter `mkfsParams` and [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) Setting Deprecated
content/docs/1.4.2/deploy/important-notes/index.md:66:The [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) will be deprecated and replaced by a new parameter `mkfsParams` as a per-StorageClass mkfs option (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`).
content/docs/1.4.2/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.4.2/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.4.2/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.2/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.2/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.4.2/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.2/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.2/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.4.2/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.4.2/deploy/upgrade/longhorn-manager.md:97:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/1.4.2/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.4.2/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.4.2/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.4.2/references/settings.md:382:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.4.2/references/settings.md:498:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.4.2/references/settings.md:508:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/1.4.2/references/settings.md:662:This deprecated setting is replaced by the new setting [`mkfsParams` in the StorageClass](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) and planned removed from v1.5.0.
content/docs/1.4.2/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.2/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.2/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.2/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.2/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.2/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.2/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.4.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.4.2/terminology.md:69:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.4.2/terminology.md:182:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.4.2/terminology.md:188:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.4.2/terminology.md:202:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.4.2/volumes-and-nodes/maintenance.md:85:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.4.2/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/1.4.2/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.4.2/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.4.2/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.4.2/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.4.2/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.4.2/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.4.2/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.4.2/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.4.2/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:39:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.4.2/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.4.2/best-practices.md:83:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.4.2/best-practices.md:113:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/1.4.2/best-practices.md:117:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.4.2/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.4.2/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.4.2/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.4.2/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.4.3/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.4.3/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.4.3/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.3/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.3/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.4.3/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.3/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.3/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.4.3/deploy/important-notes/index.md:15:After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/1.4.3/deploy/important-notes/index.md:24:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.3/deploy/important-notes/index.md:64:### Add StorageClass Parameter `mkfsParams` and [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) Setting Deprecated
content/docs/1.4.3/deploy/important-notes/index.md:66:The [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) will be deprecated and replaced by a new parameter `mkfsParams` as a per-StorageClass mkfs option (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`).
content/docs/1.4.3/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.4.3/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.4.3/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.3/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.3/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.4.3/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.3/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.3/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.4.3/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.4.3/deploy/upgrade/longhorn-manager.md:97:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/1.4.3/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.4.3/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.4.3/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.4.3/references/settings.md:382:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.4.3/references/settings.md:498:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.4.3/references/settings.md:508:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/1.4.3/references/settings.md:662:This deprecated setting is replaced by the new setting [`mkfsParams` in the StorageClass](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) and planned removed from v1.5.0.
content/docs/1.4.3/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.3/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.3/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.3/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.3/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.3/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.3/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.4.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.4.3/terminology.md:69:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.4.3/terminology.md:182:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.4.3/terminology.md:188:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.4.3/terminology.md:202:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.4.3/volumes-and-nodes/maintenance.md:85:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.4.3/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/1.4.3/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.4.3/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.4.3/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.4.3/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.4.3/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.4.3/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.4.3/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.4.3/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.4.3/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:39:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.4.3/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.4.3/best-practices.md:83:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.4.3/best-practices.md:113:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/1.4.3/best-practices.md:117:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.4.3/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.4.3/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.4.3/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.4.3/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.4.4/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.4.4/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.4.4/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.4/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.4/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.4.4/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.4/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.4/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.4.4/deploy/important-notes/index.md:15:After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/1.4.4/deploy/important-notes/index.md:24:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.4/deploy/important-notes/index.md:64:### Add StorageClass Parameter `mkfsParams` and [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) Setting Deprecated
content/docs/1.4.4/deploy/important-notes/index.md:66:The [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) will be deprecated and replaced by a new parameter `mkfsParams` as a per-StorageClass mkfs option (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`).
content/docs/1.4.4/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.4.4/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.4.4/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.4/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.4/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.4.4/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.4/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.4/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.4.4/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.4.4/deploy/upgrade/longhorn-manager.md:97:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/1.4.4/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.4.4/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.4.4/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.4.4/references/settings.md:382:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.4.4/references/settings.md:498:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.4.4/references/settings.md:508:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/1.4.4/references/settings.md:662:This deprecated setting is replaced by the new setting [`mkfsParams` in the StorageClass](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) and planned removed from v1.5.0.
content/docs/1.4.4/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.4/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.4/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.4/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.4/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.4/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.4/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.4.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.4.4/terminology.md:69:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.4.4/terminology.md:182:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.4.4/terminology.md:188:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.4.4/terminology.md:202:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.4.4/volumes-and-nodes/maintenance.md:85:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.4.4/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/1.4.4/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.4.4/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.4.4/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.4.4/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.4.4/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.4.4/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.4.4/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.4.4/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.4.4/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:39:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.4.4/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.4.4/best-practices.md:83:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.4.4/best-practices.md:113:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/1.4.4/best-practices.md:117:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.4.4/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.4.4/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.4.4/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.4.4/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.4.5/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.4.5/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.4.5/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.5/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.4.5/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.4.5/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.5/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.4.5/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.4.5/deploy/important-notes/index.md:15:After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/1.4.5/deploy/important-notes/index.md:24:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.5/deploy/important-notes/index.md:64:### Add StorageClass Parameter `mkfsParams` and [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) Setting Deprecated
content/docs/1.4.5/deploy/important-notes/index.md:66:The [`Custom mkfs.ext4 parameters`](../../references/settings/#custom-mkfsext4-parameters) will be deprecated and replaced by a new parameter `mkfsParams` as a per-StorageClass mkfs option (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`).
content/docs/1.4.5/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.4.5/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.4.5/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.5/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.5/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.4.5/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.4.5/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.4.5/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.4.5/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.4.5/deploy/upgrade/longhorn-manager.md:97:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/1.4.5/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.4.5/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.4.5/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.4.5/references/settings.md:383:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.4.5/references/settings.md:499:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.4.5/references/settings.md:509:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/1.4.5/references/settings.md:676:This deprecated setting is replaced by the new setting [`mkfsParams` in the StorageClass](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) and planned removed from v1.5.0.
content/docs/1.4.5/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.5/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.5/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.5/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.5/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.5/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.4.5/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.4.5/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.4.5/terminology.md:69:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.4.5/terminology.md:182:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.4.5/terminology.md:188:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.4.5/terminology.md:202:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.4.5/volumes-and-nodes/maintenance.md:85:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.4.5/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/1.4.5/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.4.5/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.4.5/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.4.5/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.4.5/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.4.5/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.4.5/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.4.5/volumes-and-nodes/volume-size.md:122:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.4.5/volumes-and-nodes/volume-size.md:137:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:39:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.4.5/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.4.5/best-practices.md:83:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.4.5/best-practices.md:113:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/1.4.5/best-practices.md:117:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.4.5/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.4.5/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.4.5/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.4.5/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.5.0/advanced-resources/backing-image.md:109:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.5.0/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.5.0/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.0/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.0/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.5.0/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.0/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.0/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.5.0/deploy/important-notes/index.md:34:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.0/deploy/important-notes/index.md:77:The `Custom mkfs.ext4 Parameters` setting was deprecated in Longhorn `v1.4.0` and is now removed. The per-StorageClass `mkfsParams` parameter should be used to specify mkfs options (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`) instead. See [Creating Longhorn Volumes with kubectl](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) for details.
content/docs/1.5.0/deploy/important-notes/index.md:81:The `Disable Replica Rebuild` setting was deprecated and replaced by the [Concurrent Replica Rebuild Per Node Limit](../../references/settings/#concurrent-replica-rebuild-per-node-limit) setting in Longhorn `v1.2.1`. It should already have been ignored in any Longhorn deployment upgrading to Longhorn v{{< current-version >}} and is now removed. To disable replica rebuilding across the cluster, set the `Concurrent Replica Rebuild Per Node Limit` to 0.
content/docs/1.5.0/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.5.0/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.5.0/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.0/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.0/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.5.0/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.0/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.0/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.5.0/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.5.0/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.5.0/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.5.0/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.5.0/references/settings.md:489:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.5.0/references/settings.md:629:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.5.0/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.0/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.0/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.0/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.0/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.0/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:262:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.0/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.5.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.5.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:28:- When the global setting [`Allow Volume Creation with Degraded Availability`](../../references/settings/#allow-volume-creation-with-degraded-availability) is enabled, the volume is degraded, indicating some replicas are unhealthy.
content/docs/1.5.0/terminology.md:71:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.5.0/terminology.md:184:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.5.0/terminology.md:190:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.5.0/terminology.md:204:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.5.0/volumes-and-nodes/maintenance.md:85:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.5.0/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/1.5.0/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.5.0/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.5.0/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.5.0/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.5.0/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.5.0/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.5.0/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.5.0/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.5.0/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:39:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.5.0/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.5.0/best-practices.md:84:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.5.0/best-practices.md:114:Refer to [Guaranteed Instance Manager CPU](../references/settings/#guaranteed-instance-manager-cpu) for more details.
content/docs/1.5.0/best-practices.md:118:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.5.0/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.5.0/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.5.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.5.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.5.1/advanced-resources/backing-image.md:109:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.5.1/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.5.1/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.1/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.1/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.5.1/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.1/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.1/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.5.1/deploy/important-notes/index.md:34:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.1/deploy/important-notes/index.md:77:The `Custom mkfs.ext4 Parameters` setting was deprecated in Longhorn `v1.4.0` and is now removed. The per-StorageClass `mkfsParams` parameter should be used to specify mkfs options (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`) instead. See [Creating Longhorn Volumes with kubectl](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) for details.
content/docs/1.5.1/deploy/important-notes/index.md:81:The `Disable Replica Rebuild` setting was deprecated and replaced by the [Concurrent Replica Rebuild Per Node Limit](../../references/settings/#concurrent-replica-rebuild-per-node-limit) setting in Longhorn `v1.2.1`. It should already have been ignored in any Longhorn deployment upgrading to Longhorn v{{< current-version >}} and is now removed. To disable replica rebuilding across the cluster, set the `Concurrent Replica Rebuild Per Node Limit` to 0.
content/docs/1.5.1/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.5.1/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.5.1/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.1/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.1/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.5.1/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.1/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.1/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.5.1/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.5.1/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.5.1/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.5.1/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.5.1/references/settings.md:489:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.5.1/references/settings.md:629:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.5.1/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.1/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.1/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.1/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.1/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.1/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:262:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.1/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.5.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.5.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:28:- When the global setting [`Allow Volume Creation with Degraded Availability`](../../references/settings/#allow-volume-creation-with-degraded-availability) is enabled, the volume is degraded, indicating some replicas are unhealthy.
content/docs/1.5.1/terminology.md:71:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.5.1/terminology.md:184:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.5.1/terminology.md:190:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.5.1/terminology.md:204:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.5.1/volumes-and-nodes/maintenance.md:85:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.5.1/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/1.5.1/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.5.1/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.5.1/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.5.1/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.5.1/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.5.1/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.5.1/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.5.1/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.5.1/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:57:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.5.1/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.5.1/best-practices.md:84:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.5.1/best-practices.md:114:Refer to [Guaranteed Instance Manager CPU](../references/settings/#guaranteed-instance-manager-cpu) for more details.
content/docs/1.5.1/best-practices.md:118:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.5.1/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.5.1/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.5.1/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.5.1/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.5.2/advanced-resources/backing-image.md:109:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.5.2/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.5.2/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.2/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.2/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.5.2/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.2/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.2/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.5.2/deploy/important-notes/index.md:45:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.2/deploy/important-notes/index.md:88:The `Custom mkfs.ext4 Parameters` setting was deprecated in Longhorn `v1.4.0` and is now removed. The per-StorageClass `mkfsParams` parameter should be used to specify mkfs options (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`) instead. See [Creating Longhorn Volumes with kubectl](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) for details.
content/docs/1.5.2/deploy/important-notes/index.md:92:The `Disable Replica Rebuild` setting was deprecated and replaced by the [Concurrent Replica Rebuild Per Node Limit](../../references/settings/#concurrent-replica-rebuild-per-node-limit) setting in Longhorn `v1.2.1`. It should already have been ignored in any Longhorn deployment upgrading to Longhorn v{{< current-version >}} and is now removed. To disable replica rebuilding across the cluster, set the `Concurrent Replica Rebuild Per Node Limit` to 0.
content/docs/1.5.2/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.5.2/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.5.2/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.2/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.2/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.5.2/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.2/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.2/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.5.2/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.5.2/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.5.2/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.5.2/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.5.2/references/settings.md:489:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.5.2/references/settings.md:629:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.5.2/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.2/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.2/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.2/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.2/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.2/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:262:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.2/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.5.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.5.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:28:- When the global setting [`Allow Volume Creation with Degraded Availability`](../../references/settings/#allow-volume-creation-with-degraded-availability) is enabled, the volume is degraded, indicating some replicas are unhealthy.
content/docs/1.5.2/terminology.md:71:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.5.2/terminology.md:184:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.5.2/terminology.md:190:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.5.2/terminology.md:204:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.5.2/volumes-and-nodes/maintenance.md:85:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.5.2/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/1.5.2/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.5.2/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.5.2/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.5.2/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.5.2/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.5.2/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.5.2/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.5.2/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.5.2/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:57:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.5.2/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.5.2/best-practices.md:84:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.5.2/best-practices.md:114:Refer to [Guaranteed Instance Manager CPU](../references/settings/#guaranteed-instance-manager-cpu) for more details.
content/docs/1.5.2/best-practices.md:118:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.5.2/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.5.2/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.5.2/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.5.2/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.5.3/advanced-resources/backing-image.md:109:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.5.3/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.5.3/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.3/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.3/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.5.3/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.3/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.3/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.5.3/deploy/important-notes/index.md:45:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.3/deploy/important-notes/index.md:88:The `Custom mkfs.ext4 Parameters` setting was deprecated in Longhorn `v1.4.0` and is now removed. The per-StorageClass `mkfsParams` parameter should be used to specify mkfs options (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`) instead. See [Creating Longhorn Volumes with kubectl](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) for details.
content/docs/1.5.3/deploy/important-notes/index.md:92:The `Disable Replica Rebuild` setting was deprecated and replaced by the [Concurrent Replica Rebuild Per Node Limit](../../references/settings/#concurrent-replica-rebuild-per-node-limit) setting in Longhorn `v1.2.1`. It should already have been ignored in any Longhorn deployment upgrading to Longhorn v{{< current-version >}} and is now removed. To disable replica rebuilding across the cluster, set the `Concurrent Replica Rebuild Per Node Limit` to 0.
content/docs/1.5.3/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.5.3/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.5.3/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.3/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.3/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.5.3/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.3/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.3/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.5.3/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.5.3/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.5.3/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.5.3/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.5.3/references/settings.md:489:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.5.3/references/settings.md:629:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.5.3/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.3/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.3/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.3/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.3/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.3/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:262:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.3/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.5.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.5.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:28:- When the global setting [`Allow Volume Creation with Degraded Availability`](../../references/settings/#allow-volume-creation-with-degraded-availability) is enabled, the volume is degraded, indicating some replicas are unhealthy.
content/docs/1.5.3/terminology.md:71:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.5.3/terminology.md:184:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.5.3/terminology.md:190:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.5.3/terminology.md:204:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.5.3/volumes-and-nodes/maintenance.md:85:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.5.3/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/1.5.3/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.5.3/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.5.3/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.5.3/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.5.3/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.5.3/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.5.3/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.5.3/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.5.3/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:53:You can set up a [RecurringJob](../../snapshots-and-backups/scheduling-backups-and-snapshots/#set-up-recurring-jobs) to periodically trim the filesystem.
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:61:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.5.3/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.5.3/best-practices.md:84:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.5.3/best-practices.md:114:Refer to [Guaranteed Instance Manager CPU](../references/settings/#guaranteed-instance-manager-cpu) for more details.
content/docs/1.5.3/best-practices.md:118:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.5.3/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.5.3/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.5.3/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.5.3/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.5.4/advanced-resources/backing-image.md:109:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.5.4/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.5.4/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.4/advanced-resources/deploy/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.5.4/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.5.4/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.4/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.5.4/advanced-resources/rwx-workloads.md:21:    Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.5.4/deploy/important-notes/index.md:45:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.4/deploy/important-notes/index.md:57:Recommendations](../../volumes-and-nodes/maintenance/#node-drain-policy-recommendations) section for help deciding which
content/docs/1.5.4/deploy/important-notes/index.md:104:The `Custom mkfs.ext4 Parameters` setting was deprecated in Longhorn `v1.4.0` and is now removed. The per-StorageClass `mkfsParams` parameter should be used to specify mkfs options (e.g., `-I 256 -b 4096 -O ^metadata_csum,^64bit`) instead. See [Creating Longhorn Volumes with kubectl](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) for details.
content/docs/1.5.4/deploy/important-notes/index.md:108:The `Disable Replica Rebuild` setting was deprecated and replaced by the [Concurrent Replica Rebuild Per Node Limit](../../references/settings/#concurrent-replica-rebuild-per-node-limit) setting in Longhorn `v1.2.1`. It should already have been ignored in any Longhorn deployment upgrading to Longhorn v{{< current-version >}} and is now removed. To disable replica rebuilding across the cluster, set the `Concurrent Replica Rebuild Per Node Limit` to 0.
content/docs/1.5.4/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.5.4/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.5.4/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.4/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.4/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.5.4/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.5.4/deploy/uninstall/_index.md:17:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.5.4/deploy/uninstall/_index.md:75:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.5.4/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/1.5.4/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.5.4/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.5.4/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:203:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.5.4/references/settings.md:106:Recommendations](../../volumes-and-nodes/maintenance/#node-drain-policy-recommendations) for help deciding which is most
content/docs/1.5.4/references/settings.md:503:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.5.4/references/settings.md:643:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/1.5.4/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.4/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.4/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.4/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.4/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.4/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:262:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.5.4/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.5.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.5.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:28:- When the global setting [`Allow Volume Creation with Degraded Availability`](../../references/settings/#allow-volume-creation-with-degraded-availability) is enabled, the volume is degraded, indicating some replicas are unhealthy.
content/docs/1.5.4/terminology.md:71:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.5.4/terminology.md:184:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.5.4/terminology.md:190:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.5.4/terminology.md:204:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.5.4/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/1.5.4/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.5.4/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.5.4/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.5.4/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/1.5.4/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.5.4/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.5.4/volumes-and-nodes/maintenance.md:99:guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.5.4/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.5.4/volumes-and-nodes/volume-size.md:122:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.5.4/volumes-and-nodes/volume-size.md:137:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:53:You can set up a [RecurringJob](../../snapshots-and-backups/scheduling-backups-and-snapshots/#set-up-recurring-jobs) to periodically trim the filesystem.
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:61:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.5.4/troubleshoot/troubleshooting.md:25:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.5.4/best-practices.md:84:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.5.4/best-practices.md:114:Refer to [Guaranteed Instance Manager CPU](../references/settings/#guaranteed-instance-manager-cpu) for more details.
content/docs/1.5.4/best-practices.md:118:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.5.4/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.5.4/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.5.4/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.5.4/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.6.0/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.6.0/advanced-resources/os-distro-specific/talos-linux-support.md:34:By default, Talos Linux applies a `baseline` pod security profile across namespaces, except for the kube-system namespace. This default setting restricts Longhorn's ability to manage and access system resources. For more information, see [Root and Privileged Permission](../../../deploy/install/#root-and-privileged-permission).
content/docs/1.6.0/advanced-resources/os-distro-specific/talos-linux-support.md:36:For detailed instructions, see [Pod Security Policies Disabled & Pod Security Admission Introduction](../../../deploy/important-notes/#pod-security-policies-disabled--pod-security-admission-introduction) and Talos' documentation on [Pod Security](https://www.talos.dev/v1.6/kubernetes-guides/configuration/pod-security/).
content/docs/1.6.0/advanced-resources/os-distro-specific/okd-support.md:29:To understand more about configuring the disks for Longhorn, please refer to the section [Configuring Defaults for Nodes and Disks](../../../nodes-and-volumes/nodes/default-disk-and-node-config/#launch-longhorn-with-multiple-disks)
content/docs/1.6.0/advanced-resources/os-distro-specific/okd-support.md:94:Please refer to the section [Customizing Default Disks for New Nodes](../../../nodes-and-volumes/nodes/default-disk-and-node-config/#customizing-default-disks-for-new-nodes) to label and annotate storage node on where your device is by oc commands:
content/docs/1.6.0/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.6.0/advanced-resources/security/volume-encryption.md:98:Longhorn supports [offline expansion](../../../nodes-and-volumes/volumes/expansion/#encrypted-volume) for encrypted volumes.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.6.0/advanced-resources/backing-image/backing-image.md:110:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.6.0/advanced-resources/cluster-restore/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.6.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stale replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../../nodes-and-volumes/volumes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.6.0/deploy/important-notes/index.md:67:Recommendations](../../maintenance/maintenance/#node-drain-policy-recommendations) section for help deciding which
content/docs/1.6.0/deploy/important-notes/index.md:121:> Whenever engine upgrade enforcement causes upgrade failure, Longhorn allows you to revert to the previous version because Longhorn Manager will block the entire upgrade. However, Longhorn prohibits downgrading when an upgrade is successful. For more information, see [Upgrade Path Enforcement](../../deploy/upgrade/#upgrade-path-enforcement-and-downgrade-prevention).
content/docs/1.6.0/deploy/important-notes/index.md:142:Starting with Longhorn v1.6.0, Longhorn allows you to modify the [Danger Zone settings](https://longhorn.io/docs/1.6.0/references/settings/#danger-zone) without the need to wait for all volumes to become detached. Your preferred settings are immediately applied in the following scenarios:
content/docs/1.6.0/deploy/important-notes/index.md:151:  | [Kubernetes Taint Toleration](../../references/settings/#kubernetes-taint-toleration)| [Taints and Tolerations](../../advanced-resources/deploy/taint-toleration/) | System-managed components |
content/docs/1.6.0/deploy/important-notes/index.md:152:  | [Priority Class](../../references/settings/#priority-class) | [Priority Class](../../advanced-resources/deploy/priority-class/) | System-managed components |
content/docs/1.6.0/deploy/important-notes/index.md:153:  | [System Managed Components Node Selector](../../references/settings/#system-managed-components-node-selector) | [Node Selector](../../advanced-resources/deploy/node-selector/) | System-managed components |
content/docs/1.6.0/deploy/important-notes/index.md:154:  | [Storage Network](../../references/settings/#storage-network) | [Storage Network](../../advanced-resources/deploy/storage-network/) | Instance Manager and Backing Image components |
content/docs/1.6.0/deploy/important-notes/index.md:155:  | [V1 Data Engine](../../references/settings/#v1-data-engine) || Instance Manager component |
content/docs/1.6.0/deploy/important-notes/index.md:156:  | [V2 Data Engine](../../references/settings/#v2-data-engine) | [V2 Data Engine (Preview Feature)](../../v2-data-engine/) | Instance Manager component |
content/docs/1.6.0/deploy/important-notes/index.md:157:  | [Guaranteed Instance Manager CPU](../../references/settings/#guaranteed-instance-manager-cpu) || Instance Manager component |
content/docs/1.6.0/deploy/important-notes/index.md:158:  | [Guaranteed Instance Manager CPU for V2 Data Engine](../../references/settings/#guaranteed-instance-manager-cpu-for-v2-data-engine) || Instance Manager component |
content/docs/1.6.0/deploy/install/_index.md:44:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.6.0/deploy/install/_index.md:231:- For Talos Linux, [the NFS client is part of the `kubelet` image maintained by the Talos team](https://www.talos.dev/v1.6/kubernetes-guides/configuration/storage/#nfs).
content/docs/1.6.0/deploy/install/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.6.0/deploy/install/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.6.0/deploy/install/install-with-argocd.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.6.0/deploy/install/install-with-fleet.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.6.0/deploy/install/install-with-flux.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.6.0/deploy/install/install-with-flux.md:10:  - [Install the Flux CLI](https://fluxcd.io/flux/installation/#install-the-flux-cli).
content/docs/1.6.0/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.6.0/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.6.0/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.6.0/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.6.0/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.6.0/deploy/uninstall/_index.md:20:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.6.0/deploy/uninstall/_index.md:78:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.6.0/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../install/airgap/#using-a-rancher-app)
content/docs/1.6.0/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.6.0/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.6.0/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:203:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.6.0/references/storage-class-parameters.md:52:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.6.0/references/storage-class-parameters.md:56:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.6.0/references/storage-class-parameters.md:60:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.6.0/references/storage-class-parameters.md:64:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.6.0/references/storage-class-parameters.md:68:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.6.0/references/settings.md:114:Recommendations](../../maintenance/maintenance/#node-drain-policy-recommendations) for help deciding which is most
content/docs/1.6.0/references/settings.md:555:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.6.0/references/settings.md:710:See [Multiple Disks Support](../../nodes-and-volumes/nodes/multidisk/#configuration) for details.
content/docs/1.6.0/references/settings.md:744:Starting with Longhorn v1.6.0, Longhorn allows you to modify the [Danger Zone settings](https://longhorn.io/docs/1.6.0/references/settings/#danger-zone) without the need to wait for all volumes to become detached. Your preferred settings are immediately applied in the following scenarios:
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:262:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.6.0/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.6.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.6.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:28:- When the global setting [`Allow Volume Creation with Degraded Availability`](../../references/settings/#allow-volume-creation-with-degraded-availability) is enabled, the volume is degraded, indicating some replicas are unhealthy.
content/docs/1.6.0/terminology.md:71:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.6.0/terminology.md:184:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.6.0/terminology.md:190:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.6.0/terminology.md:204:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.6.0/troubleshoot/troubleshooting.md:36:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.6.0/maintenance/maintenance.md:105:guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.6.0/nodes-and-volumes/nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:43:- [Disable Scheduling On Cordoned Node](../../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:44:- [Replica Soft Anti-Affinity](../../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:45:- [Replica Zone Level Soft Anti-Affinity](../../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:46:- [Replica Disk Level Soft Anti-Affinity](../../../references/settings/#replica-disk-level-soft-anti-affinity)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:47:- [Storage Minimal Available Percentage](../../../references/settings/#storage-minimal-available-percentage)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:48:- [Storage Over Provisioning Percentage](../../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:49:- [Allow Empty Node Selector Volume](../../../references/settings/#allow-empty-node-selector-volume)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:50:- [Allow Empty Disk Selector Volume](../../../references/settings/#allow-empty-disk-selector-volume)
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:55:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.6.0/nodes-and-volumes/nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:21:    Please refer to [Installing NFSv4 client](../../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:122:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:137:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../../references/settings/#automatically-clean-up-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:53:You can set up a [RecurringJob](../../../snapshots-and-backups/scheduling-backups-and-snapshots/#set-up-recurring-jobs) to periodically trim the filesystem.
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:61:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.6.0/best-practices.md:104:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../nodes-and-volumes/nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.6.0/best-practices.md:124:- **Storage network**: Use a [dedicated storage network](https://longhorn.io/docs/1.6.0/advanced-resources/deploy/storage-network/#setting-storage-network) to improve IO performance and stability.  
content/docs/1.6.0/best-practices.md:126:- **Longhorn disk**: Use a [dedicated disk](https://longhorn.io/docs/1.6.0/nodes-and-volumes/multidisk/#add-a-disk) for Longhorn storage instead of using the root disk.  
content/docs/1.6.0/best-practices.md:128:- **Replica count**: Set the [default replica count](https://longhorn.io/docs/1.6.0/references/settings/#default-replica-count) to "2" to achieve data availability with better disk space usage or less impact to system performance. This practice is especially beneficial to data-intensive applications.  
content/docs/1.6.0/best-practices.md:142:  For applications with replication capability, periodically [delete all types of snapshots](https://longhorn.io/docs/1.6.0/concepts/#243-deleting-snapshots).  
content/docs/1.6.0/best-practices.md:152:- **System backup**: Create periodic [system backups](https://longhorn.io/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system/#create-longhorn-system-backup).  
content/docs/1.6.0/best-practices.md:178:Refer to [Guaranteed Instance Manager CPU](../references/settings/#guaranteed-instance-manager-cpu) for more details.
content/docs/1.6.0/best-practices.md:186:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.6.0/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.6.0/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.6.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.6.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:11:  - CSI driver components images names and tags can be found [here.](../../../concepts/#13-csi-driver)
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:12:  - We recommend using a short registry URL due to a Kubernetes limitation on the length of pod metadata labels. For more information, refer to [this section](./#longhorn-instance-manager-metadatalabels-must-be-no-more-than-63-characters)
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:327:it's known Kubernetes limitation that label value should be no more than 63 characters [here](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set)
content/docs/archives/0.8.1/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/0.8.1/deploy/accessing-the-ui/longhorn-ingress.md:50:1. Create pre-requisite resources according to the [NGINX Ingress Controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/0.8.1/deploy/accessing-the-ui/longhorn-ingress.md:52:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/0.8.1/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/0.8.1/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/0.8.1/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/0.8.1/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/0.8.1/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/0.8.1/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/0.8.1/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/0.8.1/deploy/upgrade/_index.md:14:- To upgrade from v0.7.0+, see [this section.](./longhorn-manager/#upgrading-longhorn-manager-from-v070)
content/docs/archives/0.8.1/deploy/upgrade/_index.md:15:- To upgrade from v0.6.2 or older to v0.8.1, see [this section.](./longhorn-manager/#upgrading-from-v062-or-older-version-to-v081)
content/docs/archives/0.8.1/deploy/upgrade/_index.md:16:- To upgrade from v0.6.2 to v0.7.0, see [this section.](./longhorn-manager/#upgrading-longhorn-manager-from-v062-to-v070)
content/docs/archives/0.8.1/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details. 
content/docs/archives/0.8.1/high-availability/recover-volume.md:8:> This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/0.8.1/high-availability/recover-volume.md:83:- The above setup is not included when the related workload is launched. In this case, the [volume mount propagation](https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation) is not `Bidirectional`, and the Longhorn remount operation won't be propagated to the workload containers if the containers are not restarted.
content/docs/archives/0.8.1/references/settings.md:46:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/0.8.1/references/settings.md:128:The users can set this to a lower value if they don't want overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots would need space to store as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/0.8.1/references/settings.md:135:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:108:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/0.8.1/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time. 
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/0.8.1/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/0.8.1/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/0.8.1/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from being interfered by unexpected data R/W, Longhorn supports offline expansion only.  The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/0.8.1/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/0.8.1/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/0.8.1/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/0.8.1/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.0.0/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:11:  - CSI driver component's images, names and tags can be found [here.](../../../concepts/#13-csi-driver)
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:12:  - We recommend using a short registry URL due to a Kubernetes limitation on the length of pod metadata labels. For more information, refer to [this section](./#longhorn-instance-manager-metadatalabels-must-be-no-more-than-63-characters)
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:327:it's known Kubernetes limitation that label value should be no more than 63 characters [here](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set)
content/docs/archives/1.0.0/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.0.0/best-practices.md:53:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` directory. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-disk-on-the-node)
content/docs/archives/1.0.0/best-practices.md:75:We recommend allowing Longhorn Engine to have guaranteed CPU allocation. The value is how many CPUs should be reserved for each Engine/Replica Instance Manager Pod created by Longhorn. By default, the value is 0.25 CPUs. For details, refer to the [settings reference.](../references/settings/#guaranteed-engine-cpu)
content/docs/archives/1.0.0/best-practices.md:79:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:155:1. Create pre-requisite resources according to the [NGINX Ingress Controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:157:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.0.0/deploy/install/_index.md:25:    - By default Longhorn installation requires a three-nodes cluster since the default replica count is 3 and the [node level soft anti-affinity](https://longhorn.io/docs/1.0.0/references/settings/#replica-node-level-soft-anti-affinity) is disabled.
content/docs/archives/1.0.0/deploy/install/_index.md:37:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.0.0/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.0/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.0.0/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.0.0/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.0/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.0.0/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.0/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.0.0/deploy/upgrade/_index.md:14:- To upgrade from v0.8.1, see [this section.](./longhorn-manager/#upgrading-from-v081-to-v100)
content/docs/archives/1.0.0/deploy/upgrade/_index.md:15:- To upgrade from v0.7.0+, see [this section.](./longhorn-manager/#upgrading-from-v070) 
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:28:1. If you still have any volumes using the pre-v0.7.0 CSI driver name io.rancher.longhorn, follow the instructions [here](https://longhorn.io/docs/0.8.1/deploy/upgrade/longhorn-manager/#migrate-pvs-and-pvcs-for-the-volumes-launched-in-v062-or-older) to convert your old PVs.
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:33:1. Perform the engine upgrade according to the [offline engine upgrade instructions,](../upgrade-engine/#offline-upgrades) but don't scale back the workload just yet.
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:36:    > Please make sure you have at least 2 vCPUs per node before updating this setting to 0.25. See the [settings reference](../../../references/settings/#guaranteed-engine-cpu) for details.
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:38:1. We also recommend updating the **Replica Node Soft Anti-affinity** setting to false. Refer to the [settings reference](../../../references/settings/#replica-node-level-soft-anti-affinity) for details.
content/docs/archives/1.0.0/high-availability/node-failure.md:66:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.0.0/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.0/high-availability/recover-volume.md:83:- The above setup is not included when the related workload is launched. In this case, the [volume mount propagation](https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation) is not `Bidirectional`, and the Longhorn remount operation won't be propagated to the workload containers if the containers are not restarted.
content/docs/archives/1.0.0/references/settings.md:141:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.0.0/references/settings.md:160:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.0.0/references/settings.md:167:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:113:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.0.0/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.0.0/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.0.0/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.0.0/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from being interfered by unexpected data R/W, Longhorn supports offline expansion only.  The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:42:If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.0.0/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.0.0/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.0.0/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.0.0/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.0.0/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.0.0/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.0.0/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.0.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.0.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.0.1/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.0.1/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.0.1/best-practices.md:53:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` directory. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-disk-on-the-node)
content/docs/archives/1.0.1/best-practices.md:75:We recommend allowing Longhorn Engine to have guaranteed CPU allocation. The value is how many CPUs should be reserved for each Engine/Replica Instance Manager Pod created by Longhorn. By default, the value is 0.25 CPUs. For details, refer to the [settings reference.](../references/settings/#guaranteed-engine-cpu)
content/docs/archives/1.0.1/best-practices.md:79:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:155:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:157:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.0.1/deploy/install/_index.md:25:    - By default Longhorn installation requires a three-nodes cluster since the default replica count is 3 and the [node level soft anti-affinity](https://longhorn.io/docs/1.0.0/references/settings/#replica-node-level-soft-anti-affinity) is disabled.
content/docs/archives/1.0.1/deploy/install/_index.md:37:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.0.1/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.1/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.0.1/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.0.1/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.1/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.0.1/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.1/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.0.1/high-availability/node-failure.md:66:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.0.1/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.1/high-availability/recover-volume.md:83:- The above setup is not included when the related workload is launched. In this case, the [volume mount propagation](https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation) is not `Bidirectional`, and the Longhorn remount operation won't be propagated to the workload containers if the containers are not restarted.
content/docs/archives/1.0.1/references/settings.md:142:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.0.1/references/settings.md:161:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.0.1/references/settings.md:168:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:128:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.0.1/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.0.1/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.0.1/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.0.1/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from being interfered by unexpected data R/W, Longhorn supports offline expansion only.  The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:42:If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.0.1/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.0.1/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.0.1/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.0.1/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.0.1/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.0.1/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` is greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.0.1/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.0.1/concepts.md:145:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.0.1/concepts.md:362:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.0.1/concepts.md:416:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.0.2/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:199:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:285:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.0.2/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.0.2/best-practices.md:58:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-disk-on-the-node)
content/docs/archives/1.0.2/best-practices.md:80:We recommend allowing Longhorn Engine to have guaranteed CPU allocation. The value is how many CPUs should be reserved for each Engine/Replica Instance Manager Pod created by Longhorn. By default, the value is 0.25 CPUs. For details, refer to the [settings reference.](../references/settings/#guaranteed-engine-cpu)
content/docs/archives/1.0.2/best-practices.md:84:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:155:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:157:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.0.2/deploy/install/_index.md:25:    - By default Longhorn installation requires a three-nodes cluster since the default replica count is 3 and the [node level soft anti-affinity](https://longhorn.io/docs/1.0.0/references/settings/#replica-node-level-soft-anti-affinity) is disabled.
content/docs/archives/1.0.2/deploy/install/_index.md:39:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.0.2/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.2/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.0.2/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.0.2/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.2/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.0.2/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.0.2/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.0.2/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v1.0.1` is changed to `longhorn/longhorn-manager:v1.0.2` in Longhorn images section. For more information, see the air gap installation steps for v1.0.2 [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.0.2/high-availability/node-failure.md:70:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.0.2/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.2/high-availability/recover-volume.md:83:- The above setup is not included when the related workload is launched. In this case, the [volume mount propagation](https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation) is not `Bidirectional`, and the Longhorn remount operation won't be propagated to the workload containers if the containers are not restarted.
content/docs/archives/1.0.2/references/settings.md:142:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.0.2/references/settings.md:161:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.0.2/references/settings.md:168:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:132:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.0.2/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.0.2/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.0.2/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.0.2/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from being interfered by unexpected data R/W, Longhorn supports offline expansion only.  The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:42:If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.0.2/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.0.2/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.0.2/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.0.2/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.0.2/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.0.2/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` is greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.0.2/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.0.2/concepts.md:145:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.0.2/concepts.md:362:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.0.2/concepts.md:416:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.1.0/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.1.0/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:282:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.1.0/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.1.0/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:155:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:157:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.1.0/deploy/install/_index.md:39:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.1.0/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.0/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.1.0/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.1.0/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.0/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.1.0/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.0/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.1.0/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.1.0/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.1.0/high-availability/node-failure.md:57:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.1.0/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.1.0/references/settings.md:233:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.1.0/references/settings.md:258:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.1.0/references/settings.md:265:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.1.0/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.1.0/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.1.0/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.1.0/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.1.0/volumes-and-nodes/maintenance.md:54:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.1.0/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.1.0/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.1.0/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.1.0/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.1.0/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.1.0/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.1.0/best-practices.md:62:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.1.0/best-practices.md:94:For details, refer to the [settings reference.](../references/settings/#guaranteed-engine-cpu)
content/docs/archives/1.1.0/best-practices.md:98:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.1.0/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.1.0/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.1.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.1.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.1.1/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.1.1/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:282:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.1.1/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.1.1/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.1.1/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:161:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:163:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.1.1/deploy/install/_index.md:39:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.1.1/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.1/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.1.1/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.1.1/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.1/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.1.1/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.1/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.1.1/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.1.1/deploy/upgrade/longhorn-manager.md:86:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.1.1/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.1.1/high-availability/node-failure.md:57:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.1.1/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.1.1/references/settings.md:270:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.1.1/references/settings.md:314:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.1.1/references/settings.md:324:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.1.1/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.1.1/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.1.1/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.1.1/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.1.1/volumes-and-nodes/maintenance.md:54:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.1.1/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.1.1/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.1.1/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.1.1/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.1.1/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.1.1/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.1.1/best-practices.md:62:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.1.1/best-practices.md:92:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.1.1/best-practices.md:96:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.1.1/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.1.1/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.1.1/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.1.1/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.1.2/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.1.2/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:282:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.1.2/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.1.2/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.1.2/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:161:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:163:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.1.2/deploy/install/_index.md:39:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.1.2/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.2/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.1.2/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.1.2/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.2/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.1.2/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.2/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.1.2/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.1.2/deploy/upgrade/longhorn-manager.md:86:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.1.2/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.1.2/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.1.2/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.1.2/references/settings.md:259:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.1.2/references/settings.md:303:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.1.2/references/settings.md:313:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.1.2/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.1.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.1.2/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.1.2/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.1.2/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.1.2/volumes-and-nodes/maintenance.md:54:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.1.2/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.1.2/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.1.2/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.1.2/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.1.2/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.1.2/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.1.2/best-practices.md:62:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.1.2/best-practices.md:92:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.1.2/best-practices.md:96:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.1.2/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.1.2/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.1.2/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.1.2/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.1.3/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.1.3/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.1.3/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.1.3/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.1.3/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:161:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:163:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.1.3/deploy/install/_index.md:39:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.1.3/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.3/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.1.3/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.1.3/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.3/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.1.3/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.1.3/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.1.3/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.1.3/deploy/upgrade/longhorn-manager.md:86:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.1.3/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.1.3/high-availability/node-failure.md:57:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.1.3/monitoring/integrating-with-rancher-monitoring.md:36:You can then set up a [Grafana](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/tools/monitoring/viewing-metrics/#grafana) dashboard for visualization.
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.1.3/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.1.3/references/settings.md:259:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.1.3/references/settings.md:305:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.1.3/references/settings.md:315:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.1.3/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.1.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.1.3/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.1.3/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.1.3/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.1.3/volumes-and-nodes/maintenance.md:55:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.1.3/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/archives/1.1.3/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.1.3/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.1.3/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.1.3/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.1.3/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.1.3/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.1.3/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.1.3/best-practices.md:62:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.1.3/best-practices.md:92:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.1.3/best-practices.md:96:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.1.3/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.1.3/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.1.3/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.1.3/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.2.0/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.2.0/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.0/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.0/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.0/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.2.0/deploy/important-notes/index.md:13:   [[doc](https://longhorn.io/docs/1.2.0/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.2.0/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.2.0/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.0/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.2.0/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.2.0/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.0/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.2.0/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.0/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.0/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.2.0/deploy/upgrade/longhorn-manager.md:86:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.2.0/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.2.0/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.0/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.0/references/settings.md:260:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.2.0/references/settings.md:336:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.2.0/references/settings.md:346:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.0/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.2.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.2.0/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.2.0/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.2.0/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.2.0/volumes-and-nodes/maintenance.md:54:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.2.0/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.2.0/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.2.0/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.2.0/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.2.0/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.2.0/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.2.0/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.2.0/best-practices.md:62:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.2.0/best-practices.md:92:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.2.0/best-practices.md:96:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.2.0/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.2.0/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.2.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.2.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.2.1/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.2.1/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.1/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.1/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.1/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.2.1/deploy/important-notes/index.md:11:1. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/1.2.1/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.2.1/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.2.1/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.1/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.2.1/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.2.1/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.1/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.2.1/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.1/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.1/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.2.1/deploy/upgrade/longhorn-manager.md:86:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.2.1/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.2.1/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.1/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.1/references/settings.md:270:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.2.1/references/settings.md:346:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.2.1/references/settings.md:356:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.1/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.2.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.2.1/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.2.1/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.2.1/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.2.1/volumes-and-nodes/maintenance.md:54:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.2.1/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.2.1/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.2.1/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.2.1/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.2.1/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.2.1/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.2.1/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.2.1/best-practices.md:62:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.2.1/best-practices.md:92:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.2.1/best-practices.md:96:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.2.1/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.2.1/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.2.1/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.2.1/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.2.2/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.2.2/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.2/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.2/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.2/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.2.2/deploy/important-notes/index.md:11:1. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.2.2/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.2.2/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.2/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.2.2/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.2.2/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.2/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.2.2/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.2/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.2/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.2.2/deploy/upgrade/longhorn-manager.md:86:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.2.2/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.2.2/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.2/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.2/references/settings.md:271:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.2.2/references/settings.md:347:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.2.2/references/settings.md:357:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:    For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.2/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.2.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.2.2/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.2.2/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.2.2/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.2.2/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.2.2/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.2.2/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.2.2/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.2.2/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.2.2/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.2.2/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.2.2/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.2.2/best-practices.md:62:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.2.2/best-practices.md:92:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.2.2/best-practices.md:96:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.2.2/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.2.2/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.2.2/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.2.2/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.2.3/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.3/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.2.3/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.3/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.3/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.2.3/deploy/important-notes/index.md:11:1. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.2.3/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.2.3/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.3/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.2.3/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.2.3/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.3/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.2.3/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.3/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.3/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.2.3/deploy/upgrade/longhorn-manager.md:100:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.2.3/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.2.3/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.3/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.3/references/settings.md:271:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.2.3/references/settings.md:347:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.2.3/references/settings.md:357:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.3/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.2.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.2.3/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.2.3/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.2.3/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.2.3/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.2.3/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.2.3/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.2.3/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.2.3/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.2.3/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.2.3/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.2.3/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.2.3/best-practices.md:68:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.2.3/best-practices.md:98:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.2.3/best-practices.md:102:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.2.3/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.2.3/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.2.3/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.2.3/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.2.4/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.4/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.2.4/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.4/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.4/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.4/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.2.4/deploy/important-notes/index.md:11:2. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.2.4/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.2.4/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.4/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.2.4/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.2.4/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.4/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.2.4/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.4/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.4/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.2.4/deploy/upgrade/longhorn-manager.md:100:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.2.4/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.2.4/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.4/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.4/references/settings.md:271:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.2.4/references/settings.md:347:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.2.4/references/settings.md:357:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.4/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.2.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.2.4/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.2.4/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.2.4/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.2.4/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.2.4/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.2.4/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.2.4/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.2.4/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.2.4/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.2.4/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.2.4/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.2.4/best-practices.md:70:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.2.4/best-practices.md:100:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.2.4/best-practices.md:104:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.2.4/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.2.4/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.2.4/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.2.4/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.2.5/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.5/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.2.5/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.5/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.5/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.5/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.2.5/deploy/important-notes/index.md:11:1. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.2.5/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.2.5/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.5/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.2.5/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.2.5/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.5/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.2.5/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.5/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.5/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.2.5/deploy/upgrade/longhorn-manager.md:100:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.2.5/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.2.5/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.5/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.5/references/settings.md:272:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.2.5/references/settings.md:356:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.2.5/references/settings.md:366:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.5/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.2.5/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.2.5/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.2.5/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.2.5/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.2.5/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.2.5/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/archives/1.2.5/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.2.5/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.2.5/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.2.5/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.2.5/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.2.5/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.2.5/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/archives/1.2.5/best-practices.md:68:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.2.5/best-practices.md:98:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.2.5/best-practices.md:102:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.2.5/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.2.5/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.2.5/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.2.5/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.2.6/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.6/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.2.6/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.2.6/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.2.6/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.6/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.2.6/deploy/important-notes/index.md:11:1. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.2.6/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.2.6/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.6/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.2.6/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.2.6/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.6/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.2.6/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.2.6/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.2.6/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.2.6/deploy/upgrade/longhorn-manager.md:100:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.2.6/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.2.6/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:372:   For more information on how to define alert rules see [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.6/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.2.6/references/settings.md:272:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.2.6/references/settings.md:356:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.2.6/references/settings.md:366:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:7:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.2.6/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.2.6/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.2.6/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.2.6/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.2.6/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.2.6/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.2.6/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.2.6/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.2.6/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.2.6/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.2.6/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.2.6/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.2.6/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/archives/1.2.6/best-practices.md:68:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.2.6/best-practices.md:98:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.2.6/best-practices.md:102:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.2.6/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.2.6/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.2.6/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.2.6/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.3.0/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/archives/1.3.0/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.0/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.3.0/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.0/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/archives/1.3.0/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.0/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.0/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.0/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.3.0/deploy/important-notes/index.md:11:2. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.3.0/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.3.0/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.0/deploy/install/install-with-helm.md:13:  - 2. If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.3.0/deploy/install/install-with-helm.md:20:> **Note**: The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.3.0/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.0/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.3.0/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.0/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.0/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.3.0/deploy/upgrade/longhorn-manager.md:97:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.3.0/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.3.0/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/archives/1.3.0/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.3.0/references/settings.md:282:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.3.0/references/settings.md:358:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.3.0/references/settings.md:368:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.0/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.3.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.3.0/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.3.0/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.3.0/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.3.0/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.3.0/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.3.0/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.3.0/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.3.0/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.3.0/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.3.0/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.3.0/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/archives/1.3.0/best-practices.md:81:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.3.0/best-practices.md:111:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.3.0/best-practices.md:115:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.3.0/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.3.0/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.3.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.3.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.3.1/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/archives/1.3.1/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.1/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.3.1/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.1/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/archives/1.3.1/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.1/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.1/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.1/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.3.1/deploy/important-notes/index.md:11:2. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.3.1/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.3.1/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.1/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.3.1/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.3.1/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.1/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.3.1/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.1/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.1/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.3.1/deploy/upgrade/longhorn-manager.md:100:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.3.1/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.3.1/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/archives/1.3.1/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.3.1/references/settings.md:282:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.3.1/references/settings.md:358:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.3.1/references/settings.md:368:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.1/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.3.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.3.1/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.3.1/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.3.1/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.3.1/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.3.1/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.3.1/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.3.1/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.3.1/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.3.1/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.3.1/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.3.1/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/archives/1.3.1/best-practices.md:79:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.3.1/best-practices.md:109:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.3.1/best-practices.md:113:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.3.1/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.3.1/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.3.1/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.3.1/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.3.2/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/archives/1.3.2/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.2/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.3.2/advanced-resources/data-recovery/recover-without-system.md:28:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.2/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/archives/1.3.2/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.2/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.2/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.2/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.3.2/deploy/important-notes/index.md:11:2. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.3.2/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.3.2/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.2/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.3.2/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.3.2/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.2/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.3.2/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.2/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.2/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.3.2/deploy/upgrade/longhorn-manager.md:100:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.3.2/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.3.2/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/archives/1.3.2/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.3.2/references/settings.md:283:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.3.2/references/settings.md:367:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.3.2/references/settings.md:377:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.2/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.3.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.3.2/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.3.2/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.3.2/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.3.2/volumes-and-nodes/maintenance.md:56:* If Longhorn is installed as a Rancher catalog app, follow [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version) to upgrade Kubernetes.
content/docs/archives/1.3.2/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/archives/1.3.2/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.3.2/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.3.2/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.3.2/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.3.2/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.3.2/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.3.2/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/archives/1.3.2/best-practices.md:79:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.3.2/best-practices.md:109:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.3.2/best-practices.md:113:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.3.2/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.3.2/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.3.2/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.3.2/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.3.3/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/archives/1.3.3/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.3/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.3/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/archives/1.3.3/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.3/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.3/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.3/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.3.3/deploy/important-notes/index.md:12:2. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.3.3/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.3.3/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.3/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.3.3/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.3.3/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.3/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.3.3/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.3/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.3/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.3.3/deploy/upgrade/longhorn-manager.md:100:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.3.3/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.3.3/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/archives/1.3.3/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.3.3/references/settings.md:287:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.3.3/references/settings.md:371:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.3.3/references/settings.md:381:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.3/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.3.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.3.3/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.3.3/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.3.3/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:82:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/archives/1.3.3/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/archives/1.3.3/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.3.3/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.3.3/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.3.3/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.3.3/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.3.3/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.3.3/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/archives/1.3.3/best-practices.md:79:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.3.3/best-practices.md:109:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.3.3/best-practices.md:113:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.3.3/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.3.3/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.3.3/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.3.3/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/1.3.4/advanced-resources/backing-image.md:92:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/archives/1.3.4/advanced-resources/cluster-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.4/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:284:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/archives/1.3.4/advanced-resources/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stable replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../volumes-and-nodes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/archives/1.3.4/advanced-resources/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/archives/1.3.4/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.4/advanced-resources/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.4/advanced-resources/rwx-workloads.md:20:To be able to use RWX volumes, each client node needs to have a NFSv4 client installed. Please refer to [Installing NFSv4 client](../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/archives/1.3.4/deploy/important-notes/index.md:12:2. After the upgrade, the recurring job settings of volumes will be migrated to new recurring job resources, and the `RecurringJobs` field in the volume spec will be deprecated. [[doc](https://longhorn.io/docs/{{< current-version >}}/deploy/upgrade/#4-automatically-migrate-recurring-jobs)]
content/docs/archives/1.3.4/deploy/install/_index.md:41:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/archives/1.3.4/deploy/install/install-with-helm.md:10:- Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.4/deploy/install/install-with-helm.md:15:The initial settings for Longhorn can be [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.3.4/deploy/install/install-with-helm.md:21:If you're using a Helm version prior to version 3.0, you need to [install Tiller in your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/1.3.4/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.4/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/archives/1.3.4/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/archives/1.3.4/deploy/uninstall/_index.md:90:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/1.3.4/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../../advanced-resources/deploy/airgap/#using-a-rancher-app)
content/docs/archives/1.3.4/deploy/upgrade/longhorn-manager.md:100:To avoid crashing existing volumes, as well as switch from the deprecated setting `Guaranteed Engine CPU` to [the new instance manager CPU reservation mechanism](../../../best-practices/#guaranteed-instance-manager-cpu), Longhorn will automatically set `Engine Manager CPU Request` and `Replica Manager CPU Request` from each node based on the deprecated setting value during upgrade. Then, the new global instance manager CPU settings [`Guaranteed Engine Manager CPU`](../../../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../../../references/settings/#guaranteed-replica-manager-cpu) won't take effect.
content/docs/archives/1.3.4/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/archives/1.3.4/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:184:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/archives/1.3.4/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/archives/1.3.4/references/settings.md:287:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/archives/1.3.4/references/settings.md:371:See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details.
content/docs/archives/1.3.4/references/settings.md:381:This value can be lowered to avoid overprovisioning storage. See [Multiple Disks Support](../../volumes-and-nodes/multidisk/#configuration) for details. Also, a replica of volume may take more space than the volume's size since the snapshots need storage space as well. The users can delete snapshots to reclaim spaces.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:8:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:201:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/archives/1.3.4/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/archives/1.3.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/archives/1.3.4/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/1.3.4/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/1.3.4/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:8:To prevent the frontend expansion from interference by unexpected data R/W, Longhorn supports offline expansion only. The `detached` volume will be automatically attached to a random node with [maintenance mode.](../../concepts/#22-reverting-volumes-in-maintenance-mode)
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:82:Some example solutions that use this upgrade methods are [k3s automated upgrades](https://docs.k3s.io/upgrades/automated), [Rancher's Kubernetes upgrade guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/archives/1.3.4/volumes-and-nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction).
content/docs/archives/1.3.4/volumes-and-nodes/scheduling.md:41:- [Disable Scheduling On Cordoned Node](../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/archives/1.3.4/volumes-and-nodes/scheduling.md:42:- [Replica Soft Anti-Affinity](../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/archives/1.3.4/volumes-and-nodes/scheduling.md:43:- [Replica Zone Level Soft Anti-Affinity](../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/archives/1.3.4/volumes-and-nodes/scheduling.md:44:- [Storage Minimal Available Percentage](../../references/settings/#storage-minimal-available-percentage)
content/docs/archives/1.3.4/volumes-and-nodes/scheduling.md:45:- [Storage Over Provisioning Percentage](../../references/settings/#storage-over-provisioning-percentage)
content/docs/archives/1.3.4/volumes-and-nodes/scheduling.md:50:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/archives/1.3.4/volumes-and-nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:121:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:136:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../references/settings/#automatically-cleanup-system-generated-snapshot) is enabled, then formula would become:
content/docs/archives/1.3.4/best-practices.md:79:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../volumes-and-nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/archives/1.3.4/best-practices.md:109:For details, refer to the settings references [`Guaranteed Engine Manager CPU`](../references/settings/#guaranteed-engine-manager-cpu) and [`Guaranteed Replica Manager CPU`](../references/settings/#guaranteed-replica-manager-cpu).
content/docs/archives/1.3.4/best-practices.md:113:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/archives/1.3.4/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/archives/1.3.4/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/archives/1.3.4/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/1.3.4/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/0.8.0/_index.md:16:* Recurring [snapshot and backup](concepts/#24-snapshots)
content/docs/archives/0.8.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/archives/0.8.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/archives/0.8.0/getting-started/quick-start-with-helm.md:12:3. Kubernetes v1.14+ cluster with 1 or more nodes and Mount Propagation feature enabled. If your Kubernetes cluster was provisioned by Rancher v2.0.7+ or later, MountPropagation feature is enabled by default. [Check your Kubernetes environment now](https://github.com/longhorn/longhorn/#environment-check-script). If MountPropagation is disabled, Base Image feature will be disabled.
content/docs/archives/0.8.0/getting-started/quick-start-with-helm.md:26:To install Longhorn using Helm, you first need to [install Helm](https://helm.sh/docs/intro/install/) locally. If you're using a version prior to version 3.0, you need to [install Tiller into your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/0.8.0/install/install-with-helm.md:9:To install Longhorn using Helm, you first need to [install Helm](https://helm.sh/docs/intro/install/) locally. If you're using a version prior to version 3.0, you need to [install Tiller into your Kubernetes cluster with role-based access control (RBAC)](https://v2.helm.sh/docs/using_helm/#tiller-namespaces-and-rbac).
content/docs/archives/0.8.0/install/install-with-rancher.md:15:3. Kubernetes v1.14+ cluster with 1 or more nodes and Mount Propagation feature enabled. If your Kubernetes cluster was provisioned by Rancher v2.0.7+ or later, MountPropagation feature is enabled by default. [Check your Kubernetes environment now](https://github.com/longhorn/longhorn/#environment-check-script). If MountPropagation is disabled, the Base Image feature will be disabled.
content/docs/archives/0.8.0/install/uninstall-longhorn.md:74:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/archives/0.8.0/terminology.md:54:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/archives/0.8.0/terminology.md:181:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/archives/0.8.0/terminology.md:195:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/archives/0.8.0/users-guide/backup-and-restore/backupstores-and-backuptargets.md:105:Please follow [the Kubernetes document](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) to create the secret.
content/docs/archives/0.8.0/users-guide/node-failure.md:30:2. If the node is **not back online within 5 - 6 minutes** of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will become `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details. 
content/docs/archives/0.8.0/users-guide/settings.md:14:* Description: The target used for backup. Support NFS or S3. See [Snapshot and Backup](../../concepts/#24-snapshots) for details.
content/docs/archives/0.8.0/users-guide/settings.md:19:[Backups and Secondary Storage](../../concepts/#3-backups-and-secondary-storage) for details.
content/docs/archives/0.8.0/users-guide/settings.md:23:* Description: In seconds. The interval to poll the backup store for updating volumes' Last Backup field. Set to 0 to disable the polling. See [Disaster Recovery Volume](../../concepts/#33-disaster-recovery-volumes) for details.
content/docs/archives/0.8.0/users-guide/troubleshooting.md:14:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.6.1/advanced-resources/backing-image/backing-image.md:110:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.6.1/advanced-resources/cluster-restore/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.6.1/advanced-resources/data-cleanup/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stale replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../../nodes-and-volumes/volumes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.6.1/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.6.1/advanced-resources/os-distro-specific/okd-support.md:29:To understand more about configuring the disks for Longhorn, please refer to the section [Configuring Defaults for Nodes and Disks](../../../nodes-and-volumes/nodes/default-disk-and-node-config/#launch-longhorn-with-multiple-disks)
content/docs/1.6.1/advanced-resources/os-distro-specific/okd-support.md:94:Please refer to the section [Customizing Default Disks for New Nodes](../../../nodes-and-volumes/nodes/default-disk-and-node-config/#customizing-default-disks-for-new-nodes) to label and annotate storage node on where your device is by oc commands:
content/docs/1.6.1/advanced-resources/os-distro-specific/talos-linux-support.md:34:By default, Talos Linux applies a `baseline` pod security profile across namespaces, except for the kube-system namespace. This default setting restricts Longhorn's ability to manage and access system resources. For more information, see [Root and Privileged Permission](../../../deploy/install/#root-and-privileged-permission).
content/docs/1.6.1/advanced-resources/os-distro-specific/talos-linux-support.md:36:For detailed instructions, see [Pod Security Policies Disabled & Pod Security Admission Introduction](../../../deploy/important-notes/#pod-security-policies-disabled--pod-security-admission-introduction) and Talos' documentation on [Pod Security](https://www.talos.dev/v1.6/kubernetes-guides/configuration/pod-security/).
content/docs/1.6.1/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.6.1/advanced-resources/security/volume-encryption.md:99:Longhorn supports [offline expansion](../../../nodes-and-volumes/volumes/expansion/#encrypted-volume) for encrypted volumes.
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.6.1/deploy/important-notes/index.md:67:Recommendations](../../maintenance/maintenance/#node-drain-policy-recommendations) section for help deciding which
content/docs/1.6.1/deploy/important-notes/index.md:121:> Whenever engine upgrade enforcement causes upgrade failure, Longhorn allows you to revert to the previous version because Longhorn Manager will block the entire upgrade. However, Longhorn prohibits downgrading when an upgrade is successful. For more information, see [Upgrade Path Enforcement](../../deploy/upgrade/#upgrade-path-enforcement-and-downgrade-prevention).
content/docs/1.6.1/deploy/important-notes/index.md:142:Starting with Longhorn v1.6.0, Longhorn allows you to modify the [Danger Zone settings](https://longhorn.io/docs/1.6.0/references/settings/#danger-zone) without the need to wait for all volumes to become detached. Your preferred settings are immediately applied in the following scenarios:
content/docs/1.6.1/deploy/important-notes/index.md:151:  | [Kubernetes Taint Toleration](../../references/settings/#kubernetes-taint-toleration)| [Taints and Tolerations](../../advanced-resources/deploy/taint-toleration/) | System-managed components |
content/docs/1.6.1/deploy/important-notes/index.md:152:  | [Priority Class](../../references/settings/#priority-class) | [Priority Class](../../advanced-resources/deploy/priority-class/) | System-managed components |
content/docs/1.6.1/deploy/important-notes/index.md:153:  | [System Managed Components Node Selector](../../references/settings/#system-managed-components-node-selector) | [Node Selector](../../advanced-resources/deploy/node-selector/) | System-managed components |
content/docs/1.6.1/deploy/important-notes/index.md:154:  | [Storage Network](../../references/settings/#storage-network) | [Storage Network](../../advanced-resources/deploy/storage-network/) | Instance Manager and Backing Image components |
content/docs/1.6.1/deploy/important-notes/index.md:155:  | [V1 Data Engine](../../references/settings/#v1-data-engine) || Instance Manager component |
content/docs/1.6.1/deploy/important-notes/index.md:156:  | [V2 Data Engine](../../references/settings/#v2-data-engine) | [V2 Data Engine (Preview Feature)](../../v2-data-engine/) | Instance Manager component |
content/docs/1.6.1/deploy/important-notes/index.md:157:  | [Guaranteed Instance Manager CPU](../../references/settings/#guaranteed-instance-manager-cpu) || Instance Manager component |
content/docs/1.6.1/deploy/important-notes/index.md:158:  | [Guaranteed Instance Manager CPU for V2 Data Engine](../../references/settings/#guaranteed-instance-manager-cpu-for-v2-data-engine) || Instance Manager component |
content/docs/1.6.1/deploy/install/_index.md:44:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.6.1/deploy/install/_index.md:231:- For Talos Linux, [the NFS client is part of the `kubelet` image maintained by the Talos team](https://www.talos.dev/v1.6/kubernetes-guides/configuration/storage/#nfs).
content/docs/1.6.1/deploy/install/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.6.1/deploy/install/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.6.1/deploy/install/install-with-argocd.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.6.1/deploy/install/install-with-fleet.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.6.1/deploy/install/install-with-flux.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.6.1/deploy/install/install-with-flux.md:10:  - [Install the Flux CLI](https://fluxcd.io/flux/installation/#install-the-flux-cli).
content/docs/1.6.1/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.6.1/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.6.1/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.6.1/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.6.1/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.6.1/deploy/uninstall/_index.md:20:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.6.1/deploy/uninstall/_index.md:78:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.6.1/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../install/airgap/#using-a-rancher-app)
content/docs/1.6.1/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.6.1/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.6.1/maintenance/maintenance.md:105:guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.6.1/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:203:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.6.1/nodes-and-volumes/nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:43:- [Disable Scheduling On Cordoned Node](../../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:44:- [Replica Soft Anti-Affinity](../../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:45:- [Replica Zone Level Soft Anti-Affinity](../../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:46:- [Replica Disk Level Soft Anti-Affinity](../../../references/settings/#replica-disk-level-soft-anti-affinity)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:47:- [Storage Minimal Available Percentage](../../../references/settings/#storage-minimal-available-percentage)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:48:- [Storage Over Provisioning Percentage](../../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:49:- [Allow Empty Node Selector Volume](../../../references/settings/#allow-empty-node-selector-volume)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:50:- [Allow Empty Disk Selector Volume](../../../references/settings/#allow-empty-disk-selector-volume)
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:55:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.6.1/nodes-and-volumes/nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:21:    Please refer to [Installing NFSv4 client](../../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:122:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:137:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../../references/settings/#automatically-clean-up-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:53:You can set up a [RecurringJob](../../../snapshots-and-backups/scheduling-backups-and-snapshots/#set-up-recurring-jobs) to periodically trim the filesystem.
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:61:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.6.1/references/storage-class-parameters.md:52:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.6.1/references/storage-class-parameters.md:56:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.6.1/references/storage-class-parameters.md:60:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.6.1/references/storage-class-parameters.md:64:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.6.1/references/storage-class-parameters.md:68:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.6.1/references/settings.md:114:Recommendations](../../maintenance/maintenance/#node-drain-policy-recommendations) for help deciding which is most
content/docs/1.6.1/references/settings.md:555:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.6.1/references/settings.md:710:See [Multiple Disks Support](../../nodes-and-volumes/nodes/multidisk/#configuration) for details.
content/docs/1.6.1/references/settings.md:744:Starting with Longhorn v1.6.0, Longhorn allows you to modify the [Danger Zone settings](https://longhorn.io/docs/1.6.0/references/settings/#danger-zone) without the need to wait for all volumes to become detached. Your preferred settings are immediately applied in the following scenarios:
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:262:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.6.1/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.6.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.6.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:28:- When the global setting [`Allow Volume Creation with Degraded Availability`](../../references/settings/#allow-volume-creation-with-degraded-availability) is enabled, the volume is degraded, indicating some replicas are unhealthy.
content/docs/1.6.1/terminology.md:71:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.6.1/terminology.md:184:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.6.1/terminology.md:190:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.6.1/terminology.md:204:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.6.1/troubleshoot/troubleshooting.md:36:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.6.1/best-practices.md:104:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../nodes-and-volumes/nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.6.1/best-practices.md:124:- **Storage network**: Use a [dedicated storage network](https://longhorn.io/docs/1.6.0/advanced-resources/deploy/storage-network/#setting-storage-network) to improve IO performance and stability.  
content/docs/1.6.1/best-practices.md:126:- **Longhorn disk**: Use a [dedicated disk](https://longhorn.io/docs/1.6.0/nodes-and-volumes/multidisk/#add-a-disk) for Longhorn storage instead of using the root disk.  
content/docs/1.6.1/best-practices.md:128:- **Replica count**: Set the [default replica count](https://longhorn.io/docs/1.6.0/references/settings/#default-replica-count) to "2" to achieve data availability with better disk space usage or less impact to system performance. This practice is especially beneficial to data-intensive applications.  
content/docs/1.6.1/best-practices.md:142:  For applications with replication capability, periodically [delete all types of snapshots](https://longhorn.io/docs/1.6.0/concepts/#243-deleting-snapshots).  
content/docs/1.6.1/best-practices.md:152:- **System backup**: Create periodic [system backups](https://longhorn.io/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system/#create-longhorn-system-backup).  
content/docs/1.6.1/best-practices.md:178:Refer to [Guaranteed Instance Manager CPU](../references/settings/#guaranteed-instance-manager-cpu) for more details.
content/docs/1.6.1/best-practices.md:186:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.6.1/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.6.1/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.6.1/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.6.1/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/docs/1.7.0/advanced-resources/backing-image/backing-image.md:110:Users can [directly create then immediately use a backing image via StorageClass](./#create-and-use-a-backing-image-via-storageclass-and-pvc),
content/docs/1.7.0/advanced-resources/cluster-restore/rancher-cluster-restore.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.7.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:150:- The orphaned data cleanup mechanism does not clean up a stale replica, also known as an error replica. Instead, the stale replica is cleaned up according to the [staleReplicaTimeout](../../../nodes-and-volumes/volumes/create-volumes/#creating-longhorn-volumes-with-kubectl) setting.
content/docs/1.7.0/advanced-resources/data-recovery/export-from-replica.md:14:    By default, the data is stored at the directory specified by the setting [`Default Data Path`](https://longhorn.io/docs/0.8.1/references/settings/#default-data-path).
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:30:    - The credential secret can be referenced [here](https://longhorn.io/docs/{{< current-version >}}/snapshots-and-backups/backup-and-restore/set-backup-target/#set-up-aws-s3-backupstore) and must be created in the `longhorn-system' namespace.
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:10:For more information about how node selector work, refer to the [official Kubernetes documentation.](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
content/docs/1.7.0/advanced-resources/os-distro-specific/okd-support.md:29:To understand more about configuring the disks for Longhorn, please refer to the section [Configuring Defaults for Nodes and Disks](../../../nodes-and-volumes/nodes/default-disk-and-node-config/#launch-longhorn-with-multiple-disks)
content/docs/1.7.0/advanced-resources/os-distro-specific/okd-support.md:94:Please refer to the section [Customizing Default Disks for New Nodes](../../../nodes-and-volumes/nodes/default-disk-and-node-config/#customizing-default-disks-for-new-nodes) to label and annotate storage node on where your device is by oc commands:
content/docs/1.7.0/advanced-resources/os-distro-specific/talos-linux-support.md:34:By default, Talos Linux applies a `baseline` pod security profile across namespaces, except for the kube-system namespace. This default setting restricts Longhorn's ability to manage and access system resources. For more information, see [Root and Privileged Permission](../../../deploy/install/#root-and-privileged-permission).
content/docs/1.7.0/advanced-resources/os-distro-specific/talos-linux-support.md:36:For detailed instructions, see [Pod Security Policies Disabled & Pod Security Admission Introduction](../../../deploy/important-notes/#pod-security-policies-disabled--pod-security-admission-introduction) and Talos' documentation on [Pod Security](https://www.talos.dev/v1.6/kubernetes-guides/configuration/pod-security/).
content/docs/1.7.0/advanced-resources/security/mtls-support.md:51:For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.7.0/advanced-resources/security/volume-encryption.md:99:Longhorn supports [offline expansion](../../../nodes-and-volumes/volumes/expansion/#encrypted-volume) for encrypted volumes.
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:115:- [Concurrent volume backup restore per node limit](../../../references/settings/#concurrent-volume-backup-restore-per-node-limit)
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:116:- [Concurrent replica rebuild per node limit](../../../references/settings/#concurrent-replica-rebuild-per-node-limit)
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:117:- [Backup Target](../../../references/settings/#backup-target)
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:118:- [Backup Target Credential Secret](../../../references/settings/#backup-target-credential-secret)
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-to-a-cluster-contains-data-using-Rancher-snapshot.md:11:- The setting [`Disable Revision Counter`](../../../references/settings/#disable-revision-counter) is false. (It's false by default.) Otherwise, users need to manually check if the data among volume replicas are consistent, or directly restore volumes from backup.
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:165:1. Create pre-requisite resources according to the [nginx ingress controller documentation.](https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command)
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:167:2. Create an ELB by following [these steps.](https://kubernetes.github.io/ingress-nginx/deploy/#aws)
content/docs/1.7.0/deploy/important-notes/index.md:67:Recommendations](../../maintenance/maintenance/#node-drain-policy-recommendations) section for help deciding which
content/docs/1.7.0/deploy/important-notes/index.md:121:> Whenever engine upgrade enforcement causes upgrade failure, Longhorn allows you to revert to the previous version because Longhorn Manager will block the entire upgrade. However, Longhorn prohibits downgrading when an upgrade is successful. For more information, see [Upgrade Path Enforcement](../../deploy/upgrade/#upgrade-path-enforcement-and-downgrade-prevention).
content/docs/1.7.0/deploy/important-notes/index.md:142:Starting with Longhorn v1.6.0, Longhorn allows you to modify the [Danger Zone settings](https://longhorn.io/docs/1.6.0/references/settings/#danger-zone) without the need to wait for all volumes to become detached. Your preferred settings are immediately applied in the following scenarios:
content/docs/1.7.0/deploy/important-notes/index.md:151:  | [Kubernetes Taint Toleration](../../references/settings/#kubernetes-taint-toleration)| [Taints and Tolerations](../../advanced-resources/deploy/taint-toleration/) | System-managed components |
content/docs/1.7.0/deploy/important-notes/index.md:152:  | [Priority Class](../../references/settings/#priority-class) | [Priority Class](../../advanced-resources/deploy/priority-class/) | System-managed components |
content/docs/1.7.0/deploy/important-notes/index.md:153:  | [System Managed Components Node Selector](../../references/settings/#system-managed-components-node-selector) | [Node Selector](../../advanced-resources/deploy/node-selector/) | System-managed components |
content/docs/1.7.0/deploy/important-notes/index.md:154:  | [Storage Network](../../references/settings/#storage-network) | [Storage Network](../../advanced-resources/deploy/storage-network/) | Instance Manager and Backing Image components |
content/docs/1.7.0/deploy/important-notes/index.md:155:  | [V1 Data Engine](../../references/settings/#v1-data-engine) || Instance Manager component |
content/docs/1.7.0/deploy/important-notes/index.md:156:  | [V2 Data Engine](../../references/settings/#v2-data-engine) | [V2 Data Engine (Preview Feature)](../../v2-data-engine/) | Instance Manager component |
content/docs/1.7.0/deploy/important-notes/index.md:157:  | [Guaranteed Instance Manager CPU](../../references/settings/#guaranteed-instance-manager-cpu) || Instance Manager component |
content/docs/1.7.0/deploy/important-notes/index.md:158:  | [Guaranteed Instance Manager CPU for V2 Data Engine](../../references/settings/#guaranteed-instance-manager-cpu-for-v2-data-engine) || Instance Manager component |
content/docs/1.7.0/deploy/install/_index.md:44:For the minimum recommended hardware, refer to the [best practices guide.](../../best-practices/#minimum-recommended-hardware)
content/docs/1.7.0/deploy/install/_index.md:231:- For Talos Linux, [the NFS client is part of the `kubelet` image maintained by the Talos team](https://www.talos.dev/v1.6/kubernetes-guides/configuration/storage/#nfs).
content/docs/1.7.0/deploy/install/airgap.md:179:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.7.0/deploy/install/airgap.md:279:If you keep the images' names as recommended [here](./#recommendation), you only need to do the following steps:
content/docs/1.7.0/deploy/install/install-with-argocd.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.7.0/deploy/install/install-with-fleet.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.7.0/deploy/install/install-with-flux.md:9:  - Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.7.0/deploy/install/install-with-flux.md:10:  - [Install the Flux CLI](https://fluxcd.io/flux/installation/#install-the-flux-cli).
content/docs/1.7.0/deploy/install/install-with-helm.md:10:- Kubernetes cluster: Ensure that each node fulfills the [installation requirements](../#installation-requirements).
content/docs/1.7.0/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.7.0/deploy/install/install-with-kubectl.md:9:Each node in the Kubernetes cluster where Longhorn will be installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.7.0/deploy/install/install-with-kubectl.md:13:The initial settings for Longhorn can be customized by [editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-the-longhorn-deployment-yaml-file)
content/docs/1.7.0/deploy/install/install-with-rancher.md:13:Each node in the Kubernetes cluster where Longhorn is installed must fulfill [these requirements.](../#installation-requirements)
content/docs/1.7.0/deploy/uninstall/_index.md:20:we introduce a new setting, [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag).
content/docs/1.7.0/deploy/uninstall/_index.md:78:One reason can be that [deleting-confirmation-flag](../../references/settings/#deleting-confirmation-flag) is `false`.
content/docs/1.7.0/deploy/upgrade/longhorn-manager.md:14:For example, the image `registry.example.com/longhorn/longhorn-manager:v{{< current-version >}}` is changed to `longhorn/longhorn-manager:v{{< current-version >}}` in Longhorn images section. For more information, see the air gap installation steps [here.](../../install/airgap/#using-a-rancher-app)
content/docs/1.7.0/high-availability/node-failure.md:24:You can find more detail about the setting options in the `Pod Deletion Policy When Node is Down` in the **Settings** tab in the Longhorn UI or [Settings reference](../../references/settings/#pod-deletion-policy-when-node-is-down)
content/docs/1.7.0/high-availability/node-failure.md:34:If the node is not back online within 5 - 6 minutes of the failure, Kubernetes will try to delete all unreachable pods based on the pod eviction mechanism and these pods will be in a `Terminating` state. See [pod eviction timeout](https://kubernetes.io/docs/concepts/architecture/nodes/#condition) for details.
content/docs/1.7.0/maintenance/maintenance.md:105:guide](https://rancher.com/docs/rancher/v2.x/en/cluster-admin/upgrading-kubernetes/#upgrading-the-kubernetes-version),
content/docs/1.7.0/monitoring/alert-rules-example.md:103:See more about how to define alert rules at [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules).
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:8:Longhorn natively exposes metrics in [Prometheus text format](https://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format) on a REST endpoint `http://LONGHORN_MANAGER_IP:PORT/metrics`.
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:203:   See [Prometheus - Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#alerting-rules) for more information.
content/docs/1.7.0/nodes-and-volumes/nodes/multidisk.md:39:- There is no existing replica using the disk, including any replicas in an error state. About how to evict replicas from disabled disks, refer to [Select Disks or Nodes for Eviction](../disks-or-nodes-eviction/#select-disks-or-nodes-for-eviction)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:43:- [Disable Scheduling On Cordoned Node](../../../references/settings/#disable-scheduling-on-cordoned-node)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:44:- [Replica Soft Anti-Affinity](../../../references/settings/#replica-node-level-soft-anti-affinity) (also called Replica Node Level Soft Anti-Affinity)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:45:- [Replica Zone Level Soft Anti-Affinity](../../../references/settings/#replica-zone-level-soft-anti-affinity)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:46:- [Replica Disk Level Soft Anti-Affinity](../../../references/settings/#replica-disk-level-soft-anti-affinity)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:47:- [Storage Minimal Available Percentage](../../../references/settings/#storage-minimal-available-percentage)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:48:- [Storage Over Provisioning Percentage](../../../references/settings/#storage-over-provisioning-percentage)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:49:- [Allow Empty Node Selector Volume](../../../references/settings/#allow-empty-node-selector-volume)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:50:- [Allow Empty Disk Selector Volume](../../../references/settings/#allow-empty-disk-selector-volume)
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:55:Since these are reserved and used by Kubernetes as [well-known labels](https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone).
content/docs/1.7.0/nodes-and-volumes/nodes/node-space-usage.md:35:Notice that the allocable space may be greater than the actual available space of the node when setting `Storage Over Provisioning Percentage` to a value greater than 100. If the volumes are heavily used and lots of historical data will be stored in the volume snapshots, please be careful about using a large value for this setting. For more info about the setting, see [here](../../../references/settings/#storage-over-provisioning-percentage) for details. 
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:21:    Please refer to [Installing NFSv4 client](../../../deploy/install/#installing-nfsv4-client) for more installation details.
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:122:          If there are heavy writing tasks for volumes, the head/snapshot average actual size would be volume the nominal size. In this case, it's better to set [`Storage Over Provisioning Percentage`](../../../references/settings/#storage-over-provisioning-percentage) to be smaller than 100% to avoid disk space exhaustion.
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:137:            - Users don't want snapshot at all. Neither (manually created) snapshot nor recurring job will be launched. Assume [setting _Automatically Cleanup System Generated Snapshot_](../../../references/settings/#automatically-clean-up-system-generated-snapshot) is enabled, then formula would become:
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:53:You can set up a [RecurringJob](../../../snapshots-and-backups/scheduling-backups-and-snapshots/#set-up-recurring-jobs) to periodically trim the filesystem.
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:61:To help reclaim as much space as possible automatically, Longhorn introduces [setting _Remove Snapshots During Filesystem Trim_](../../../references/settings/#remove-snapshots-during-filesystem-trim). This allows Longhorn filesystem trim feature to automatically mark the latest snapshot and its ancestors as removed and stops at the snapshot containing multiple children. As a result, Longhorn can reclaim space for as more snapshots as possible.
content/docs/1.7.0/references/storage-class-parameters.md:52:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.7.0/references/storage-class-parameters.md:56:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.7.0/references/storage-class-parameters.md:60:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.7.0/references/storage-class-parameters.md:64:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.7.0/references/storage-class-parameters.md:68:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.7.0/references/settings.md:114:Recommendations](../../maintenance/maintenance/#node-drain-policy-recommendations) for help deciding which is most
content/docs/1.7.0/references/settings.md:555:For more information on how the backupstore poll interval affects the recovery time objective and recovery point objective, refer to the [concepts section.](../../concepts/#34-backupstore-update-intervals-rto-and-rpo)
content/docs/1.7.0/references/settings.md:710:See [Multiple Disks Support](../../nodes-and-volumes/nodes/multidisk/#configuration) for details.
content/docs/1.7.0/references/settings.md:744:Starting with Longhorn v1.6.0, Longhorn allows you to modify the [Danger Zone settings](https://longhorn.io/docs/1.6.0/references/settings/#danger-zone) without the need to wait for all volumes to become detached. Your preferred settings are immediately applied in the following scenarios:
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore. A backup of a snapshot is copied to the backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:8:For more information on how backups work, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:8:For more information on the setting `Restore Volume Recurring Jobs`, refer to the [settings](../../../references/settings/#restore-volume-recurring-jobs) section.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/restore-recurring-jobs-from-a-backup.md:10:For more information on how volume backup works, refer to the [concepts](../../../concepts/#3-backups-and-secondary-storage) section.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:10:For more information about how the backupstore works in Longhorn, see the [concepts section.](../../../concepts/#3-backups-and-secondary-storage)
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:262:   For more information on creating a secret, see [the Kubernetes documentation.](https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually) The secret must be created in the `longhorn-system` namespace for Longhorn to access it.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:6:Backups in Longhorn are objects in an off-cluster backupstore, and the endpoint to access the backupstore is the backup target. For more information, see [this section.](../../../concepts/#31-how-backups-work)
content/docs/1.7.0/snapshots-and-backups/setup-a-snapshot.md:6:A [snapshot](../../concepts/#24-snapshots) is the state of a Kubernetes Volume at any given point in time.
content/docs/1.7.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:9:For a longer explanation of how DR volumes work, see the [concepts section.](../../concepts/#33-disaster-recovery-volumes)
content/docs/1.7.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:28:- When the global setting [`Allow Volume Creation with Degraded Availability`](../../references/settings/#allow-volume-creation-with-degraded-availability) is enabled, the volume is degraded, indicating some replicas are unhealthy.
content/docs/1.7.0/terminology.md:71:For a longer explanation of how snapshots and backups work, refer to the [conceptual documentation.](../concepts/#241-how-snapshots-work)
content/docs/1.7.0/terminology.md:184:A Kubernetes resource that can be used to automatically provision a PersistentVolume for a pod. For more information, refer to the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource)
content/docs/1.7.0/terminology.md:190:See [Longhorn System Backup Bundle](../advanced-resources/system-backup-restore/backup-longhorn-system/#longhorn-system-backup-bundle) for details.
content/docs/1.7.0/terminology.md:204:These files will still be available after a container crashes, but they will not be available past the lifetime of a pod. To get storage that is still available after the lifetime of a pod, a Kubernetes [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) is required.
content/docs/1.7.0/troubleshoot/troubleshooting.md:36:By default, Kubernetes uses `/usr/libexec/kubernetes/kubelet-plugins/volume/exec/`, as stated in the [official document](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md/#prerequisites).
content/docs/1.7.0/best-practices.md:104:Don't use a symbolic link for the extra disks. Use `mount --bind` instead of `ln -s` and make sure it's in the `fstab` file. For details, see [the section about multiple disk support.](../nodes-and-volumes/nodes/multidisk/#use-an-alternative-path-for-a-disk-on-the-node)
content/docs/1.7.0/best-practices.md:124:- **Storage network**: Use a [dedicated storage network](https://longhorn.io/docs/1.6.0/advanced-resources/deploy/storage-network/#setting-storage-network) to improve IO performance and stability.  
content/docs/1.7.0/best-practices.md:126:- **Longhorn disk**: Use a [dedicated disk](https://longhorn.io/docs/1.6.0/nodes-and-volumes/multidisk/#add-a-disk) for Longhorn storage instead of using the root disk.  
content/docs/1.7.0/best-practices.md:128:- **Replica count**: Set the [default replica count](https://longhorn.io/docs/1.6.0/references/settings/#default-replica-count) to "2" to achieve data availability with better disk space usage or less impact to system performance. This practice is especially beneficial to data-intensive applications.  
content/docs/1.7.0/best-practices.md:142:  For applications with replication capability, periodically [delete all types of snapshots](https://longhorn.io/docs/1.6.0/concepts/#243-deleting-snapshots).  
content/docs/1.7.0/best-practices.md:152:- **System backup**: Create periodic [system backups](https://longhorn.io/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system/#create-longhorn-system-backup).  
content/docs/1.7.0/best-practices.md:178:Refer to [Guaranteed Instance Manager CPU](../references/settings/#guaranteed-instance-manager-cpu) for more details.
content/docs/1.7.0/best-practices.md:186:We don't recommend modifying the default StorageClass named `longhorn`, since the change of parameters might cause issues during an upgrade later. If you want to change the parameters set in the StorageClass, you can create a new StorageClass by referring to the [StorageClass examples](../references/examples/#storageclass).
content/docs/1.7.0/concepts.md:12:For the installation requirements, go to [this section.](../deploy/install/#installation-requirements)
content/docs/1.7.0/concepts.md:141:The default replica count can be changed in the [settings.](../references/settings/#default-replica-count) When a volume is attached, the replica count for the volume can be changed in the UI.
content/docs/1.7.0/concepts.md:358:A [PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a piece of persistent storage in the Kubernetes cluster, while a [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for storage. [StorageClasses](https://kubernetes.io/docs/concepts/storage/storage-classes/) allow new storage to be dynamically provisioned for workloads on demand.
content/docs/1.7.0/concepts.md:412:The VolumeClaimTemplate is important for block storage solutions like EBS and Longhorn. Because those solutions are inherently [ReadWriteOnce,](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) they cannot be shared between the Pods.
content/kb/space-consumption-guideline.md:16:Due to the fact that Longhorn volumes can hold historic data as snapshots, the volume actual size can be much greater than the spec size. For more details, you can check [this section](../../docs/1.5.1/volumes-and-nodes/volume-size/#volume-actual-size) for a better understanding over the concept of volume size.
content/kb/space-consumption-guideline.md:24:The reason for this symptom is explained in [the volume size section](../../docs/1.5.1/volumes-and-nodes/volume-size/#volume-actual-size) as well. Briefly, a Longhorn volume is a block device which does not recognize the filesystem used on top of it. Deleting a file is a filesystem layer operation that does not actually free up blocks from the underlying volume.
content/kb/tip-only-use-storage-on-a-set-of-nodes.md:23:* Install Longhorn with the setting [Create Default Disk on Labeled Nodes](https://longhorn.io/docs/1.2.2/references/settings/#create-default-disk-on-labeled-nodes) set to `true`.
content/kb/troubleshooting-dns-resolution-failed.md:46:6. If you're not using k3s _or_ rke2, check the [Kubernetes DNS Resolution Known Issues](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues) to see how to set kubelet's `--resolv-conf` flag to point to the correct `resolv.conf`.
content/kb/troubleshooting-fstrim-error.md:39:down and up the workload deployment that is using the volume. See more details at https://longhorn.io/docs/1.4.0/volumes-and-nodes/trim-filesystem/#prerequisites
content/kb/troubleshooting-fstrim-error.md:69:Upgrade the kernel to a recommended version as in the best practice page https://longhorn.io/docs/1.5.1/best-practices/#operating-system.
content/kb/troubleshooting-instance-manager-pods-are-restarted-every-hour.md:27:One potential root cause is that the cluster has the default PriorityClass (i.e., the PriorityClass with `globalDefault` field set to `true`) but the [PriorityClass setting](https://longhorn.io/docs/1.2.3/references/settings/#priority-class) in Longhorn is empty.
content/kb/troubleshooting-instance-manager-pods-are-restarted-every-hour.md:28:See more about PriorityClass at [here](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass).
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:18:According to Kubernetes [CronJob limitations](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-job-limitations):
content/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4.md:33:* https://kubernetes.io/docs/reference/labels-annotations-taints/#volume-kubernetes-io-storage-provisioner
content/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4.md:34:* https://kubernetes.io/docs/reference/labels-annotations-taints/#volume-beta-kubernetes-io-storage-provisioner-deprecated
content/kb/troubleshooting-volume-pvc-xxx-not-scheduled.md:45:1. either set [`Node Level Soft Anti-affinity` to `true`](https://longhorn.io/docs/1.0.2/references/settings/#replica-node-level-soft-anti-affinity).
content/kb/troubleshooting-volume-pvc-xxx-not-scheduled.md:46:2. or, create [a new StorageClass](https://longhorn.io/docs/1.0.2/references/examples/#storageclass) with replica count set to `1` or `2`.
content/kb/troubleshooting-volume-take-long-time-to-mount.md:55:* Related Kubernetes documentation: https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/#allow-users-to-skip-recursive-permission-changes-on-mount
content/kb/troubleshooting-websocket_handshake_error_with_unexpected_response_code_200.md:39:- To support Longhorn UI routing to different paths. e.g., /longhorn/#/dashboard
content/kb/troubleshooting-websocket_handshake_error_with_unexpected_response_code_200.md:42:As result, `<LONGHORN_URL>/<TAG>` changes to `<LONGHORN_URL>/#/<TAG>`.
content/kb/troubleshooting-websocket_handshake_error_with_unexpected_response_code_200.md:84:2. Access/Rebookmark Longhorn URL from `<LONGHORN_URL>/#`.
content/kb/troubleshooting-volume-readonly-or-io-error.md:56:1. CPU utilization is too high on the node. If the Longhorn engine doesn't have enough CPU resources to handle the request, the request might time out, result in losing the connection to a replica. You can refer to [this doc](https://longhorn.io/docs/1.1.0/best-practices/#guaranteed-engine-cpu) to see how to reserve the proper amount of CPU resources for Longhorn instance manager pods.
content/kb/troubleshooting-volume-readonly-or-io-error.md:65:Since Longhorn version v1.1.0, a new setting [`Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly`](https://longhorn.io/docs/1.1.0/references/settings/#automatically-delete-workload-pod-when-the-volume-is-detached-unexpectedly) is introduced so that Longhorn will automatically delete the workload pod that is managed by a controller (e.g. deployment, statefulset, daemonset, etc...).
