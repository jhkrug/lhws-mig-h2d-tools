content/blog/cve-2021-36779-36780.md:12:There are two vulnerabilities found in released versions (< 1.1.3, < 1.2.3) as below. They have been fixed in the latest releases (1.1.3, 1.2.3).
content/blog/kubecon-eu-2020.md:12:I am glad to announce that Longhorn will join the Virtual KubeCon EU 2020 next week! As a CNCF project, you can find us at Expo Hall -> Project Pavilion.
content/blog/kubecon-na-2020.md:20:You can find our booth at Expo Hall -> Project Pavilion.
content/blog/longhorn-v1.2.md:186:> Longhorn will automatically add the volume to the default group when the volume has no recurring job
content/blog/longhorn-v1.2.md:187:>
content/blog/performance-scalability-report-aug-2020.md:31:>Disk: 200 GiB NVMe SSD as the instance store.
content/blog/performance-scalability-report-aug-2020.md:32:>
content/blog/performance-scalability-report-aug-2020.md:33:>CPU: 8 vCPUs (Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz)
content/blog/performance-scalability-report-aug-2020.md:34:>
content/blog/performance-scalability-report-aug-2020.md:35:>Memory: 16 GB
content/blog/performance-scalability-report-aug-2020.md:36:>
content/blog/performance-scalability-report-aug-2020.md:37:>Network: Up to 10Gbps
content/blog/performance-scalability-report-aug-2020.md:45:>Kubernetes: v1.17.5.
content/blog/performance-scalability-report-aug-2020.md:46:>
content/blog/performance-scalability-report-aug-2020.md:47:>Node OS: 5.3.0-1023-aws #25~18.04.1-Ubuntu SMP
content/blog/performance-scalability-report-aug-2020.md:48:>
content/blog/performance-scalability-report-aug-2020.md:49:>Longhorn: v1.0.1
content/blog/performance-scalability-report-aug-2020.md:86:>CPU: 8 vCPUs
content/blog/performance-scalability-report-aug-2020.md:87:>
content/blog/performance-scalability-report-aug-2020.md:88:>Memory: 32 GB Memory
content/blog/performance-scalability-report-aug-2020.md:89:>
content/blog/performance-scalability-report-aug-2020.md:97:>Kubernetes: v1.18.6, installed using Rancher
content/blog/performance-scalability-report-aug-2020.md:98:>
content/blog/performance-scalability-report-aug-2020.md:99:>Longhorn v1.0.1
content/docs/1.4.0/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.4.0/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.4.0/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.4.0/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.4.0/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.4.0/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.4.0/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.4.0 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.4.0/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.4.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.4.0`
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.4.0/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.0/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.4.0/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.0/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.4.0/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.4.0/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.4.0/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.4.0/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.4.0/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.4.0/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.4.0/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:173:In v1.4.0, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.4.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.0/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.0/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.0/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.0/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.0/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:186:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:192:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:197:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:206:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:209:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:213:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:48:   >
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:49:   > For example:
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:50:   > ```yaml
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:51:   > defaultSettings:
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:52:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:53:   >  ```
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:99:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.4.0/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.0/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.0/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.0/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.4.0/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.0/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.4.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.0/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:65:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:66:   >
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:67:   > For example:
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:68:   > ```yaml
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:69:   > defaultSettings:
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:70:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:71:   >  ```
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:135:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.0/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.4.0/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.4.0/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.4.0/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.4.0/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.4.0/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.4.0/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.4.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.4.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.4.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.4.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.0/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.4.0/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.4.0/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.4.0/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.4.0/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.4.0/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.4.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.0/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.0/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.0/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.0/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:55:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:59:  > **Note:** Longhorn system restores volume with the latest backup. The system backup does not trigger volume backup. We suggest updating all volumes' last backup. Taking volume backups ensures the data is up-to-date with the system backup.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:80:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:94:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:100:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:104:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:30:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:59:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:77:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:84:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:97:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:101:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:122:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.0/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.4.0/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.4.0/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.4.0/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.4.0/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.4.0/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.0/best-practices.md:123:> Recommend: `false`
content/docs/1.4.0/best-practices.md:129:> Recommend: `false`
content/docs/1.4.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.0/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.4.0/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.4.0/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.4.0/deploy/important-notes/index.md:18:When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to v1.4.0, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890).
content/docs/1.4.0/deploy/important-notes/index.md:32:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.4.0/deploy/important-notes/index.md:35:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.0/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.4.0/deploy/install/_index.md:149:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.4.0/deploy/install/_index.md:254:The `Server Version` should be >= v1.21.
content/docs/1.4.0/deploy/install/install-with-helm.md:15:> [This script](https://github.com/longhorn/longhorn/blob/v1.4.0/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.0/deploy/install/install-with-helm.md:20:> **Note**:
content/docs/1.4.0/deploy/install/install-with-helm.md:21:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.0/deploy/install/install-with-helm.md:22:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.0/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.0/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.0/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.0/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.0/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.0/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.4.0/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.4.0/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.4.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.4.0/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.4.0/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.4.0/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.4.0/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.4.0/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.4.0/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.4.0/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.4.0/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.4.0/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.4.0/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.4.0/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.4.0/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.4.0/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.4.0/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.4.0/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.0/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.0/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.0/references/settings.md:83:> Default: `false`
content/docs/1.4.0/references/settings.md:91:> Default: `true`
content/docs/1.4.0/references/settings.md:97:> Default: `true`
content/docs/1.4.0/references/settings.md:104:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.0/references/settings.md:105:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.0/references/settings.md:106:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.0/references/settings.md:110:> Default: `true`
content/docs/1.4.0/references/settings.md:116:> Default: `0`
content/docs/1.4.0/references/settings.md:124:> Default: `5`
content/docs/1.4.0/references/settings.md:134:> Default: `false`
content/docs/1.4.0/references/settings.md:144:> Default: `longhorn.io/v1beta2`
content/docs/1.4.0/references/settings.md:150:> Default: `disabled`
content/docs/1.4.0/references/settings.md:168:> Default: `/var/lib/longhorn/`
content/docs/1.4.0/references/settings.md:186:> Default: `longhorn-static`
content/docs/1.4.0/references/settings.md:192:> Default: `3`
content/docs/1.4.0/references/settings.md:207:> Default: `false`
content/docs/1.4.0/references/settings.md:211:> Default: `false`
content/docs/1.4.0/references/settings.md:217:> Default: `true`
content/docs/1.4.0/references/settings.md:229:> Default: `do-nothing`
content/docs/1.4.0/references/settings.md:244:> Default: `600`
content/docs/1.4.0/references/settings.md:252:> Default: `if-not-present`
content/docs/1.4.0/references/settings.md:268:> Default: `60`
content/docs/1.4.0/references/settings.md:273:> Default: `300`
content/docs/1.4.0/references/settings.md:277:> **Note:**
content/docs/1.4.0/references/settings.md:278:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.0/references/settings.md:279:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.0/references/settings.md:282:> Default: `8`
content/docs/1.4.0/references/settings.md:293:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.0/references/settings.md:297:> Default: `1`
content/docs/1.4.0/references/settings.md:307:> Default: `false`
content/docs/1.4.0/references/settings.md:313:> Default: `30`
content/docs/1.4.0/references/settings.md:321:> Default: `fast-check`
content/docs/1.4.0/references/settings.md:330:> Default: `false`
content/docs/1.4.0/references/settings.md:336:> Default: `0 0 */7 * *`
content/docs/1.4.0/references/settings.md:339:> **Warning**
content/docs/1.4.0/references/settings.md:340:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.0/references/settings.md:346:> Default: `false`
content/docs/1.4.0/references/settings.md:354:> Default: `false`
content/docs/1.4.0/references/settings.md:358:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.0/references/settings.md:362:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.0/references/settings.md:368:> Example: `s3-secret`
content/docs/1.4.0/references/settings.md:374:> Default: `300`
content/docs/1.4.0/references/settings.md:382:> Default: `1440`
content/docs/1.4.0/references/settings.md:390:> Default: `1`
content/docs/1.4.0/references/settings.md:399:> Default: `1`
content/docs/1.4.0/references/settings.md:407:> Default: `false`
content/docs/1.4.0/references/settings.md:412:> Default: `ignored`
content/docs/1.4.0/references/settings.md:424:> Default: `true`
content/docs/1.4.0/references/settings.md:428:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.0/references/settings.md:432:> Default: `true`
content/docs/1.4.0/references/settings.md:440:> Default: `false`
content/docs/1.4.0/references/settings.md:448:> Default: `true`
content/docs/1.4.0/references/settings.md:454:> **Note:**
content/docs/1.4.0/references/settings.md:455:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.0/references/settings.md:456:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.0/references/settings.md:460:> Default: `disabled`
content/docs/1.4.0/references/settings.md:476:> Default: `ignored`
content/docs/1.4.0/references/settings.md:490:> Default: `25`
content/docs/1.4.0/references/settings.md:498:> Default: `200`
content/docs/1.4.0/references/settings.md:510:> Default: `5`
content/docs/1.4.0/references/settings.md:516:> **WARNING:**
content/docs/1.4.0/references/settings.md:517:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.0/references/settings.md:518:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.0/references/settings.md:519:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.0/references/settings.md:524:> Default: `12`
content/docs/1.4.0/references/settings.md:536:> **Warning:**
content/docs/1.4.0/references/settings.md:537:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.0/references/settings.md:538:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.0/references/settings.md:539:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.0/references/settings.md:540:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.0/references/settings.md:541:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.0/references/settings.md:545:> Default: `12`
content/docs/1.4.0/references/settings.md:557:> **Warning:**
content/docs/1.4.0/references/settings.md:558:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.0/references/settings.md:559:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.0/references/settings.md:560:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.0/references/settings.md:561:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.0/references/settings.md:562:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.0/references/settings.md:567:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.0/references/settings.md:586:> Example: `high-priority`
content/docs/1.4.0/references/settings.md:597:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.0/references/settings.md:603:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.0/references/settings.md:610:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.0/references/settings.md:616:> Default: `false`
content/docs/1.4.0/references/settings.md:622:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.0/references/settings.md:626:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.0/references/settings.md:630:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.0/references/settings.md:636:> Example: `false`
content/docs/1.4.0/references/settings.md:650:> Default: `false`
content/docs/1.4.0/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.0/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.0/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.0/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.0/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.0/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.0/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.0/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.0/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.0/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.0/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.0/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.0/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.0/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.0/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.0/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.0/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.0/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.0/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.0/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.0/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.0/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.0/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.0/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.0/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.0/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.0/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.0/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.0/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.0/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.0/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.0/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.0/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.0/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.0/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.0/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.0/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.0/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.0/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.0/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.0/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.0/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.0/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.0/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.0/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.0/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.0/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.0/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.0/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.0/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:47:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:48:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:       kubectl create secret generic <aws-secret> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:59:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:60:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:84:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:85:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:108:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:119:       kubectl create secret generic <aws-secret> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:120:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:127:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:133:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:kubectl create secret generic <aws-secret> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:205:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/1.4.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.0/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.4.0/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.4.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.0/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.4.0/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.4.0/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.4.0/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.4.0/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.4.0/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.4.0/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.4.0/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.4.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.0/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.0/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.4.0/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.0/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.4.0/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.4.0/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.0/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.4.0/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.4.0/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.4.0/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.0/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.4.0/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/1.4.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.0/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.4.0/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:28:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:32:Users need to figure out the mount point of the volume then manually execute `fstrim <the mount point>`.
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:56:    1. Figure out and enter into the share manager pod of the RWX volume, which actually contains the NFS server. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:58:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:60:    2. Figure out the work directory of the NFS server. The work directory is typically like `/export/<volume name>`:
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:62:    mount | grep /dev/longhorn/<volume name>
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:63:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:67:    fstrim /export/<volume name>
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:77:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:79:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.4.0/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.0/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.4.0/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.4.0/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.4.0/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.4.0/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.4.0/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.4.0/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.4.0/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.0/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.1/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.4.1/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.4.1/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.4.1/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.4.1/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.4.1/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.4.1/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.4.1 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.4.1/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.4.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.4.1`
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.4.1/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.1/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.1/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.4.1/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.1/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.4.1/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.4.1/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.4.1/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.4.1/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.4.1/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.4.1/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.4.1/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:173:In v1.4.1, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.4.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.1/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.1/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.1/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.1/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.1/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:173:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:179:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:184:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:193:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:196:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:200:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:48:   >
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:49:   > For example:
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:50:   > ```yaml
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:51:   > defaultSettings:
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:52:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:53:   >  ```
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:99:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.4.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.1/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.4.1/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.1/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.4.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.1/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:65:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:66:   >
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:67:   > For example:
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:68:   > ```yaml
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:69:   > defaultSettings:
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:70:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:71:   >  ```
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:135:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.1/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.4.1/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.4.1/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.4.1/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.4.1/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.4.1/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.4.1/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.4.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.4.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.4.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.4.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.1/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.4.1/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.4.1/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.4.1/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.4.1/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.4.1/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.1/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.4.1/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.1/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.1/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.1/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.1/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.1/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.1/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:55:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:59:  > **Note:** Longhorn system restores volume with the latest backup. The system backup does not trigger volume backup. We suggest updating all volumes' last backup. Taking volume backups ensures the data is up-to-date with the system backup.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:80:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:94:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:100:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:104:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.1/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.4.1/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.4.1/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.4.1/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.4.1/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.4.1/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.1/best-practices.md:123:> Recommend: `false`
content/docs/1.4.1/best-practices.md:129:> Recommend: `false`
content/docs/1.4.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.1/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.4.1/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.4.1/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.4.1/deploy/important-notes/index.md:18:When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to v1.4.1, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890).
content/docs/1.4.1/deploy/important-notes/index.md:32:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.4.1/deploy/important-notes/index.md:35:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.1/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.4.1/deploy/install/_index.md:149:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.4.1/deploy/install/_index.md:254:The `Server Version` should be >= v1.21.
content/docs/1.4.1/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.4.1/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.1/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.1/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.1/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.1/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.1/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.1/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.1/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.1/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.1/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.1/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.4.1/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.4.1/deploy/upgrade/_index.md:18:- To upgrade from v1.4.x (< v1.4.1) or v1.3.x, see [this section.](./longhorn-manager)
content/docs/1.4.1/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.4.x (< v1.4.1) or v1.3.x
content/docs/1.4.1/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.4.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.1/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.1/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.4.x (< v1.4.1) or v1.3.x to v1.4.1.
content/docs/1.4.1/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.4.1/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.4.1/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.4.1/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.4.1/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.4.1/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.4.1/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.4.1/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.4.1/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.4.1/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.4.1/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.4.1/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.4.1/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.4.1/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.4.1/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.4.1/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.1/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.1/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.1/references/settings.md:83:> Default: `false`
content/docs/1.4.1/references/settings.md:91:> Default: `true`
content/docs/1.4.1/references/settings.md:97:> Default: `true`
content/docs/1.4.1/references/settings.md:104:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.1/references/settings.md:105:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.1/references/settings.md:106:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.1/references/settings.md:110:> Default: `true`
content/docs/1.4.1/references/settings.md:116:> Default: `0`
content/docs/1.4.1/references/settings.md:124:> Default: `5`
content/docs/1.4.1/references/settings.md:134:> Default: `false`
content/docs/1.4.1/references/settings.md:144:> Default: `longhorn.io/v1beta2`
content/docs/1.4.1/references/settings.md:150:> Default: `disabled`
content/docs/1.4.1/references/settings.md:168:> Default: `/var/lib/longhorn/`
content/docs/1.4.1/references/settings.md:186:> Default: `longhorn-static`
content/docs/1.4.1/references/settings.md:192:> Default: `3`
content/docs/1.4.1/references/settings.md:207:> Default: `false`
content/docs/1.4.1/references/settings.md:211:> Default: `false`
content/docs/1.4.1/references/settings.md:217:> Default: `true`
content/docs/1.4.1/references/settings.md:229:> Default: `do-nothing`
content/docs/1.4.1/references/settings.md:244:> Default: `600`
content/docs/1.4.1/references/settings.md:252:> Default: `if-not-present`
content/docs/1.4.1/references/settings.md:268:> Default: `60`
content/docs/1.4.1/references/settings.md:273:> Default: `300`
content/docs/1.4.1/references/settings.md:277:> **Note:**
content/docs/1.4.1/references/settings.md:278:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.1/references/settings.md:279:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.1/references/settings.md:282:> Default: `8`
content/docs/1.4.1/references/settings.md:293:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.1/references/settings.md:297:> Default: `1`
content/docs/1.4.1/references/settings.md:307:> Default: `false`
content/docs/1.4.1/references/settings.md:313:> Default: `30`
content/docs/1.4.1/references/settings.md:321:> Default: `fast-check`
content/docs/1.4.1/references/settings.md:330:> Default: `false`
content/docs/1.4.1/references/settings.md:336:> Default: `0 0 */7 * *`
content/docs/1.4.1/references/settings.md:339:> **Warning**
content/docs/1.4.1/references/settings.md:340:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.1/references/settings.md:346:> Default: `false`
content/docs/1.4.1/references/settings.md:354:> Default: `false`
content/docs/1.4.1/references/settings.md:358:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.1/references/settings.md:362:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.1/references/settings.md:368:> Example: `s3-secret`
content/docs/1.4.1/references/settings.md:374:> Default: `300`
content/docs/1.4.1/references/settings.md:382:> Default: `1440`
content/docs/1.4.1/references/settings.md:390:> Default: `1`
content/docs/1.4.1/references/settings.md:399:> Default: `1`
content/docs/1.4.1/references/settings.md:407:> Default: `false`
content/docs/1.4.1/references/settings.md:412:> Default: `ignored`
content/docs/1.4.1/references/settings.md:424:> Default: `true`
content/docs/1.4.1/references/settings.md:428:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.1/references/settings.md:432:> Default: `true`
content/docs/1.4.1/references/settings.md:440:> Default: `false`
content/docs/1.4.1/references/settings.md:448:> Default: `true`
content/docs/1.4.1/references/settings.md:454:> **Note:**
content/docs/1.4.1/references/settings.md:455:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.1/references/settings.md:456:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.1/references/settings.md:460:> Default: `disabled`
content/docs/1.4.1/references/settings.md:476:> Default: `ignored`
content/docs/1.4.1/references/settings.md:490:> Default: `25`
content/docs/1.4.1/references/settings.md:498:> Default: `200`
content/docs/1.4.1/references/settings.md:510:> Default: `5`
content/docs/1.4.1/references/settings.md:516:> **WARNING:**
content/docs/1.4.1/references/settings.md:517:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.1/references/settings.md:518:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.1/references/settings.md:519:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.1/references/settings.md:524:> Default: `12`
content/docs/1.4.1/references/settings.md:536:> **Warning:**
content/docs/1.4.1/references/settings.md:537:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.1/references/settings.md:538:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.1/references/settings.md:539:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.1/references/settings.md:540:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.1/references/settings.md:541:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.1/references/settings.md:545:> Default: `12`
content/docs/1.4.1/references/settings.md:557:> **Warning:**
content/docs/1.4.1/references/settings.md:558:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.1/references/settings.md:559:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.1/references/settings.md:560:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.1/references/settings.md:561:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.1/references/settings.md:562:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.1/references/settings.md:567:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.1/references/settings.md:586:> Example: `high-priority`
content/docs/1.4.1/references/settings.md:597:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.1/references/settings.md:603:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.1/references/settings.md:610:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.1/references/settings.md:616:> Default: `false`
content/docs/1.4.1/references/settings.md:622:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.1/references/settings.md:626:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.1/references/settings.md:630:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.1/references/settings.md:636:> Example: `false`
content/docs/1.4.1/references/settings.md:650:> Default: `false`
content/docs/1.4.1/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.1/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.1/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.1/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.1/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.1/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.1/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.1/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.1/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.1/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.1/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.1/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.1/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.1/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.1/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.1/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.1/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.1/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.1/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.1/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.1/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.1/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.1/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.1/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.1/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.1/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.1/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.1/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.1/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.1/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.1/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.1/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.1/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.1/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.1/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.1/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.1/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.1/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.1/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.1/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.1/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.1/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.1/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.1/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.1/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.1/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.1/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.1/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.1/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.1/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:47:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:48:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:       kubectl create secret generic <aws-secret> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:59:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:60:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:84:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:85:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:108:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:119:       kubectl create secret generic <aws-secret> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:120:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:127:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:133:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:kubectl create secret generic <aws-secret> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:205:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:71:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:73:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:74:    >
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:75:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > - `snapshot` with retain value set to 5
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:77:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:78:    >
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:105:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:110:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:115:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/1.4.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.1/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.4.1/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.4.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.1/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.4.1/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.4.1/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.4.1/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.4.1/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.4.1/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.4.1/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.4.1/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.4.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.1/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.1/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.4.1/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.1/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.4.1/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.4.1/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.1/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.4.1/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.4.1/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.4.1/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.1/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.4.1/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/1.4.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.1/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.4.1/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:31:Users need to figure out the mount point of the volume then manually execute `fstrim <the mount point>`.
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:55:    1. Figure out and enter into the share manager pod of the RWX volume, which actually contains the NFS server. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:57:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:59:    2. Figure out the work directory of the NFS server. The work directory is typically like `/export/<volume name>`:
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:61:    mount | grep /dev/longhorn/<volume name>
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:62:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:66:    fstrim /export/<volume name>
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:76:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:78:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.4.1/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.1/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.4.1/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.4.1/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.4.1/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.4.1/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.4.1/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.4.1/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.4.1/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.1/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.2/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.4.2/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.4.2/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.4.2/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.4.2/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.4.2/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.4.2/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.4.2 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.4.2/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.4.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.4.2`
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.4.2/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.2/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.2/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.4.2/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.2/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.4.2/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.4.2/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.4.2/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.4.2/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.4.2/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.4.2/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.4.2/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:173:In v1.4.2, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.4.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.2/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.2/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.2/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.2/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.2/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:173:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:179:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:184:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:193:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:196:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:200:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:48:   >
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:49:   > For example:
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:50:   > ```yaml
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:51:   > defaultSettings:
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:52:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:53:   >  ```
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:99:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.4.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.2/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.4.2/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.2/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.4.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.2/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:65:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:66:   >
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:67:   > For example:
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:68:   > ```yaml
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:69:   > defaultSettings:
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:70:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:71:   >  ```
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:135:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.2/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.4.2/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.4.2/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.4.2/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.4.2/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.4.2/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.4.2/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.4.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.4.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.4.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.4.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.2/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.4.2/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.4.2/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.4.2/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.4.2/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.4.2/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.2/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.4.2/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.2/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.2/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.2/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.2/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.2/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.2/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:55:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:59:  > **Note:** Longhorn system restores volume with the latest backup. The system backup does not trigger volume backup. We suggest updating all volumes' last backup. Taking volume backups ensures the data is up-to-date with the system backup.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:80:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:94:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:100:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:104:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.2/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.4.2/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.4.2/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.4.2/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.4.2/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.4.2/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.2/best-practices.md:123:> Recommend: `false`
content/docs/1.4.2/best-practices.md:129:> Recommend: `false`
content/docs/1.4.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.2/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.4.2/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.4.2/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.4.2/deploy/important-notes/index.md:20:When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to v1.4.2, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890).
content/docs/1.4.2/deploy/important-notes/index.md:34:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.4.2/deploy/important-notes/index.md:37:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.2/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.2/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.2/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.4.2/deploy/install/_index.md:149:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.4.2/deploy/install/_index.md:254:The `Server Version` should be >= v1.21.
content/docs/1.4.2/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.4.2/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.2/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.2/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.2/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.2/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.2/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.2/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.2/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.2/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.2/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.2/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.4.2/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.4.2/deploy/upgrade/_index.md:18:- To upgrade from v1.4.x (< v1.4.2) or v1.3.x, see [this section.](./longhorn-manager)
content/docs/1.4.2/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.4.x (< v1.4.2) or v1.3.x
content/docs/1.4.2/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.4.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.2/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.2/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.4.x (< v1.4.2) or v1.3.x to v1.4.2.
content/docs/1.4.2/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.4.2/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.4.2/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.4.2/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.4.2/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.4.2/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.4.2/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.4.2/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.4.2/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.4.2/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.4.2/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.4.2/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.4.2/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.4.2/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.4.2/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.4.2/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.2/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.2/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.2/references/settings.md:84:> Default: `block-if-contains-last-replica`
content/docs/1.4.2/references/settings.md:95:> Default: `true`
content/docs/1.4.2/references/settings.md:101:> Default: `true`
content/docs/1.4.2/references/settings.md:108:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.2/references/settings.md:109:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.2/references/settings.md:110:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.2/references/settings.md:114:> Default: `true`
content/docs/1.4.2/references/settings.md:120:> Default: `0`
content/docs/1.4.2/references/settings.md:128:> Default: `5`
content/docs/1.4.2/references/settings.md:138:> Default: `false`
content/docs/1.4.2/references/settings.md:148:> Default: `longhorn.io/v1beta2`
content/docs/1.4.2/references/settings.md:154:> Default: `disabled`
content/docs/1.4.2/references/settings.md:172:> Default: `/var/lib/longhorn/`
content/docs/1.4.2/references/settings.md:190:> Default: `longhorn-static`
content/docs/1.4.2/references/settings.md:196:> Default: `3`
content/docs/1.4.2/references/settings.md:211:> Default: `false`
content/docs/1.4.2/references/settings.md:215:> Default: `false`
content/docs/1.4.2/references/settings.md:221:> Default: `true`
content/docs/1.4.2/references/settings.md:233:> Default: `do-nothing`
content/docs/1.4.2/references/settings.md:248:> Default: `600`
content/docs/1.4.2/references/settings.md:256:> Default: `if-not-present`
content/docs/1.4.2/references/settings.md:272:> Default: `60`
content/docs/1.4.2/references/settings.md:277:> Default: `300`
content/docs/1.4.2/references/settings.md:281:> **Note:**
content/docs/1.4.2/references/settings.md:282:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.2/references/settings.md:283:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.2/references/settings.md:286:> Default: `8`
content/docs/1.4.2/references/settings.md:297:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.2/references/settings.md:301:> Default: `1`
content/docs/1.4.2/references/settings.md:311:> Default: `false`
content/docs/1.4.2/references/settings.md:317:> Default: `30`
content/docs/1.4.2/references/settings.md:325:> Default: `fast-check`
content/docs/1.4.2/references/settings.md:334:> Default: `false`
content/docs/1.4.2/references/settings.md:340:> Default: `0 0 */7 * *`
content/docs/1.4.2/references/settings.md:343:> **Warning**
content/docs/1.4.2/references/settings.md:344:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.2/references/settings.md:350:> Default: `false`
content/docs/1.4.2/references/settings.md:358:> Default: `false`
content/docs/1.4.2/references/settings.md:362:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.2/references/settings.md:366:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.2/references/settings.md:372:> Example: `s3-secret`
content/docs/1.4.2/references/settings.md:378:> Default: `300`
content/docs/1.4.2/references/settings.md:386:> Default: `1440`
content/docs/1.4.2/references/settings.md:394:> Default: `1`
content/docs/1.4.2/references/settings.md:403:> Default: `1`
content/docs/1.4.2/references/settings.md:411:> Default: `false`
content/docs/1.4.2/references/settings.md:416:> Default: `ignored`
content/docs/1.4.2/references/settings.md:428:> Default: `true`
content/docs/1.4.2/references/settings.md:432:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.2/references/settings.md:436:> Default: `true`
content/docs/1.4.2/references/settings.md:444:> Default: `false`
content/docs/1.4.2/references/settings.md:452:> Default: `true`
content/docs/1.4.2/references/settings.md:458:> **Note:**
content/docs/1.4.2/references/settings.md:459:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.2/references/settings.md:460:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.2/references/settings.md:464:> Default: `disabled`
content/docs/1.4.2/references/settings.md:480:> Default: `ignored`
content/docs/1.4.2/references/settings.md:494:> Default: `25`
content/docs/1.4.2/references/settings.md:502:> Default: `200`
content/docs/1.4.2/references/settings.md:514:> Default: `5`
content/docs/1.4.2/references/settings.md:520:> **WARNING:**
content/docs/1.4.2/references/settings.md:521:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.2/references/settings.md:522:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.2/references/settings.md:523:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.2/references/settings.md:528:> Default: `12`
content/docs/1.4.2/references/settings.md:540:> **Warning:**
content/docs/1.4.2/references/settings.md:541:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.2/references/settings.md:542:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.2/references/settings.md:543:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.2/references/settings.md:544:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.2/references/settings.md:545:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.2/references/settings.md:549:> Default: `12`
content/docs/1.4.2/references/settings.md:561:> **Warning:**
content/docs/1.4.2/references/settings.md:562:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.2/references/settings.md:563:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.2/references/settings.md:564:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.2/references/settings.md:565:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.2/references/settings.md:566:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.2/references/settings.md:571:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.2/references/settings.md:590:> Example: `high-priority`
content/docs/1.4.2/references/settings.md:601:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.2/references/settings.md:607:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.2/references/settings.md:614:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.2/references/settings.md:620:> Default: `false`
content/docs/1.4.2/references/settings.md:626:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.2/references/settings.md:630:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.2/references/settings.md:634:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.2/references/settings.md:640:> Example: `false`
content/docs/1.4.2/references/settings.md:654:> Default: `false`
content/docs/1.4.2/references/settings.md:666:> Default: `false`
content/docs/1.4.2/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.2/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.2/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.2/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.2/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.2/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.2/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.2/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.2/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.2/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.2/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.2/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.2/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.2/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.2/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.2/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.2/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.2/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.2/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.2/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.2/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.2/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.2/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.2/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.2/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.2/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.2/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.2/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.2/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.2/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.2/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.2/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.2/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.2/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.2/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.2/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.2/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.2/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.2/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.2/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.2/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.2/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.2/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.2/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.2/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.2/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.2/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.2/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.2/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.2/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:47:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:48:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:       kubectl create secret generic <aws-secret> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:59:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:60:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:84:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:85:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:108:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:119:       kubectl create secret generic <aws-secret> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:120:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:127:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:133:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:kubectl create secret generic <aws-secret> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:205:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.2/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.2/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.2/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:73:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:75:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    >
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:77:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:78:    > - `snapshot` with retain value set to 5
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:107:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:112:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:117:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/1.4.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.2/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.4.2/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.4.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.2/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.4.2/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.4.2/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.4.2/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.4.2/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.4.2/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.4.2/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.4.2/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.4.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.2/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.2/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.4.2/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.2/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.4.2/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.4.2/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.2/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.4.2/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.4.2/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.4.2/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.2/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.4.2/volumes-and-nodes/maintenance.md:27:   > **Note:**
content/docs/1.4.2/volumes-and-nodes/maintenance.md:28:   > By default, if there is one last healthy replica for a volume on
content/docs/1.4.2/volumes-and-nodes/maintenance.md:29:   > the node, Longhorn will prevent the node from completing the drain
content/docs/1.4.2/volumes-and-nodes/maintenance.md:30:   > operation, to protect the last replica and prevent the disruption of the
content/docs/1.4.2/volumes-and-nodes/maintenance.md:31:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/1.4.2/volumes-and-nodes/maintenance.md:32:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/1.4.2/volumes-and-nodes/maintenance.md:35:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/1.4.2/volumes-and-nodes/maintenance.md:36:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/1.4.2/volumes-and-nodes/maintenance.md:37:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/1.4.2/volumes-and-nodes/maintenance.md:38:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/1.4.2/volumes-and-nodes/maintenance.md:42:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/1.4.2/volumes-and-nodes/maintenance.md:43:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/1.4.2/volumes-and-nodes/maintenance.md:44:   > can be reused later, once the node back online and uncordoned.
content/docs/1.4.2/volumes-and-nodes/maintenance.md:77:        kubectl delete node <node-name>
content/docs/1.4.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.2/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.4.2/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:31:Users need to figure out the mount point of the volume then manually execute `fstrim <the mount point>`.
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:55:    1. Figure out and enter into the share manager pod of the RWX volume, which actually contains the NFS server. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:57:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:59:    2. Figure out the work directory of the NFS server. The work directory is typically like `/export/<volume name>`:
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:61:    mount | grep /dev/longhorn/<volume name>
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:62:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:66:    fstrim /export/<volume name>
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:76:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:78:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.4.2/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.2/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.4.2/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.4.2/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.4.2/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.4.2/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.4.2/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.4.2/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.4.2/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.2/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.3/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.4.3/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.4.3/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.4.3/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.4.3/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.4.3/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.4.3/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.4.3 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.4.3/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.4.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.4.3`
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.4.3/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.3/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.3/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.4.3/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.3/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.4.3/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.4.3/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.4.3/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.4.3/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.4.3/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.4.3/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.4.3/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:173:In v1.4.3, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.4.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.3/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.3/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.3/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.3/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.3/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:173:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:179:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:184:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:193:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:196:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:200:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:48:   >
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:49:   > For example:
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:50:   > ```yaml
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:51:   > defaultSettings:
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:52:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:53:   >  ```
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:99:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.4.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.3/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.4.3/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.3/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.4.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.3/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:65:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:66:   >
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:67:   > For example:
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:68:   > ```yaml
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:69:   > defaultSettings:
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:70:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:71:   >  ```
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:135:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.3/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.4.3/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.4.3/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.4.3/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.4.3/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.4.3/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.4.3/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.4.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.4.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.4.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.4.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.3/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.3/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.3/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.3/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.3/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.3/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.3/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:55:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:59:  > **Note:** Longhorn system restores volume with the latest backup. The system backup does not trigger volume backup. We suggest updating all volumes' last backup. Taking volume backups ensures the data is up-to-date with the system backup.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:80:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:94:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:100:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:104:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.3/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.4.3/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.4.3/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.4.3/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.4.3/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.4.3/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.3/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.4.3/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.4.3/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.4.3/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.4.3/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.4.3/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.3/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.4.3/best-practices.md:123:> Recommend: `false`
content/docs/1.4.3/best-practices.md:129:> Recommend: `false`
content/docs/1.4.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.3/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.4.3/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.4.3/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.4.3/deploy/important-notes/index.md:20:When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to v1.4.3, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890).
content/docs/1.4.3/deploy/important-notes/index.md:34:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.4.3/deploy/important-notes/index.md:37:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.3/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.3/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.3/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.4.3/deploy/install/_index.md:149:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.4.3/deploy/install/_index.md:254:The `Server Version` should be >= v1.21.
content/docs/1.4.3/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.4.3/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.3/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.3/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.3/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.3/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.3/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.3/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.3/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.3/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.3/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.3/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.4.3/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.4.3/deploy/upgrade/_index.md:18:- To upgrade from v1.4.x (< v1.4.3) or v1.3.x, see [this section.](./longhorn-manager)
content/docs/1.4.3/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.4.x (< v1.4.3) or v1.3.x
content/docs/1.4.3/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.4.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.3/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.3/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.4.x (< v1.4.3) or v1.3.x to v1.4.3.
content/docs/1.4.3/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.4.3/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.4.3/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.4.3/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.4.3/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.4.3/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.4.3/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.4.3/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.4.3/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.4.3/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.4.3/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.4.3/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.4.3/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.4.3/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.4.3/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.4.3/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.3/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.3/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.3/references/settings.md:84:> Default: `block-if-contains-last-replica`
content/docs/1.4.3/references/settings.md:95:> Default: `true`
content/docs/1.4.3/references/settings.md:101:> Default: `true`
content/docs/1.4.3/references/settings.md:108:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.3/references/settings.md:109:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.3/references/settings.md:110:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.3/references/settings.md:114:> Default: `true`
content/docs/1.4.3/references/settings.md:120:> Default: `0`
content/docs/1.4.3/references/settings.md:128:> Default: `5`
content/docs/1.4.3/references/settings.md:138:> Default: `false`
content/docs/1.4.3/references/settings.md:148:> Default: `longhorn.io/v1beta2`
content/docs/1.4.3/references/settings.md:154:> Default: `disabled`
content/docs/1.4.3/references/settings.md:172:> Default: `/var/lib/longhorn/`
content/docs/1.4.3/references/settings.md:190:> Default: `longhorn-static`
content/docs/1.4.3/references/settings.md:196:> Default: `3`
content/docs/1.4.3/references/settings.md:211:> Default: `false`
content/docs/1.4.3/references/settings.md:215:> Default: `false`
content/docs/1.4.3/references/settings.md:221:> Default: `true`
content/docs/1.4.3/references/settings.md:233:> Default: `do-nothing`
content/docs/1.4.3/references/settings.md:248:> Default: `600`
content/docs/1.4.3/references/settings.md:256:> Default: `if-not-present`
content/docs/1.4.3/references/settings.md:272:> Default: `60`
content/docs/1.4.3/references/settings.md:277:> Default: `300`
content/docs/1.4.3/references/settings.md:281:> **Note:**
content/docs/1.4.3/references/settings.md:282:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.3/references/settings.md:283:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.3/references/settings.md:286:> Default: `8`
content/docs/1.4.3/references/settings.md:297:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.3/references/settings.md:301:> Default: `1`
content/docs/1.4.3/references/settings.md:311:> Default: `false`
content/docs/1.4.3/references/settings.md:317:> Default: `30`
content/docs/1.4.3/references/settings.md:325:> Default: `fast-check`
content/docs/1.4.3/references/settings.md:334:> Default: `false`
content/docs/1.4.3/references/settings.md:340:> Default: `0 0 */7 * *`
content/docs/1.4.3/references/settings.md:343:> **Warning**
content/docs/1.4.3/references/settings.md:344:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.3/references/settings.md:350:> Default: `false`
content/docs/1.4.3/references/settings.md:358:> Default: `false`
content/docs/1.4.3/references/settings.md:362:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.3/references/settings.md:366:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.3/references/settings.md:372:> Example: `s3-secret`
content/docs/1.4.3/references/settings.md:378:> Default: `300`
content/docs/1.4.3/references/settings.md:386:> Default: `1440`
content/docs/1.4.3/references/settings.md:394:> Default: `1`
content/docs/1.4.3/references/settings.md:403:> Default: `1`
content/docs/1.4.3/references/settings.md:411:> Default: `false`
content/docs/1.4.3/references/settings.md:416:> Default: `ignored`
content/docs/1.4.3/references/settings.md:428:> Default: `true`
content/docs/1.4.3/references/settings.md:432:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.3/references/settings.md:436:> Default: `true`
content/docs/1.4.3/references/settings.md:444:> Default: `false`
content/docs/1.4.3/references/settings.md:452:> Default: `true`
content/docs/1.4.3/references/settings.md:458:> **Note:**
content/docs/1.4.3/references/settings.md:459:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.3/references/settings.md:460:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.3/references/settings.md:464:> Default: `disabled`
content/docs/1.4.3/references/settings.md:480:> Default: `ignored`
content/docs/1.4.3/references/settings.md:494:> Default: `25`
content/docs/1.4.3/references/settings.md:502:> Default: `200`
content/docs/1.4.3/references/settings.md:514:> Default: `5`
content/docs/1.4.3/references/settings.md:520:> **WARNING:**
content/docs/1.4.3/references/settings.md:521:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.3/references/settings.md:522:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.3/references/settings.md:523:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.3/references/settings.md:528:> Default: `12`
content/docs/1.4.3/references/settings.md:540:> **Warning:**
content/docs/1.4.3/references/settings.md:541:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.3/references/settings.md:542:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.3/references/settings.md:543:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.3/references/settings.md:544:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.3/references/settings.md:545:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.3/references/settings.md:549:> Default: `12`
content/docs/1.4.3/references/settings.md:561:> **Warning:**
content/docs/1.4.3/references/settings.md:562:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.3/references/settings.md:563:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.3/references/settings.md:564:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.3/references/settings.md:565:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.3/references/settings.md:566:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.3/references/settings.md:571:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.3/references/settings.md:590:> Example: `high-priority`
content/docs/1.4.3/references/settings.md:601:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.3/references/settings.md:607:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.3/references/settings.md:614:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.3/references/settings.md:620:> Default: `false`
content/docs/1.4.3/references/settings.md:626:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.3/references/settings.md:630:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.3/references/settings.md:634:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.3/references/settings.md:640:> Example: `false`
content/docs/1.4.3/references/settings.md:654:> Default: `false`
content/docs/1.4.3/references/settings.md:666:> Default: `false`
content/docs/1.4.3/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.3/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.3/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.3/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.3/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.3/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.3/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.3/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.3/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.3/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.3/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.3/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.3/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.3/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.3/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.3/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.3/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.3/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.3/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.3/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.3/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.3/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.3/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.3/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.3/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.3/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.3/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.3/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.3/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.3/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.3/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.3/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.3/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.3/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.3/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.3/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.3/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.3/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.3/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.3/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.3/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.3/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.3/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.3/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.3/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.3/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.3/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.3/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.3/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.3/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:47:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:48:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:       kubectl create secret generic <aws-secret> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:59:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:60:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:84:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:85:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:108:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:119:       kubectl create secret generic <aws-secret> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:120:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:127:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:133:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:kubectl create secret generic <aws-secret> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:205:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.3/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.3/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.3/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:73:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:75:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    >
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:77:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:78:    > - `snapshot` with retain value set to 5
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:107:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:112:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:117:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/1.4.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.3/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.4.3/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.4.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.3/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.4.3/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.4.3/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.4.3/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.4.3/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.4.3/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.4.3/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.4.3/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.4.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.3/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.3/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.4.3/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.3/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.4.3/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.4.3/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.3/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.4.3/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.4.3/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.4.3/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.3/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.4.3/volumes-and-nodes/maintenance.md:27:   > **Note:**
content/docs/1.4.3/volumes-and-nodes/maintenance.md:28:   > By default, if there is one last healthy replica for a volume on
content/docs/1.4.3/volumes-and-nodes/maintenance.md:29:   > the node, Longhorn will prevent the node from completing the drain
content/docs/1.4.3/volumes-and-nodes/maintenance.md:30:   > operation, to protect the last replica and prevent the disruption of the
content/docs/1.4.3/volumes-and-nodes/maintenance.md:31:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/1.4.3/volumes-and-nodes/maintenance.md:32:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/1.4.3/volumes-and-nodes/maintenance.md:35:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/1.4.3/volumes-and-nodes/maintenance.md:36:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/1.4.3/volumes-and-nodes/maintenance.md:37:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/1.4.3/volumes-and-nodes/maintenance.md:38:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/1.4.3/volumes-and-nodes/maintenance.md:42:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/1.4.3/volumes-and-nodes/maintenance.md:43:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/1.4.3/volumes-and-nodes/maintenance.md:44:   > can be reused later, once the node back online and uncordoned.
content/docs/1.4.3/volumes-and-nodes/maintenance.md:77:        kubectl delete node <node-name>
content/docs/1.4.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.3/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.4.3/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:31:Users need to figure out the mount point of the volume then manually execute `fstrim <the mount point>`.
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:55:    1. Figure out and enter into the share manager pod of the RWX volume, which actually contains the NFS server. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:57:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:59:    2. Figure out the work directory of the NFS server. The work directory is typically like `/export/<volume name>`:
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:61:    mount | grep /dev/longhorn/<volume name>
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:62:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:66:    fstrim /export/<volume name>
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:76:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:78:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.4.3/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.3/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.4.3/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.4.3/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.4.3/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.4.3/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.4.3/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.4.3/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.4.3/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.3/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.4/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.4.4/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.4.4/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.4.4/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.4.4/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.4.4/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.4.4/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.4.4 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.4.4/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.4.4/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.4.4`
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.4.4/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.4/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.4/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.4.4/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.4/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.4.4/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.4.4/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.4.4/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.4.4/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.4.4/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.4.4/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.4.4/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:173:In v1.4.4, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.4.4/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.4/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.4/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.4/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.4/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.4/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:173:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:179:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:184:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:193:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:196:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:200:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:48:   >
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:49:   > For example:
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:50:   > ```yaml
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:51:   > defaultSettings:
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:52:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:53:   >  ```
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:99:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.4.4/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.4/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.4/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.4/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.4.4/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.4/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.4.4/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.4/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:65:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:66:   >
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:67:   > For example:
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:68:   > ```yaml
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:69:   > defaultSettings:
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:70:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:71:   >  ```
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:135:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.4/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.4/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.4.4/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.4.4/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.4.4/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.4.4/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.4.4/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.4.4/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.4.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.4.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.4.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.4.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.4/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.4/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.4/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.4/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.4/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.4/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.4/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:55:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:59:  > **Note:** Longhorn system restores volume with the latest backup. The system backup does not trigger volume backup. We suggest updating all volumes' last backup. Taking volume backups ensures the data is up-to-date with the system backup.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:80:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:94:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:100:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:104:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.4/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.4.4/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.4.4/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.4.4/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.4.4/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.4.4/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.4/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.4.4/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.4.4/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.4.4/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.4.4/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.4.4/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.4/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.4.4/advanced-resources/rwx-workloads.md:36:> **Notice**  
content/docs/1.4.4/advanced-resources/rwx-workloads.md:37:> An RWX volume must have the access mode set to `ReadWriteMany` and the "migratable" flag disabled (*parameters.migratable: `false`*).
content/docs/1.4.4/advanced-resources/rwx-workloads.md:72:> **Important:**
content/docs/1.4.4/advanced-resources/rwx-workloads.md:73:> To create PVCs for RWX volumes using the sample StorageClass, replace the `nfsOptions` string with a customized comma-separated list of legal options.
content/docs/1.4.4/best-practices.md:123:> Recommend: `false`
content/docs/1.4.4/best-practices.md:129:> Recommend: `false`
content/docs/1.4.4/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.4/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.4.4/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.4.4/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.4.4/deploy/important-notes/index.md:20:When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to v1.4.4, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890).
content/docs/1.4.4/deploy/important-notes/index.md:34:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.4.4/deploy/important-notes/index.md:37:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.4/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.4/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.4/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.4.4/deploy/install/_index.md:151:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.4.4/deploy/install/_index.md:256:The `Server Version` should be >= v1.21.
content/docs/1.4.4/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.4.4/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.4/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.4/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.4/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.4/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.4/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.4/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.4/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.4/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.4/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.4/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.4.4/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.4.4/deploy/upgrade/_index.md:18:- To upgrade from v1.4.x (< v1.4.4) or v1.3.x, see [this section.](./longhorn-manager)
content/docs/1.4.4/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.4.x (< v1.4.4) or v1.3.x
content/docs/1.4.4/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.4.4/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.4/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.4/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.4.x (< v1.4.4) or v1.3.x to v1.4.4.
content/docs/1.4.4/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.4.4/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.4.4/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.4.4/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.4.4/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.4.4/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.4.4/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.4.4/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.4.4/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.4.4/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.4.4/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.4.4/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.4.4/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.4.4/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.4.4/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.4.4/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.4/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.4/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.4/references/settings.md:84:> Default: `block-if-contains-last-replica`
content/docs/1.4.4/references/settings.md:95:> Default: `true`
content/docs/1.4.4/references/settings.md:101:> Default: `true`
content/docs/1.4.4/references/settings.md:108:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.4/references/settings.md:109:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.4/references/settings.md:110:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.4/references/settings.md:114:> Default: `true`
content/docs/1.4.4/references/settings.md:120:> Default: `0`
content/docs/1.4.4/references/settings.md:128:> Default: `5`
content/docs/1.4.4/references/settings.md:138:> Default: `false`
content/docs/1.4.4/references/settings.md:148:> Default: `longhorn.io/v1beta2`
content/docs/1.4.4/references/settings.md:154:> Default: `disabled`
content/docs/1.4.4/references/settings.md:172:> Default: `/var/lib/longhorn/`
content/docs/1.4.4/references/settings.md:190:> Default: `longhorn-static`
content/docs/1.4.4/references/settings.md:196:> Default: `3`
content/docs/1.4.4/references/settings.md:211:> Default: `false`
content/docs/1.4.4/references/settings.md:215:> Default: `false`
content/docs/1.4.4/references/settings.md:221:> Default: `true`
content/docs/1.4.4/references/settings.md:233:> Default: `do-nothing`
content/docs/1.4.4/references/settings.md:248:> Default: `600`
content/docs/1.4.4/references/settings.md:256:> Default: `if-not-present`
content/docs/1.4.4/references/settings.md:272:> Default: `60`
content/docs/1.4.4/references/settings.md:277:> Default: `300`
content/docs/1.4.4/references/settings.md:281:> **Note:**
content/docs/1.4.4/references/settings.md:282:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.4/references/settings.md:283:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.4/references/settings.md:286:> Default: `8`
content/docs/1.4.4/references/settings.md:297:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.4/references/settings.md:301:> Default: `1`
content/docs/1.4.4/references/settings.md:311:> Default: `false`
content/docs/1.4.4/references/settings.md:317:> Default: `30`
content/docs/1.4.4/references/settings.md:325:> Default: `fast-check`
content/docs/1.4.4/references/settings.md:334:> Default: `false`
content/docs/1.4.4/references/settings.md:340:> Default: `0 0 */7 * *`
content/docs/1.4.4/references/settings.md:343:> **Warning**
content/docs/1.4.4/references/settings.md:344:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.4/references/settings.md:350:> Default: `false`
content/docs/1.4.4/references/settings.md:358:> Default: `false`
content/docs/1.4.4/references/settings.md:362:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.4/references/settings.md:366:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.4/references/settings.md:372:> Example: `s3-secret`
content/docs/1.4.4/references/settings.md:378:> Default: `300`
content/docs/1.4.4/references/settings.md:386:> Default: `1440`
content/docs/1.4.4/references/settings.md:394:> Default: `1`
content/docs/1.4.4/references/settings.md:403:> Default: `1`
content/docs/1.4.4/references/settings.md:411:> Default: `false`
content/docs/1.4.4/references/settings.md:416:> Default: `ignored`
content/docs/1.4.4/references/settings.md:428:> Default: `true`
content/docs/1.4.4/references/settings.md:432:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.4/references/settings.md:436:> Default: `true`
content/docs/1.4.4/references/settings.md:444:> Default: `false`
content/docs/1.4.4/references/settings.md:452:> Default: `true`
content/docs/1.4.4/references/settings.md:458:> **Note:**
content/docs/1.4.4/references/settings.md:459:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.4/references/settings.md:460:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.4/references/settings.md:464:> Default: `disabled`
content/docs/1.4.4/references/settings.md:480:> Default: `ignored`
content/docs/1.4.4/references/settings.md:494:> Default: `25`
content/docs/1.4.4/references/settings.md:502:> Default: `200`
content/docs/1.4.4/references/settings.md:514:> Default: `5`
content/docs/1.4.4/references/settings.md:520:> **WARNING:**
content/docs/1.4.4/references/settings.md:521:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.4/references/settings.md:522:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.4/references/settings.md:523:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.4/references/settings.md:528:> Default: `12`
content/docs/1.4.4/references/settings.md:540:> **Warning:**
content/docs/1.4.4/references/settings.md:541:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.4/references/settings.md:542:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.4/references/settings.md:543:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.4/references/settings.md:544:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.4/references/settings.md:545:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.4/references/settings.md:549:> Default: `12`
content/docs/1.4.4/references/settings.md:561:> **Warning:**
content/docs/1.4.4/references/settings.md:562:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.4/references/settings.md:563:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.4/references/settings.md:564:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.4/references/settings.md:565:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.4/references/settings.md:566:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.4/references/settings.md:571:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.4/references/settings.md:590:> Example: `high-priority`
content/docs/1.4.4/references/settings.md:601:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.4/references/settings.md:607:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.4/references/settings.md:614:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.4/references/settings.md:620:> Default: `false`
content/docs/1.4.4/references/settings.md:626:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.4/references/settings.md:630:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.4/references/settings.md:634:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.4/references/settings.md:640:> Example: `false`
content/docs/1.4.4/references/settings.md:654:> Default: `false`
content/docs/1.4.4/references/settings.md:666:> Default: `false`
content/docs/1.4.4/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.4/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.4/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.4/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.4/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.4/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.4/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.4/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.4/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.4/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.4/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.4/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.4/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.4/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.4/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.4/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.4/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.4/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.4/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.4/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.4/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.4/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.4/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.4/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.4/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.4/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.4/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.4/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.4/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.4/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.4/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.4/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.4/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.4/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.4/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.4/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.4/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.4/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.4/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.4/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.4/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.4/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.4/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.4/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.4/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.4/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.4/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.4/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.4/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.4/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:47:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:48:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:       kubectl create secret generic <aws-secret> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:59:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:60:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:84:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:85:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:108:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:119:       kubectl create secret generic <aws-secret> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:120:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:127:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:133:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:kubectl create secret generic <aws-secret> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:205:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.4/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.4/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.4/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:73:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:75:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    >
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:77:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:78:    > - `snapshot` with retain value set to 5
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:107:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:112:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:117:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/1.4.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.4/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.4/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.4.4/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.4.4/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.4/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.4/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.4.4/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.4.4/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.4.4/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.4.4/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.4.4/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.4.4/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.4.4/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.4.4/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.4/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.4/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.4/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.4.4/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.4/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.4.4/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.4.4/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.4/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.4.4/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.4.4/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.4.4/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.4/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.4.4/volumes-and-nodes/maintenance.md:27:   > **Note:**
content/docs/1.4.4/volumes-and-nodes/maintenance.md:28:   > By default, if there is one last healthy replica for a volume on
content/docs/1.4.4/volumes-and-nodes/maintenance.md:29:   > the node, Longhorn will prevent the node from completing the drain
content/docs/1.4.4/volumes-and-nodes/maintenance.md:30:   > operation, to protect the last replica and prevent the disruption of the
content/docs/1.4.4/volumes-and-nodes/maintenance.md:31:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/1.4.4/volumes-and-nodes/maintenance.md:32:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/1.4.4/volumes-and-nodes/maintenance.md:35:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/1.4.4/volumes-and-nodes/maintenance.md:36:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/1.4.4/volumes-and-nodes/maintenance.md:37:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/1.4.4/volumes-and-nodes/maintenance.md:38:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/1.4.4/volumes-and-nodes/maintenance.md:42:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/1.4.4/volumes-and-nodes/maintenance.md:43:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/1.4.4/volumes-and-nodes/maintenance.md:44:   > can be reused later, once the node back online and uncordoned.
content/docs/1.4.4/volumes-and-nodes/maintenance.md:77:        kubectl delete node <node-name>
content/docs/1.4.4/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.4/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.4/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.4/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.4.4/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:31:Users need to figure out the mount point of the volume then manually execute `fstrim <the mount point>`.
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:55:    1. Figure out and enter into the share manager pod of the RWX volume, which actually contains the NFS server. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:57:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:59:    2. Figure out the work directory of the NFS server. The work directory is typically like `/export/<volume name>`:
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:61:    mount | grep /dev/longhorn/<volume name>
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:62:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:66:    fstrim /export/<volume name>
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:76:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:78:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.4.4/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.4/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.4.4/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.4.4/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.4.4/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.4.4/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.4.4/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.4.4/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.4.4/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.4/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.5/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.4.5/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.4.5/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.4.5/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.4.5/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.4.5/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.4.5/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.4.5 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.4.5/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.4.5/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.4.5`
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.4.5/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.5/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.5/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.4.5/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.5/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.4.5/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.4.5/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.4.5/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.4.5/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.4.5/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.4.5/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.4.5/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:173:In v1.4.5, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.4.5/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.5/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.5/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.4.5/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.4.5/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.4.5/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:173:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:179:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:184:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:193:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:196:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:200:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:48:   >
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:49:   > For example:
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:50:   > ```yaml
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:51:   > defaultSettings:
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:52:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:53:   >  ```
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:99:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.4.5/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.5/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.5/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.5/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.4.5/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.5/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.4.5/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.5/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:65:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:66:   >
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:67:   > For example:
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:68:   > ```yaml
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:69:   > defaultSettings:
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:70:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:71:   >  ```
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:135:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.5/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.5/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.4.5/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.4.5/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.4.5/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.4.5/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.4.5/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.4.5/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.4.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.4.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.4.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.4.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.5/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.5/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.5/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.5/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.5/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.5/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.5/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:55:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:59:  > **Note:** Longhorn system restores volume with the latest backup. The system backup does not trigger volume backup. We suggest updating all volumes' last backup. Taking volume backups ensures the data is up-to-date with the system backup.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:80:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:94:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:100:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:104:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.5/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.4.5/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.4.5/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.4.5/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.4.5/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.4.5/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.5/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.4.5/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.4.5/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.4.5/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.4.5/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.4.5/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.4.5/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.4.5/advanced-resources/rwx-workloads.md:36:> **Notice**  
content/docs/1.4.5/advanced-resources/rwx-workloads.md:37:> An RWX volume must have the access mode set to `ReadWriteMany` and the "migratable" flag disabled (*parameters.migratable: `false`*).
content/docs/1.4.5/advanced-resources/rwx-workloads.md:72:> **Important:**
content/docs/1.4.5/advanced-resources/rwx-workloads.md:73:> To create PVCs for RWX volumes using the sample StorageClass, replace the `nfsOptions` string with a customized comma-separated list of legal options.
content/docs/1.4.5/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.5/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.4.5/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.4.5/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.4.5/deploy/important-notes/index.md:20:When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to v1.4.5, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890).
content/docs/1.4.5/deploy/important-notes/index.md:34:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.4.5/deploy/important-notes/index.md:37:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.5/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.5/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.5/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.4.5/deploy/install/_index.md:149:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.4.5/deploy/install/_index.md:254:The `Server Version` should be >= v1.21.
content/docs/1.4.5/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.4.5/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.5/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.5/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.5/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.5/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.5/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.5/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.5/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.5/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.5/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.5/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.4.5/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.4.5/deploy/upgrade/_index.md:18:- To upgrade from v1.4.x (< v1.4.5) or v1.3.x, see [this section.](./longhorn-manager)
content/docs/1.4.5/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.4.x (< v1.4.5) or v1.3.x
content/docs/1.4.5/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.4.5/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.5/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.5/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.4.x (< v1.4.5) or v1.3.x to v1.4.5.
content/docs/1.4.5/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.4.5/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.4.5/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.4.5/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.4.5/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.4.5/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.4.5/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.4.5/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.4.5/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.4.5/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.4.5/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.4.5/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.4.5/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.4.5/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.4.5/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.4.5/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.5/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.5/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.4.5/references/settings.md:85:> Default: `block-if-contains-last-replica`
content/docs/1.4.5/references/settings.md:96:> Default: `true`
content/docs/1.4.5/references/settings.md:102:> Default: `true`
content/docs/1.4.5/references/settings.md:109:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.5/references/settings.md:110:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.5/references/settings.md:111:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.5/references/settings.md:115:> Default: `true`
content/docs/1.4.5/references/settings.md:121:> Default: `0`
content/docs/1.4.5/references/settings.md:129:> Default: `5`
content/docs/1.4.5/references/settings.md:139:> Default: `false`
content/docs/1.4.5/references/settings.md:149:> Default: `longhorn.io/v1beta2`
content/docs/1.4.5/references/settings.md:155:> Default: `disabled`
content/docs/1.4.5/references/settings.md:173:> Default: `/var/lib/longhorn/`
content/docs/1.4.5/references/settings.md:191:> Default: `longhorn-static`
content/docs/1.4.5/references/settings.md:197:> Default: `3`
content/docs/1.4.5/references/settings.md:212:> Default: `false`
content/docs/1.4.5/references/settings.md:216:> Default: `false`
content/docs/1.4.5/references/settings.md:222:> Default: `true`
content/docs/1.4.5/references/settings.md:234:> Default: `do-nothing`
content/docs/1.4.5/references/settings.md:249:> Default: `600`
content/docs/1.4.5/references/settings.md:257:> Default: `if-not-present`
content/docs/1.4.5/references/settings.md:273:> Default: `60`
content/docs/1.4.5/references/settings.md:278:> Default: `300`
content/docs/1.4.5/references/settings.md:282:> **Note:**
content/docs/1.4.5/references/settings.md:283:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.5/references/settings.md:284:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.5/references/settings.md:287:> Default: `8`
content/docs/1.4.5/references/settings.md:298:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.5/references/settings.md:302:> Default: `1`
content/docs/1.4.5/references/settings.md:312:> Default: `false`
content/docs/1.4.5/references/settings.md:318:> Default: `30`
content/docs/1.4.5/references/settings.md:326:> Default: `fast-check`
content/docs/1.4.5/references/settings.md:335:> Default: `false`
content/docs/1.4.5/references/settings.md:341:> Default: `0 0 */7 * *`
content/docs/1.4.5/references/settings.md:344:> **Warning**
content/docs/1.4.5/references/settings.md:345:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.5/references/settings.md:351:> Default: `false`
content/docs/1.4.5/references/settings.md:359:> Default: `false`
content/docs/1.4.5/references/settings.md:363:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.5/references/settings.md:367:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.5/references/settings.md:373:> Example: `s3-secret`
content/docs/1.4.5/references/settings.md:379:> Default: `300`
content/docs/1.4.5/references/settings.md:387:> Default: `1440`
content/docs/1.4.5/references/settings.md:395:> Default: `1`
content/docs/1.4.5/references/settings.md:404:> Default: `1`
content/docs/1.4.5/references/settings.md:412:> Default: `false`
content/docs/1.4.5/references/settings.md:417:> Default: `ignored`
content/docs/1.4.5/references/settings.md:429:> Default: `true`
content/docs/1.4.5/references/settings.md:433:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.5/references/settings.md:437:> Default: `true`
content/docs/1.4.5/references/settings.md:445:> Default: `false`
content/docs/1.4.5/references/settings.md:453:> Default: `true`
content/docs/1.4.5/references/settings.md:459:> **Note:**
content/docs/1.4.5/references/settings.md:460:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.5/references/settings.md:461:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.5/references/settings.md:465:> Default: `disabled`
content/docs/1.4.5/references/settings.md:481:> Default: `ignored`
content/docs/1.4.5/references/settings.md:495:> Default: `25`
content/docs/1.4.5/references/settings.md:503:> Default: `200`
content/docs/1.4.5/references/settings.md:515:> Default: `5`
content/docs/1.4.5/references/settings.md:521:> **WARNING:**
content/docs/1.4.5/references/settings.md:522:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.5/references/settings.md:523:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.5/references/settings.md:524:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.5/references/settings.md:529:> Default: `12`
content/docs/1.4.5/references/settings.md:541:> **Warning:**
content/docs/1.4.5/references/settings.md:542:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.5/references/settings.md:543:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.5/references/settings.md:544:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.5/references/settings.md:545:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.5/references/settings.md:546:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.5/references/settings.md:550:> Default: `12`
content/docs/1.4.5/references/settings.md:562:> **Warning:**
content/docs/1.4.5/references/settings.md:563:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.5/references/settings.md:564:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.5/references/settings.md:565:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.5/references/settings.md:566:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.5/references/settings.md:567:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.5/references/settings.md:572:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.5/references/settings.md:591:> Example: `high-priority`
content/docs/1.4.5/references/settings.md:602:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.5/references/settings.md:608:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.5/references/settings.md:615:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.5/references/settings.md:621:> Default: `false`
content/docs/1.4.5/references/settings.md:627:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.5/references/settings.md:631:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.5/references/settings.md:635:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.5/references/settings.md:641:> Example: `false`
content/docs/1.4.5/references/settings.md:653:> Default: `false`
content/docs/1.4.5/references/settings.md:668:> Default: `false`
content/docs/1.4.5/references/settings.md:680:> Default: `false`
content/docs/1.4.5/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.5/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.5/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.5/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.5/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.5/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.5/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.5/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.5/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.5/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.5/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.5/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.5/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.5/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.5/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.5/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.5/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.5/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.5/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.5/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.5/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.5/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.5/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.5/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.5/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.5/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.5/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.5/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.5/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.5/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.5/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.5/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.5/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.5/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.5/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.5/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.5/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.5/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.5/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.5/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.5/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.5/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.5/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.5/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.5/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.5/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.5/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.5/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.5/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.5/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:47:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:48:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:       kubectl create secret generic <aws-secret> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:59:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:60:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:84:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:85:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:108:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:119:       kubectl create secret generic <aws-secret> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:120:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:127:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:133:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:kubectl create secret generic <aws-secret> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:205:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.5/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.5/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.5/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:73:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:75:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    >
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:77:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:78:    > - `snapshot` with retain value set to 5
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:107:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:112:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:117:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/1.4.5/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.5/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.5/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.4.5/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.4.5/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.5/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.5/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.4.5/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.4.5/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.4.5/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.4.5/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.4.5/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.4.5/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.4.5/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.4.5/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.5/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.5/volumes-and-nodes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.5/volumes-and-nodes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/1.4.5/volumes-and-nodes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.5/volumes-and-nodes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/1.4.5/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/1.4.5/volumes-and-nodes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.4.5/volumes-and-nodes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/1.4.5/volumes-and-nodes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/1.4.5/volumes-and-nodes/expansion.md:123:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.4.5/volumes-and-nodes/expansion.md:131:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.5/volumes-and-nodes/expansion.md:133:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.4.5/volumes-and-nodes/maintenance.md:27:   > **Note:**
content/docs/1.4.5/volumes-and-nodes/maintenance.md:28:   > By default, if there is one last healthy replica for a volume on
content/docs/1.4.5/volumes-and-nodes/maintenance.md:29:   > the node, Longhorn will prevent the node from completing the drain
content/docs/1.4.5/volumes-and-nodes/maintenance.md:30:   > operation, to protect the last replica and prevent the disruption of the
content/docs/1.4.5/volumes-and-nodes/maintenance.md:31:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/1.4.5/volumes-and-nodes/maintenance.md:32:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/1.4.5/volumes-and-nodes/maintenance.md:35:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/1.4.5/volumes-and-nodes/maintenance.md:36:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/1.4.5/volumes-and-nodes/maintenance.md:37:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/1.4.5/volumes-and-nodes/maintenance.md:38:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/1.4.5/volumes-and-nodes/maintenance.md:42:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/1.4.5/volumes-and-nodes/maintenance.md:43:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/1.4.5/volumes-and-nodes/maintenance.md:44:   > can be reused later, once the node back online and uncordoned.
content/docs/1.4.5/volumes-and-nodes/maintenance.md:77:        kubectl delete node <node-name>
content/docs/1.4.5/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.5/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.5/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.5/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.4.5/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:31:Users need to figure out the mount point of the volume then manually execute `fstrim <the mount point>`.
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:55:    1. Figure out and enter into the share manager pod of the RWX volume, which actually contains the NFS server. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:57:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:59:    2. Figure out the work directory of the NFS server. The work directory is typically like `/export/<volume name>`:
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:61:    mount | grep /dev/longhorn/<volume name>
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:62:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:66:    fstrim /export/<volume name>
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:76:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:78:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.4.5/volumes-and-nodes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.5/volumes-and-nodes/volume-size.md:44:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.4.5/volumes-and-nodes/volume-size.md:50:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.4.5/volumes-and-nodes/volume-size.md:51:     >
content/docs/1.4.5/volumes-and-nodes/volume-size.md:52:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.4.5/volumes-and-nodes/volume-size.md:71:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.4.5/volumes-and-nodes/volume-size.md:72:    >
content/docs/1.4.5/volumes-and-nodes/volume-size.md:73:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.4.5/best-practices.md:123:> Recommend: `false`
content/docs/1.4.5/best-practices.md:129:> Recommend: `false`
content/docs/1.4.5/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.5/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.0/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.5.0/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.5.0/advanced-resources/backing-image.md:114:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.5.0/advanced-resources/backing-image.md:135:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.5.0/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.5.0/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.5.0/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.5.0 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.5.0/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.5.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.5.0`
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.5.0/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.0/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.0/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.5.0/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.0/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.5.0/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.5.0/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.5.0/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.5.0/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.5.0/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.5.0/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.5.0/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:173:In v1.5.0, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.5.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.0/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.0/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.0/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.0/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.0/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:168:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:174:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:179:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:188:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:191:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:195:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:39:   >
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.5.0/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.0/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.0/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.0/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.5.0/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.0/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.5.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.0/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.0/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.5.0/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.5.0/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.5.0/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.5.0/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.5.0/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.5.0/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.5.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.5.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.5.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.5.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.0/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.5.0/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.5.0/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.5.0/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.5.0/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.5.0/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.0/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.5.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.0/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.0/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.0/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.0/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:56:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:60:  > **Note:** Longhorn system restores volume with the latest backup. We recommend updating the last backup for all volumes. By taking volume backups, you ensure that the data is up-to-date with the system backup. For more information, please refer to the [Configuration - Volume Backup Policy](#volume-backup-policy) section.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:92:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:106:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:112:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:116:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.0/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.5.0/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.5.0/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.5.0/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.5.0/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.5.0/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.0/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.0/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.0/best-practices.md:124:> Recommend: `false`
content/docs/1.5.0/best-practices.md:130:> Recommend: `false`
content/docs/1.5.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.0/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.5.0/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.5.0/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.5.0/deploy/important-notes/index.md:44:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.5.0/deploy/important-notes/index.md:47:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.0/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.5.0/deploy/install/_index.md:151:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.5.0/deploy/install/_index.md:256:The `Server Version` should be >= v1.21.
content/docs/1.5.0/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.5.0/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.0/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.0/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.0/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.0/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.0/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.0/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.0/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.0/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.0/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.0/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.5.0/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.5.0/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.5.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.0/deploy/upgrade/_index.md:18:> **Warning:**
content/docs/1.5.0/deploy/upgrade/_index.md:19:> The Downgrade Prevention feature was introduced in v1.5.0 so Longhorn is unable to prevent downgrade attempts in older versions.
content/docs/1.5.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.5.0/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.5.0/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.5.0/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.5.0/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.5.0/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.5.0/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.5.0/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.5.0/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.5.0/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.5.0/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.5.0/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.5.0/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.5.0/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.5.0/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.5.0/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.0/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.0/references/networking.md:77:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.0/references/networking.md:89:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.0/references/settings.md:86:> Default: `block-if-contains-last-replica`
content/docs/1.5.0/references/settings.md:97:> Default: `true`
content/docs/1.5.0/references/settings.md:103:> Default: `true`
content/docs/1.5.0/references/settings.md:110:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.0/references/settings.md:111:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.0/references/settings.md:112:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.0/references/settings.md:116:> Default: `true`
content/docs/1.5.0/references/settings.md:122:> Default: `0`
content/docs/1.5.0/references/settings.md:130:> Default: `5`
content/docs/1.5.0/references/settings.md:140:> Default: `false`
content/docs/1.5.0/references/settings.md:150:> Default: `longhorn.io/v1beta2`
content/docs/1.5.0/references/settings.md:156:> Default: `disabled`
content/docs/1.5.0/references/settings.md:174:> Default: `/var/lib/longhorn/`
content/docs/1.5.0/references/settings.md:188:> Default: `longhorn-static`
content/docs/1.5.0/references/settings.md:194:> Default: `3`
content/docs/1.5.0/references/settings.md:205:> Default: `false`
content/docs/1.5.0/references/settings.md:209:> Default: `false`
content/docs/1.5.0/references/settings.md:215:> Default: `true`
content/docs/1.5.0/references/settings.md:223:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.0/references/settings.md:227:> Default: `true`
content/docs/1.5.0/references/settings.md:235:  > This value may not be accurate for virtual machines.
content/docs/1.5.0/references/settings.md:308:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.0/references/settings.md:312:> Default: `do-nothing`
content/docs/1.5.0/references/settings.md:327:> Default: `600`
content/docs/1.5.0/references/settings.md:335:> Default: `if-not-present`
content/docs/1.5.0/references/settings.md:351:> Default: `60`
content/docs/1.5.0/references/settings.md:356:> Default: `300`
content/docs/1.5.0/references/settings.md:360:> **Note:**
content/docs/1.5.0/references/settings.md:361:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.0/references/settings.md:362:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.0/references/settings.md:365:> Default: `8`
content/docs/1.5.0/references/settings.md:376:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.0/references/settings.md:380:> Default: `1`
content/docs/1.5.0/references/settings.md:390:> Default: `false`
content/docs/1.5.0/references/settings.md:396:> Default: `30`
content/docs/1.5.0/references/settings.md:403:> Default: `false`
content/docs/1.5.0/references/settings.md:407:> **Warning**
content/docs/1.5.0/references/settings.md:408:>
content/docs/1.5.0/references/settings.md:409:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.0/references/settings.md:410:>
content/docs/1.5.0/references/settings.md:411:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.0/references/settings.md:412:>
content/docs/1.5.0/references/settings.md:413:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.0/references/settings.md:417:> Default: `enabled`
content/docs/1.5.0/references/settings.md:429:> Default: `fast-check`
content/docs/1.5.0/references/settings.md:438:> Default: `false`
content/docs/1.5.0/references/settings.md:444:> Default: `0 0 */7 * *`
content/docs/1.5.0/references/settings.md:447:> **Warning**
content/docs/1.5.0/references/settings.md:448:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.0/references/settings.md:454:> Default: `false`
content/docs/1.5.0/references/settings.md:462:> Default: `false`
content/docs/1.5.0/references/settings.md:466:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.0/references/settings.md:470:> Examples:  
content/docs/1.5.0/references/settings.md:471:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.0/references/settings.md:472:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.0/references/settings.md:473:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.0/references/settings.md:479:> Example: `s3-secret`
content/docs/1.5.0/references/settings.md:485:> Default: `300`
content/docs/1.5.0/references/settings.md:493:> Default: `1440`
content/docs/1.5.0/references/settings.md:501:> Default: `1`
content/docs/1.5.0/references/settings.md:510:> Default: `1`
content/docs/1.5.0/references/settings.md:518:> Default: `false`
content/docs/1.5.0/references/settings.md:523:> Default: `ignored`
content/docs/1.5.0/references/settings.md:533:> Default: `lz4`
content/docs/1.5.0/references/settings.md:545:> Default: `2`
content/docs/1.5.0/references/settings.md:551:> Default: `2`
content/docs/1.5.0/references/settings.md:559:> Default: `true`
content/docs/1.5.0/references/settings.md:563:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.0/references/settings.md:567:> Default: `true`
content/docs/1.5.0/references/settings.md:575:> Default: `false`
content/docs/1.5.0/references/settings.md:583:> Default: `true`
content/docs/1.5.0/references/settings.md:589:> **Note:**
content/docs/1.5.0/references/settings.md:590:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.0/references/settings.md:591:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.0/references/settings.md:595:> Default: `disabled`
content/docs/1.5.0/references/settings.md:611:> Default: `ignored`
content/docs/1.5.0/references/settings.md:625:> Default: `25`
content/docs/1.5.0/references/settings.md:633:> Default: `100`
content/docs/1.5.0/references/settings.md:643:> Default: `30`
content/docs/1.5.0/references/settings.md:653:> Default: `5`
content/docs/1.5.0/references/settings.md:659:> **WARNING:**
content/docs/1.5.0/references/settings.md:660:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.0/references/settings.md:661:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.0/references/settings.md:662:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.0/references/settings.md:667:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.0/references/settings.md:686:> Example: `high-priority`
content/docs/1.5.0/references/settings.md:697:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.0/references/settings.md:703:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.0/references/settings.md:710:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.0/references/settings.md:716:> Default: `false`
content/docs/1.5.0/references/settings.md:722:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.0/references/settings.md:726:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.0/references/settings.md:730:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.0/references/settings.md:736:> Example: `false`
content/docs/1.5.0/references/settings.md:748:> Default: `12`
content/docs/1.5.0/references/settings.md:760:> **Warning:**
content/docs/1.5.0/references/settings.md:761:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.0/references/settings.md:762:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.0/references/settings.md:763:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.0/references/settings.md:764:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.0/references/settings.md:765:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.0/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.0/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.0/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.0/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.0/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.0/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.0/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.0/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.0/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.0/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.0/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.0/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.0/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.0/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.0/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.0/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.0/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.0/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.0/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.0/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.0/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.0/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.0/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.0/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.0/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.0/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.0/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.0/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.0/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.0/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.0/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.0/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.0/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.0/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.0/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.0/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.0/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.0/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.0/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.0/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.0/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.0/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.0/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.0/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.0/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.0/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.0/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.0/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.0/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.0/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.0/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.0/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.0/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:50:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:61:       kubectl create secret generic <aws-secret> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:62:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:111:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:       kubectl create secret generic <aws-secret> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:123:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:139:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:kubectl create secret generic <aws-secret> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:159:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:160:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:161:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:264:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:334:  cat <<EOF >>cifs_secret.yml
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:349:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:402:    cat <<EOF >>azblob_secret.yml
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:423:    kubectl create secret generic <azblob-secret> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:424:      --from-literal=AZBLOB_ACCOUNT_NAME=<your-azure-storage-account-name> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:425:      --from-literal=AZBLOB_ACCOUNT_KEY=<your-azure-storage-account-key> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:426:      --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:427:      --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:428:      --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:432:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:446:   - If you set `<endpoint-suffix>` in the URL, the default endpoint suffix will be `core.windows.net`.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:447:   - If you set `AZBLOB_ENDPOINT` in the secret, Longhorn will use `AZBLOB_ENDPOINT` as your storage URL, and `<endpoint-suffix>` will not be used even if it has been set. 
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:52:To see the BackingImage, click **Setting > Backing Image** in the top navigation bar and click the BackingImage mentioned in the `VolumeSnapshotContent.snapshotHandle`.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:108:The parameters can be retrieved from the **Setting > Backing Image** page in the Longhorn UI.
content/docs/1.5.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:82:    > - `snapshot` with retain value set to 5
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:83:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:84:    >
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:85:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:121:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:129:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:137:kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:149:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/source=enabled
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:157:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:165:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:173:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.0/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.5.0/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.5.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.0/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.5.0/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.5.0/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.5.0/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.5.0/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.5.0/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.5.0/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.5.0/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.5.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.0/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.0/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.5.0/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.0/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.5.0/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.5.0/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.0/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.5.0/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.5.0/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.5.0/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.5.0/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.5.0/volumes-and-nodes/maintenance.md:27:   > **Note:**
content/docs/1.5.0/volumes-and-nodes/maintenance.md:28:   > By default, if there is one last healthy replica for a volume on
content/docs/1.5.0/volumes-and-nodes/maintenance.md:29:   > the node, Longhorn will prevent the node from completing the drain
content/docs/1.5.0/volumes-and-nodes/maintenance.md:30:   > operation, to protect the last replica and prevent the disruption of the
content/docs/1.5.0/volumes-and-nodes/maintenance.md:31:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/1.5.0/volumes-and-nodes/maintenance.md:32:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/1.5.0/volumes-and-nodes/maintenance.md:35:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/1.5.0/volumes-and-nodes/maintenance.md:36:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/1.5.0/volumes-and-nodes/maintenance.md:37:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/1.5.0/volumes-and-nodes/maintenance.md:38:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/1.5.0/volumes-and-nodes/maintenance.md:42:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/1.5.0/volumes-and-nodes/maintenance.md:43:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/1.5.0/volumes-and-nodes/maintenance.md:44:   > can be reused later, once the node back online and uncordoned.
content/docs/1.5.0/volumes-and-nodes/maintenance.md:77:        kubectl delete node <node-name>
content/docs/1.5.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.0/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.5.0/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:31:Users need to figure out the mount point of the volume then manually execute `fstrim <the mount point>`.
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:55:    1. Figure out and enter into the share manager pod of the RWX volume, which actually contains the NFS server. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:57:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:59:    2. Figure out the work directory of the NFS server. The work directory is typically like `/export/<volume name>`:
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:61:    mount | grep /dev/longhorn/<volume name>
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:62:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:66:    fstrim /export/<volume name>
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:76:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:78:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.5.0/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.0/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.5.0/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.5.0/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.5.0/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.5.0/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.5.0/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.5.0/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.5.0/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.0/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.0/v2-data-engine/prerequisites.md:11:  > **NOTICE**
content/docs/1.5.0/v2-data-engine/prerequisites.md:12:  >
content/docs/1.5.0/v2-data-engine/prerequisites.md:13:  > Currently, V2 Data Engine only supports `x86_64` platform.
content/docs/1.5.0/v2-data-engine/quick-start.md:71:  echo 512 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
content/docs/1.5.0/v2-data-engine/quick-start.md:76:  echo "vm.nr_hugepages=512" >> /etc/sysctl.conf
content/docs/1.5.0/v2-data-engine/quick-start.md:81:> **NOTICE:**
content/docs/1.5.0/v2-data-engine/quick-start.md:82:>
content/docs/1.5.0/v2-data-engine/quick-start.md:83:> Make sure that the version of `nvme-cli` is equal to or greater than `1.12`.
content/docs/1.5.0/v2-data-engine/quick-start.md:84:>
content/docs/1.5.0/v2-data-engine/quick-start.md:85:> If the version of `nvme-cli` installed by the below steps is not equal to or greater than `1.12`., you will need to compile the utility from the [source codes](https://github.com/linux-nvme/nvme-cli) and install it on each Longhorn node by manual.
content/docs/1.5.0/v2-data-engine/quick-start.md:86:>
content/docs/1.5.0/v2-data-engine/quick-start.md:87:> Also, install the **uuid development library** before compiling to support the `show-hostnqn` subcommand.
content/docs/1.5.0/v2-data-engine/quick-start.md:88:>
content/docs/1.5.0/v2-data-engine/quick-start.md:89:> For SUSE/OpenSUSE you can install it use this command:
content/docs/1.5.0/v2-data-engine/quick-start.md:90:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:91:> zypper install uuid-devel
content/docs/1.5.0/v2-data-engine/quick-start.md:92:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:93:>
content/docs/1.5.0/v2-data-engine/quick-start.md:94:> For Debian and Ubuntu, use this command:
content/docs/1.5.0/v2-data-engine/quick-start.md:95:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:96:> apt install uuid-dev
content/docs/1.5.0/v2-data-engine/quick-start.md:97:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:98:>
content/docs/1.5.0/v2-data-engine/quick-start.md:99:> For RHEL, CentOS, and EKS with `EKS Kubernetes Worker AMI with AmazonLinux2 image`, use this command:
content/docs/1.5.0/v2-data-engine/quick-start.md:100:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:101:> yum install uuid-devel
content/docs/1.5.0/v2-data-engine/quick-start.md:102:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:103:>
content/docs/1.5.0/v2-data-engine/quick-start.md:173:Or, you can enable it in `Setting > General > V2 Data Engine`.
content/docs/1.5.0/v2-data-engine/quick-start.md:207:You can observe the utilization of allocated huge pages on each node by running the command `kubectl get node <node name> -o yaml`.
content/docs/1.5.0/v2-data-engine/quick-start.md:254:kubectl -n longhorn-system edit node.longhorn.io <NODE NAME>
content/docs/1.5.0/v2-data-engine/quick-start.md:259:<DISK NAME>:
content/docs/1.5.1/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.5.1/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.5.1/advanced-resources/backing-image.md:114:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.5.1/advanced-resources/backing-image.md:135:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.5.1/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.5.1/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.5.1/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.5.1 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.5.1/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.5.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.5.1`
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.5.1/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.1/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.5.1/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.1/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.5.1/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.5.1/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.5.1/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.5.1/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.5.1/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.5.1/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.5.1/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:173:In v1.5.1, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.5.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.1/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.1/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.1/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.1/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.1/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:168:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:174:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:179:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:188:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:191:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:195:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:39:   >
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.5.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.1/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.5.1/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.1/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.5.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.1/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.1/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.5.1/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.5.1/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.5.1/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.5.1/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.5.1/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.5.1/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.5.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.5.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.5.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.5.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.1/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.5.1/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.5.1/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.5.1/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.5.1/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.5.1/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.5.1/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.1/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.1/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.1/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.1/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.1/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.1/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:56:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:60:  > **Note:** Longhorn system restores volume with the latest backup. We recommend updating the last backup for all volumes. By taking volume backups, you ensure that the data is up-to-date with the system backup. For more information, please refer to the [Configuration - Volume Backup Policy](#volume-backup-policy) section.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:92:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:106:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:112:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:116:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.1/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.5.1/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.5.1/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.5.1/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.5.1/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.5.1/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.1/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.1/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.1/best-practices.md:124:> Recommend: `false`
content/docs/1.5.1/best-practices.md:130:> Recommend: `false`
content/docs/1.5.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.1/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.5.1/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.5.1/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.5.1/deploy/important-notes/index.md:44:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.5.1/deploy/important-notes/index.md:47:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.1/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.5.1/deploy/install/_index.md:151:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.5.1/deploy/install/_index.md:256:The `Server Version` should be >= v1.21.
content/docs/1.5.1/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.5.1/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.1/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.1/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.1/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.1/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.1/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.1/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.1/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.1/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.1/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.5.1/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.5.1/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.1/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.1/deploy/upgrade/_index.md:39:- To upgrade from v1.5.x (< v1.5.1) or v1.4.x, see [this section.](./longhorn-manager)
content/docs/1.5.1/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.5.x (< v1.5.1) or v1.4.x.
content/docs/1.5.1/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.5.x (< v1.5.1) or v1.4.x
content/docs/1.5.1/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.5.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.1/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.1/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.5.1/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.5.1/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.5.1/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.5.1/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.5.1/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.5.1/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.5.1/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.5.1/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.5.1/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.5.1/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.5.1/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.5.1/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.5.1/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.5.1/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.5.1/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.1/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.1/references/networking.md:76:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.1/references/networking.md:88:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.1/references/settings.md:86:> Default: `block-if-contains-last-replica`
content/docs/1.5.1/references/settings.md:97:> Default: `true`
content/docs/1.5.1/references/settings.md:103:> Default: `true`
content/docs/1.5.1/references/settings.md:110:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.1/references/settings.md:111:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.1/references/settings.md:112:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.1/references/settings.md:116:> Default: `true`
content/docs/1.5.1/references/settings.md:122:> Default: `0`
content/docs/1.5.1/references/settings.md:130:> Default: `5`
content/docs/1.5.1/references/settings.md:140:> Default: `false`
content/docs/1.5.1/references/settings.md:150:> Default: `longhorn.io/v1beta2`
content/docs/1.5.1/references/settings.md:156:> Default: `disabled`
content/docs/1.5.1/references/settings.md:174:> Default: `/var/lib/longhorn/`
content/docs/1.5.1/references/settings.md:188:> Default: `longhorn-static`
content/docs/1.5.1/references/settings.md:194:> Default: `3`
content/docs/1.5.1/references/settings.md:205:> Default: `false`
content/docs/1.5.1/references/settings.md:209:> Default: `false`
content/docs/1.5.1/references/settings.md:215:> Default: `true`
content/docs/1.5.1/references/settings.md:223:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.1/references/settings.md:227:> Default: `true`
content/docs/1.5.1/references/settings.md:235:  > This value may not be accurate for virtual machines.
content/docs/1.5.1/references/settings.md:308:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.1/references/settings.md:312:> Default: `do-nothing`
content/docs/1.5.1/references/settings.md:327:> Default: `600`
content/docs/1.5.1/references/settings.md:335:> Default: `if-not-present`
content/docs/1.5.1/references/settings.md:351:> Default: `60`
content/docs/1.5.1/references/settings.md:356:> Default: `300`
content/docs/1.5.1/references/settings.md:360:> **Note:**
content/docs/1.5.1/references/settings.md:361:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.1/references/settings.md:362:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.1/references/settings.md:365:> Default: `8`
content/docs/1.5.1/references/settings.md:376:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.1/references/settings.md:380:> Default: `1`
content/docs/1.5.1/references/settings.md:390:> Default: `false`
content/docs/1.5.1/references/settings.md:396:> Default: `30`
content/docs/1.5.1/references/settings.md:403:> Default: `false`
content/docs/1.5.1/references/settings.md:407:> **Warning**
content/docs/1.5.1/references/settings.md:408:>
content/docs/1.5.1/references/settings.md:409:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.1/references/settings.md:410:>
content/docs/1.5.1/references/settings.md:411:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.1/references/settings.md:412:>
content/docs/1.5.1/references/settings.md:413:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.1/references/settings.md:417:> Default: `enabled`
content/docs/1.5.1/references/settings.md:429:> Default: `fast-check`
content/docs/1.5.1/references/settings.md:438:> Default: `false`
content/docs/1.5.1/references/settings.md:444:> Default: `0 0 */7 * *`
content/docs/1.5.1/references/settings.md:447:> **Warning**
content/docs/1.5.1/references/settings.md:448:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.1/references/settings.md:454:> Default: `false`
content/docs/1.5.1/references/settings.md:462:> Default: `false`
content/docs/1.5.1/references/settings.md:466:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.1/references/settings.md:470:> Examples:  
content/docs/1.5.1/references/settings.md:471:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.1/references/settings.md:472:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.1/references/settings.md:473:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.1/references/settings.md:479:> Example: `s3-secret`
content/docs/1.5.1/references/settings.md:485:> Default: `300`
content/docs/1.5.1/references/settings.md:493:> Default: `1440`
content/docs/1.5.1/references/settings.md:501:> Default: `1`
content/docs/1.5.1/references/settings.md:510:> Default: `1`
content/docs/1.5.1/references/settings.md:518:> Default: `false`
content/docs/1.5.1/references/settings.md:523:> Default: `ignored`
content/docs/1.5.1/references/settings.md:533:> Default: `lz4`
content/docs/1.5.1/references/settings.md:545:> Default: `2`
content/docs/1.5.1/references/settings.md:551:> Default: `2`
content/docs/1.5.1/references/settings.md:559:> Default: `true`
content/docs/1.5.1/references/settings.md:563:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.1/references/settings.md:567:> Default: `true`
content/docs/1.5.1/references/settings.md:575:> Default: `false`
content/docs/1.5.1/references/settings.md:583:> Default: `true`
content/docs/1.5.1/references/settings.md:589:> **Note:**
content/docs/1.5.1/references/settings.md:590:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.1/references/settings.md:591:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.1/references/settings.md:595:> Default: `disabled`
content/docs/1.5.1/references/settings.md:611:> Default: `ignored`
content/docs/1.5.1/references/settings.md:625:> Default: `25`
content/docs/1.5.1/references/settings.md:633:> Default: `100`
content/docs/1.5.1/references/settings.md:643:> Default: `30`
content/docs/1.5.1/references/settings.md:653:> Default: `5`
content/docs/1.5.1/references/settings.md:659:> **WARNING:**
content/docs/1.5.1/references/settings.md:660:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.1/references/settings.md:661:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.1/references/settings.md:662:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.1/references/settings.md:667:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.1/references/settings.md:686:> Example: `high-priority`
content/docs/1.5.1/references/settings.md:697:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.1/references/settings.md:703:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.1/references/settings.md:710:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.1/references/settings.md:716:> Default: `false`
content/docs/1.5.1/references/settings.md:722:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.1/references/settings.md:726:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.1/references/settings.md:730:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.1/references/settings.md:736:> Example: `false`
content/docs/1.5.1/references/settings.md:748:> Default: `12`
content/docs/1.5.1/references/settings.md:760:> **Warning:**
content/docs/1.5.1/references/settings.md:761:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.1/references/settings.md:762:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.1/references/settings.md:763:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.1/references/settings.md:764:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.1/references/settings.md:765:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.1/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.1/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.1/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.1/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.1/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.1/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.1/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.1/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.1/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.1/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.1/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.1/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.1/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.1/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.1/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.1/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.1/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.1/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.1/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.1/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.1/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.1/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.1/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.1/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.1/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.1/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.1/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.1/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.1/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.1/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.1/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.1/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.1/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.1/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.1/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.1/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.1/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.1/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.1/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.1/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.1/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.1/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.1/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.1/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.1/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.1/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.1/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.1/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.1/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.1/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.1/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.1/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.1/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:50:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:61:       kubectl create secret generic <aws-secret> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:62:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:111:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:       kubectl create secret generic <aws-secret> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:123:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:139:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:kubectl create secret generic <aws-secret> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:159:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:160:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:161:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:264:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:334:  cat <<EOF >>cifs_secret.yml
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:349:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:402:    cat <<EOF >>azblob_secret.yml
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:423:    kubectl create secret generic <azblob-secret> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:424:      --from-literal=AZBLOB_ACCOUNT_NAME=<your-azure-storage-account-name> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:425:      --from-literal=AZBLOB_ACCOUNT_KEY=<your-azure-storage-account-key> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:426:      --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:427:      --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:428:      --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:432:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:446:   - If you set `<endpoint-suffix>` in the URL, the default endpoint suffix will be `core.windows.net`.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:447:   - If you set `AZBLOB_ENDPOINT` in the secret, Longhorn will use `AZBLOB_ENDPOINT` as your storage URL, and `<endpoint-suffix>` will not be used even if it has been set. 
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:52:To see the BackingImage, click **Setting > Backing Image** in the top navigation bar and click the BackingImage mentioned in the `VolumeSnapshotContent.snapshotHandle`.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:108:The parameters can be retrieved from the **Setting > Backing Image** page in the Longhorn UI.
content/docs/1.5.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:82:    > - `snapshot` with retain value set to 5
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:83:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:84:    >
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:85:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:121:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:129:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:137:kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:149:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/source=enabled
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:157:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:165:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:173:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.1/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.5.1/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.5.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.1/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.5.1/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.5.1/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.5.1/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.5.1/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.5.1/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.5.1/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.5.1/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.5.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.1/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.1/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.5.1/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.1/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.5.1/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.5.1/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.1/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.5.1/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.5.1/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.5.1/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.5.1/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.5.1/volumes-and-nodes/maintenance.md:27:   > **Note:**
content/docs/1.5.1/volumes-and-nodes/maintenance.md:28:   > By default, if there is one last healthy replica for a volume on
content/docs/1.5.1/volumes-and-nodes/maintenance.md:29:   > the node, Longhorn will prevent the node from completing the drain
content/docs/1.5.1/volumes-and-nodes/maintenance.md:30:   > operation, to protect the last replica and prevent the disruption of the
content/docs/1.5.1/volumes-and-nodes/maintenance.md:31:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/1.5.1/volumes-and-nodes/maintenance.md:32:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/1.5.1/volumes-and-nodes/maintenance.md:35:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/1.5.1/volumes-and-nodes/maintenance.md:36:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/1.5.1/volumes-and-nodes/maintenance.md:37:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/1.5.1/volumes-and-nodes/maintenance.md:38:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/1.5.1/volumes-and-nodes/maintenance.md:42:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/1.5.1/volumes-and-nodes/maintenance.md:43:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/1.5.1/volumes-and-nodes/maintenance.md:44:   > can be reused later, once the node back online and uncordoned.
content/docs/1.5.1/volumes-and-nodes/maintenance.md:77:        kubectl delete node <node-name>
content/docs/1.5.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.1/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.5.1/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:31:When using `fstrim`, you must identify the mount point of the volume and then run the command `fstrim <the mount point>`.
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:34:- RWX volume: The mount point is the share manager pod of the volume. The share manager pod contains the NFS server and is typically named `share-manager-<volume name>`.
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:39:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:41:1. Identify the work directory of the NFS server (for example, `/export/<volume name>`).
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:43:    mount | grep <volume name>
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:44:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:48:    fstrim /export/<volume name>
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:76:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:78:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.5.1/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.1/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.5.1/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.5.1/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.5.1/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.5.1/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.5.1/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.5.1/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.5.1/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.1/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.1/v2-data-engine/prerequisites.md:11:  > **NOTICE**
content/docs/1.5.1/v2-data-engine/prerequisites.md:12:  >
content/docs/1.5.1/v2-data-engine/prerequisites.md:13:  > Currently, V2 Data Engine only supports `x86_64` platform.
content/docs/1.5.1/v2-data-engine/quick-start.md:71:  echo 512 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
content/docs/1.5.1/v2-data-engine/quick-start.md:76:  echo "vm.nr_hugepages=512" >> /etc/sysctl.conf
content/docs/1.5.1/v2-data-engine/quick-start.md:81:> **NOTICE:**
content/docs/1.5.1/v2-data-engine/quick-start.md:82:>
content/docs/1.5.1/v2-data-engine/quick-start.md:83:> Make sure that the version of `nvme-cli` is equal to or greater than `1.12`.
content/docs/1.5.1/v2-data-engine/quick-start.md:84:>
content/docs/1.5.1/v2-data-engine/quick-start.md:85:> If the version of `nvme-cli` installed by the below steps is not equal to or greater than `1.12`., you will need to compile the utility from the [source codes](https://github.com/linux-nvme/nvme-cli) and install it on each Longhorn node by manual.
content/docs/1.5.1/v2-data-engine/quick-start.md:86:>
content/docs/1.5.1/v2-data-engine/quick-start.md:87:> Also, install the **uuid development library** before compiling to support the `show-hostnqn` subcommand.
content/docs/1.5.1/v2-data-engine/quick-start.md:88:>
content/docs/1.5.1/v2-data-engine/quick-start.md:89:> For SUSE/OpenSUSE you can install it use this command:
content/docs/1.5.1/v2-data-engine/quick-start.md:90:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:91:> zypper install uuid-devel
content/docs/1.5.1/v2-data-engine/quick-start.md:92:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:93:>
content/docs/1.5.1/v2-data-engine/quick-start.md:94:> For Debian and Ubuntu, use this command:
content/docs/1.5.1/v2-data-engine/quick-start.md:95:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:96:> apt install uuid-dev
content/docs/1.5.1/v2-data-engine/quick-start.md:97:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:98:>
content/docs/1.5.1/v2-data-engine/quick-start.md:99:> For RHEL, CentOS, and EKS with `EKS Kubernetes Worker AMI with AmazonLinux2 image`, use this command:
content/docs/1.5.1/v2-data-engine/quick-start.md:100:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:101:> yum install uuid-devel
content/docs/1.5.1/v2-data-engine/quick-start.md:102:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:103:>
content/docs/1.5.1/v2-data-engine/quick-start.md:173:Or, you can enable it in `Setting > General > V2 Data Engine`.
content/docs/1.5.1/v2-data-engine/quick-start.md:207:You can observe the utilization of allocated huge pages on each node by running the command `kubectl get node <node name> -o yaml`.
content/docs/1.5.1/v2-data-engine/quick-start.md:254:kubectl -n longhorn-system edit node.longhorn.io <NODE NAME>
content/docs/1.5.1/v2-data-engine/quick-start.md:259:<DISK NAME>:
content/docs/1.5.2/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.5.2/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.5.2/advanced-resources/backing-image.md:114:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.5.2/advanced-resources/backing-image.md:135:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.5.2/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.5.2/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.5.2/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.5.2 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.5.2/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.5.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.5.2`
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.5.2/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.2/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.2/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.5.2/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.2/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.5.2/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.5.2/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.5.2/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.5.2/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.5.2/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.5.2/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.5.2/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:173:In v1.5.2, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.5.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.2/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.2/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.2/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.2/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.2/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:168:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:174:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:179:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:188:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:191:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:195:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:39:   >
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.5.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.2/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.5.2/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.2/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.5.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.2/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.2/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.5.2/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.5.2/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.5.2/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.5.2/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.5.2/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.5.2/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.5.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.5.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.5.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.5.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.2/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.2/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.2/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.2/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.2/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.2/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.2/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:56:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:60:  > **Note:** Longhorn system restores volume with the latest backup. We recommend updating the last backup for all volumes. By taking volume backups, you ensure that the data is up-to-date with the system backup. For more information, please refer to the [Configuration - Volume Backup Policy](#volume-backup-policy) section.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:92:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:106:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:112:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:116:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.2/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.5.2/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.5.2/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.5.2/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.5.2/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.5.2/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.2/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.5.2/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.5.2/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.5.2/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.5.2/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.5.2/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.2/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.5.2/advanced-resources/rwx-workloads.md:36:> **Notice**  
content/docs/1.5.2/advanced-resources/rwx-workloads.md:37:> An RWX volume must have the access mode set to `ReadWriteMany` and the "migratable" flag disabled (*parameters.migratable: `false`*).
content/docs/1.5.2/advanced-resources/rwx-workloads.md:72:> **Important:**
content/docs/1.5.2/advanced-resources/rwx-workloads.md:73:> To create PVCs for RWX volumes using the sample StorageClass, replace the `nfsOptions` string with a customized comma-separated list of legal options.
content/docs/1.5.2/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.2/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.2/best-practices.md:124:> Recommend: `false`
content/docs/1.5.2/best-practices.md:130:> Recommend: `false`
content/docs/1.5.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.2/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.5.2/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.5.2/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.5.2/deploy/important-notes/index.md:55:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.5.2/deploy/important-notes/index.md:58:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.2/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.2/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.2/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.5.2/deploy/install/_index.md:153:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.5.2/deploy/install/_index.md:258:The `Server Version` should be >= v1.21.
content/docs/1.5.2/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.5.2/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.2/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.2/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.2/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.2/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.2/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.2/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.2/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.2/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.2/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.2/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.5.2/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.5.2/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.2/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.2/deploy/upgrade/_index.md:39:- To upgrade from v1.5.x (< v1.5.2) or v1.4.x, see [this section.](./longhorn-manager)
content/docs/1.5.2/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.5.x (< v1.5.2) or v1.4.x.
content/docs/1.5.2/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.5.x (< v1.5.2) or v1.4.x
content/docs/1.5.2/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.5.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.2/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.2/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.5.2/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.5.2/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.5.2/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.5.2/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.5.2/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.5.2/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.5.2/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.5.2/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.5.2/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.5.2/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.5.2/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.5.2/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.5.2/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.5.2/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.5.2/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.2/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.2/references/networking.md:76:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.2/references/networking.md:88:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.2/references/settings.md:86:> Default: `block-if-contains-last-replica`
content/docs/1.5.2/references/settings.md:97:> Default: `true`
content/docs/1.5.2/references/settings.md:103:> Default: `true`
content/docs/1.5.2/references/settings.md:110:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.2/references/settings.md:111:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.2/references/settings.md:112:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.2/references/settings.md:116:> Default: `true`
content/docs/1.5.2/references/settings.md:122:> Default: `0`
content/docs/1.5.2/references/settings.md:130:> Default: `5`
content/docs/1.5.2/references/settings.md:140:> Default: `false`
content/docs/1.5.2/references/settings.md:150:> Default: `longhorn.io/v1beta2`
content/docs/1.5.2/references/settings.md:156:> Default: `disabled`
content/docs/1.5.2/references/settings.md:174:> Default: `/var/lib/longhorn/`
content/docs/1.5.2/references/settings.md:188:> Default: `longhorn-static`
content/docs/1.5.2/references/settings.md:194:> Default: `3`
content/docs/1.5.2/references/settings.md:205:> Default: `false`
content/docs/1.5.2/references/settings.md:209:> Default: `false`
content/docs/1.5.2/references/settings.md:215:> Default: `true`
content/docs/1.5.2/references/settings.md:223:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.2/references/settings.md:227:> Default: `true`
content/docs/1.5.2/references/settings.md:235:  > This value may not be accurate for virtual machines.
content/docs/1.5.2/references/settings.md:308:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.2/references/settings.md:312:> Default: `do-nothing`
content/docs/1.5.2/references/settings.md:327:> Default: `600`
content/docs/1.5.2/references/settings.md:335:> Default: `if-not-present`
content/docs/1.5.2/references/settings.md:351:> Default: `60`
content/docs/1.5.2/references/settings.md:356:> Default: `300`
content/docs/1.5.2/references/settings.md:360:> **Note:**
content/docs/1.5.2/references/settings.md:361:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.2/references/settings.md:362:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.2/references/settings.md:365:> Default: `8`
content/docs/1.5.2/references/settings.md:376:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.2/references/settings.md:380:> Default: `1`
content/docs/1.5.2/references/settings.md:390:> Default: `false`
content/docs/1.5.2/references/settings.md:396:> Default: `30`
content/docs/1.5.2/references/settings.md:403:> Default: `false`
content/docs/1.5.2/references/settings.md:407:> **Warning**
content/docs/1.5.2/references/settings.md:408:>
content/docs/1.5.2/references/settings.md:409:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.2/references/settings.md:410:>
content/docs/1.5.2/references/settings.md:411:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.2/references/settings.md:412:>
content/docs/1.5.2/references/settings.md:413:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.2/references/settings.md:417:> Default: `enabled`
content/docs/1.5.2/references/settings.md:429:> Default: `fast-check`
content/docs/1.5.2/references/settings.md:438:> Default: `false`
content/docs/1.5.2/references/settings.md:444:> Default: `0 0 */7 * *`
content/docs/1.5.2/references/settings.md:447:> **Warning**
content/docs/1.5.2/references/settings.md:448:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.2/references/settings.md:454:> Default: `false`
content/docs/1.5.2/references/settings.md:462:> Default: `false`
content/docs/1.5.2/references/settings.md:466:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.2/references/settings.md:470:> Examples:  
content/docs/1.5.2/references/settings.md:471:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.2/references/settings.md:472:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.2/references/settings.md:473:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.2/references/settings.md:479:> Example: `s3-secret`
content/docs/1.5.2/references/settings.md:485:> Default: `300`
content/docs/1.5.2/references/settings.md:493:> Default: `1440`
content/docs/1.5.2/references/settings.md:501:> Default: `1`
content/docs/1.5.2/references/settings.md:510:> Default: `1`
content/docs/1.5.2/references/settings.md:518:> Default: `false`
content/docs/1.5.2/references/settings.md:523:> Default: `ignored`
content/docs/1.5.2/references/settings.md:533:> Default: `lz4`
content/docs/1.5.2/references/settings.md:545:> Default: `2`
content/docs/1.5.2/references/settings.md:551:> Default: `2`
content/docs/1.5.2/references/settings.md:559:> Default: `true`
content/docs/1.5.2/references/settings.md:563:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.2/references/settings.md:567:> Default: `true`
content/docs/1.5.2/references/settings.md:575:> Default: `false`
content/docs/1.5.2/references/settings.md:583:> Default: `true`
content/docs/1.5.2/references/settings.md:589:> **Note:**
content/docs/1.5.2/references/settings.md:590:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.2/references/settings.md:591:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.2/references/settings.md:595:> Default: `disabled`
content/docs/1.5.2/references/settings.md:611:> Default: `ignored`
content/docs/1.5.2/references/settings.md:625:> Default: `25`
content/docs/1.5.2/references/settings.md:633:> Default: `100`
content/docs/1.5.2/references/settings.md:643:> Default: `30`
content/docs/1.5.2/references/settings.md:653:> Default: `5`
content/docs/1.5.2/references/settings.md:659:> **WARNING:**
content/docs/1.5.2/references/settings.md:660:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.2/references/settings.md:661:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.2/references/settings.md:662:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.2/references/settings.md:667:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.2/references/settings.md:686:> Example: `high-priority`
content/docs/1.5.2/references/settings.md:697:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.2/references/settings.md:703:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.2/references/settings.md:710:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.2/references/settings.md:716:> Default: `false`
content/docs/1.5.2/references/settings.md:722:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.2/references/settings.md:726:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.2/references/settings.md:730:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.2/references/settings.md:736:> Example: `false`
content/docs/1.5.2/references/settings.md:748:> Default: `12`
content/docs/1.5.2/references/settings.md:760:> **Warning:**
content/docs/1.5.2/references/settings.md:761:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.2/references/settings.md:762:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.2/references/settings.md:763:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.2/references/settings.md:764:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.2/references/settings.md:765:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.2/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.2/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.2/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.2/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.2/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.2/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.2/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.2/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.2/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.2/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.2/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.2/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.2/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.2/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.2/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.2/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.2/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.2/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.2/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.2/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.2/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.2/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.2/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.2/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.2/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.2/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.2/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.2/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.2/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.2/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.2/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.2/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.2/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.2/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.2/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.2/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.2/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.2/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.2/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.2/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.2/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.2/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.2/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.2/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.2/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.2/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.2/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.2/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.2/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.2/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.5.2/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.2/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.2/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:50:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:61:       kubectl create secret generic <aws-secret> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:62:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:111:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:       kubectl create secret generic <aws-secret> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:123:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:139:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:kubectl create secret generic <aws-secret> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:159:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:160:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:161:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:264:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:334:  cat <<EOF >>cifs_secret.yml
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:349:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:402:    cat <<EOF >>azblob_secret.yml
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:423:    kubectl create secret generic <azblob-secret> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:424:      --from-literal=AZBLOB_ACCOUNT_NAME=<your-azure-storage-account-name> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:425:      --from-literal=AZBLOB_ACCOUNT_KEY=<your-azure-storage-account-key> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:426:      --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:427:      --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:428:      --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:432:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:446:   - If you set `<endpoint-suffix>` in the URL, the default endpoint suffix will be `core.windows.net`.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:447:   - If you set `AZBLOB_ENDPOINT` in the secret, Longhorn will use `AZBLOB_ENDPOINT` as your storage URL, and `<endpoint-suffix>` will not be used even if it has been set. 
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:52:To see the BackingImage, click **Setting > Backing Image** in the top navigation bar and click the BackingImage mentioned in the `VolumeSnapshotContent.snapshotHandle`.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:108:The parameters can be retrieved from the **Setting > Backing Image** page in the Longhorn UI.
content/docs/1.5.2/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.2/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.2/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:82:    > - `snapshot` with retain value set to 5
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:83:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:84:    >
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:85:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:121:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:129:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:137:kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:149:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/source=enabled
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:157:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:165:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:173:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.2/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.5.2/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.5.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.2/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.5.2/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.5.2/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.5.2/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.5.2/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.5.2/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.5.2/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.5.2/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.5.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.2/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.2/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.5.2/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.2/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.5.2/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.5.2/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.2/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.5.2/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.5.2/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.5.2/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.5.2/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.5.2/volumes-and-nodes/maintenance.md:27:   > **Note:**
content/docs/1.5.2/volumes-and-nodes/maintenance.md:28:   > By default, if there is one last healthy replica for a volume on
content/docs/1.5.2/volumes-and-nodes/maintenance.md:29:   > the node, Longhorn will prevent the node from completing the drain
content/docs/1.5.2/volumes-and-nodes/maintenance.md:30:   > operation, to protect the last replica and prevent the disruption of the
content/docs/1.5.2/volumes-and-nodes/maintenance.md:31:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/1.5.2/volumes-and-nodes/maintenance.md:32:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/1.5.2/volumes-and-nodes/maintenance.md:35:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/1.5.2/volumes-and-nodes/maintenance.md:36:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/1.5.2/volumes-and-nodes/maintenance.md:37:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/1.5.2/volumes-and-nodes/maintenance.md:38:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/1.5.2/volumes-and-nodes/maintenance.md:42:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/1.5.2/volumes-and-nodes/maintenance.md:43:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/1.5.2/volumes-and-nodes/maintenance.md:44:   > can be reused later, once the node back online and uncordoned.
content/docs/1.5.2/volumes-and-nodes/maintenance.md:77:        kubectl delete node <node-name>
content/docs/1.5.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.2/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.5.2/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:31:When using `fstrim`, you must identify the mount point of the volume and then run the command `fstrim <the mount point>`.
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:34:- RWX volume: The mount point is the share manager pod of the volume. The share manager pod contains the NFS server and is typically named `share-manager-<volume name>`.
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:39:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:41:1. Identify the work directory of the NFS server (for example, `/export/<volume name>`).
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:43:    mount | grep <volume name>
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:44:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:48:    fstrim /export/<volume name>
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:76:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:78:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.5.2/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.2/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.5.2/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.5.2/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.5.2/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.5.2/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.5.2/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.5.2/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.5.2/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.2/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.2/v2-data-engine/prerequisites.md:11:  > **NOTICE**
content/docs/1.5.2/v2-data-engine/prerequisites.md:12:  >
content/docs/1.5.2/v2-data-engine/prerequisites.md:13:  > Currently, V2 Data Engine only supports `x86_64` platform.
content/docs/1.5.2/v2-data-engine/quick-start.md:71:  echo 512 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
content/docs/1.5.2/v2-data-engine/quick-start.md:76:  echo "vm.nr_hugepages=512" >> /etc/sysctl.conf
content/docs/1.5.2/v2-data-engine/quick-start.md:121:Or, you can enable it in `Setting > General > V2 Data Engine`.
content/docs/1.5.2/v2-data-engine/quick-start.md:155:You can observe the utilization of allocated huge pages on each node by running the command `kubectl get node <node name> -o yaml`.
content/docs/1.5.2/v2-data-engine/quick-start.md:202:kubectl -n longhorn-system edit node.longhorn.io <NODE NAME>
content/docs/1.5.2/v2-data-engine/quick-start.md:207:<DISK NAME>:
content/docs/1.5.3/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.5.3/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.5.3/advanced-resources/backing-image.md:114:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.5.3/advanced-resources/backing-image.md:135:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.5.3/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.5.3/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.5.3/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.5.3 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.5.3/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.5.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.5.3`
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.5.3/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.3/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.5.3/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.3/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.5.3/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.5.3/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.5.3/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.5.3/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.5.3/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.5.3/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.5.3/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:173:In v1.5.3, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.5.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.3/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.3/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.3/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.3/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.3/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:168:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:174:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:179:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:188:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:191:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:195:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:39:   >
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.5.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.3/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.5.3/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.3/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.5.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.3/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.3/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.5.3/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.5.3/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.5.3/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.5.3/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.5.3/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.5.3/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.5.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.5.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.5.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.5.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.3/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.3/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.3/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.3/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.3/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.3/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.3/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:56:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:60:  > **Note:** Longhorn system restores volume with the latest backup. We recommend updating the last backup for all volumes. By taking volume backups, you ensure that the data is up-to-date with the system backup. For more information, please refer to the [Configuration - Volume Backup Policy](#volume-backup-policy) section.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:92:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:106:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:112:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:116:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.3/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.5.3/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.5.3/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.5.3/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.5.3/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.5.3/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.3/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.5.3/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.5.3/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.5.3/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.5.3/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.5.3/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.5.3/advanced-resources/rwx-workloads.md:36:> **Notice**  
content/docs/1.5.3/advanced-resources/rwx-workloads.md:37:> An RWX volume must have the access mode set to `ReadWriteMany` and the "migratable" flag disabled (*parameters.migratable: `false`*).
content/docs/1.5.3/advanced-resources/rwx-workloads.md:72:> **Important:**
content/docs/1.5.3/advanced-resources/rwx-workloads.md:73:> To create PVCs for RWX volumes using the sample StorageClass, replace the `nfsOptions` string with a customized comma-separated list of legal options.
content/docs/1.5.3/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.3/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.3/best-practices.md:124:> Recommend: `false`
content/docs/1.5.3/best-practices.md:130:> Recommend: `false`
content/docs/1.5.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.3/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.5.3/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.5.3/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.5.3/deploy/important-notes/index.md:55:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.5.3/deploy/important-notes/index.md:58:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.3/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.3/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.3/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.5.3/deploy/install/_index.md:153:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.5.3/deploy/install/_index.md:258:The `Server Version` should be >= v1.21.
content/docs/1.5.3/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.5.3/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.3/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.3/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.3/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.3/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.3/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.3/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.3/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.3/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.3/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.5.3/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.5.3/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.3/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.3/deploy/upgrade/_index.md:39:- To upgrade from v1.5.x (< v1.5.3) or v1.4.x, see [this section.](./longhorn-manager)
content/docs/1.5.3/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.5.x (< v1.5.3) or v1.4.x.
content/docs/1.5.3/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.5.x (< v1.5.3) or v1.4.x
content/docs/1.5.3/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.5.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.3/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.3/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.5.3/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.5.3/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.5.3/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.5.3/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.5.3/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.5.3/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.5.3/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.5.3/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.5.3/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.5.3/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.5.3/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.5.3/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.5.3/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.5.3/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.5.3/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.3/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.3/references/networking.md:76:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.3/references/networking.md:88:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.3/references/settings.md:86:> Default: `block-if-contains-last-replica`
content/docs/1.5.3/references/settings.md:97:> Default: `true`
content/docs/1.5.3/references/settings.md:103:> Default: `true`
content/docs/1.5.3/references/settings.md:110:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.3/references/settings.md:111:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.3/references/settings.md:112:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.3/references/settings.md:116:> Default: `true`
content/docs/1.5.3/references/settings.md:122:> Default: `0`
content/docs/1.5.3/references/settings.md:130:> Default: `5`
content/docs/1.5.3/references/settings.md:140:> Default: `false`
content/docs/1.5.3/references/settings.md:150:> Default: `longhorn.io/v1beta2`
content/docs/1.5.3/references/settings.md:156:> Default: `disabled`
content/docs/1.5.3/references/settings.md:174:> Default: `/var/lib/longhorn/`
content/docs/1.5.3/references/settings.md:188:> Default: `longhorn-static`
content/docs/1.5.3/references/settings.md:194:> Default: `3`
content/docs/1.5.3/references/settings.md:205:> Default: `false`
content/docs/1.5.3/references/settings.md:209:> Default: `false`
content/docs/1.5.3/references/settings.md:215:> Default: `true`
content/docs/1.5.3/references/settings.md:223:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.3/references/settings.md:227:> Default: `true`
content/docs/1.5.3/references/settings.md:235:  > This value may not be accurate for virtual machines.
content/docs/1.5.3/references/settings.md:308:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.3/references/settings.md:312:> Default: `do-nothing`
content/docs/1.5.3/references/settings.md:327:> Default: `600`
content/docs/1.5.3/references/settings.md:335:> Default: `if-not-present`
content/docs/1.5.3/references/settings.md:351:> Default: `60`
content/docs/1.5.3/references/settings.md:356:> Default: `300`
content/docs/1.5.3/references/settings.md:360:> **Note:**
content/docs/1.5.3/references/settings.md:361:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.3/references/settings.md:362:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.3/references/settings.md:365:> Default: `8`
content/docs/1.5.3/references/settings.md:376:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.3/references/settings.md:380:> Default: `1`
content/docs/1.5.3/references/settings.md:390:> Default: `false`
content/docs/1.5.3/references/settings.md:396:> Default: `30`
content/docs/1.5.3/references/settings.md:403:> Default: `false`
content/docs/1.5.3/references/settings.md:407:> **Warning**
content/docs/1.5.3/references/settings.md:408:>
content/docs/1.5.3/references/settings.md:409:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.3/references/settings.md:410:>
content/docs/1.5.3/references/settings.md:411:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.3/references/settings.md:412:>
content/docs/1.5.3/references/settings.md:413:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.3/references/settings.md:417:> Default: `enabled`
content/docs/1.5.3/references/settings.md:429:> Default: `fast-check`
content/docs/1.5.3/references/settings.md:438:> Default: `false`
content/docs/1.5.3/references/settings.md:444:> Default: `0 0 */7 * *`
content/docs/1.5.3/references/settings.md:447:> **Warning**
content/docs/1.5.3/references/settings.md:448:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.3/references/settings.md:454:> Default: `false`
content/docs/1.5.3/references/settings.md:462:> Default: `false`
content/docs/1.5.3/references/settings.md:466:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.3/references/settings.md:470:> Examples:  
content/docs/1.5.3/references/settings.md:471:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.3/references/settings.md:472:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.3/references/settings.md:473:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.3/references/settings.md:479:> Example: `s3-secret`
content/docs/1.5.3/references/settings.md:485:> Default: `300`
content/docs/1.5.3/references/settings.md:493:> Default: `1440`
content/docs/1.5.3/references/settings.md:501:> Default: `1`
content/docs/1.5.3/references/settings.md:510:> Default: `1`
content/docs/1.5.3/references/settings.md:518:> Default: `false`
content/docs/1.5.3/references/settings.md:523:> Default: `ignored`
content/docs/1.5.3/references/settings.md:533:> Default: `lz4`
content/docs/1.5.3/references/settings.md:545:> Default: `2`
content/docs/1.5.3/references/settings.md:551:> Default: `2`
content/docs/1.5.3/references/settings.md:559:> Default: `true`
content/docs/1.5.3/references/settings.md:563:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.3/references/settings.md:567:> Default: `true`
content/docs/1.5.3/references/settings.md:575:> Default: `false`
content/docs/1.5.3/references/settings.md:583:> Default: `true`
content/docs/1.5.3/references/settings.md:589:> **Note:**
content/docs/1.5.3/references/settings.md:590:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.3/references/settings.md:591:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.3/references/settings.md:595:> Default: `disabled`
content/docs/1.5.3/references/settings.md:611:> Default: `ignored`
content/docs/1.5.3/references/settings.md:625:> Default: `25`
content/docs/1.5.3/references/settings.md:633:> Default: `100`
content/docs/1.5.3/references/settings.md:643:> Default: `30`
content/docs/1.5.3/references/settings.md:653:> Default: `5`
content/docs/1.5.3/references/settings.md:659:> **WARNING:**
content/docs/1.5.3/references/settings.md:660:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.3/references/settings.md:661:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.3/references/settings.md:662:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.3/references/settings.md:667:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.3/references/settings.md:686:> Example: `high-priority`
content/docs/1.5.3/references/settings.md:697:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.3/references/settings.md:703:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.3/references/settings.md:710:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.3/references/settings.md:716:> Default: `false`
content/docs/1.5.3/references/settings.md:722:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.3/references/settings.md:726:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.3/references/settings.md:730:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.3/references/settings.md:736:> Example: `false`
content/docs/1.5.3/references/settings.md:748:> Default: `12`
content/docs/1.5.3/references/settings.md:760:> **Warning:**
content/docs/1.5.3/references/settings.md:761:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.3/references/settings.md:762:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.3/references/settings.md:763:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.3/references/settings.md:764:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.3/references/settings.md:765:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.3/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.3/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.3/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.3/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.3/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.3/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.3/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.3/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.3/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.3/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.3/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.3/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.3/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.3/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.3/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.3/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.3/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.3/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.3/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.3/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.3/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.3/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.3/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.3/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.3/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.3/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.3/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.3/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.3/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.3/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.3/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.3/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.3/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.3/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.3/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.3/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.3/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.3/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.3/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.3/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.3/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.3/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.3/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.3/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.3/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.3/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.3/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.3/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.3/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.3/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.5.3/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.3/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.3/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:50:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:61:       kubectl create secret generic <aws-secret> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:62:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:111:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:       kubectl create secret generic <aws-secret> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:123:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:139:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:kubectl create secret generic <aws-secret> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:159:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:160:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:161:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:264:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:334:  cat <<EOF >>cifs_secret.yml
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:349:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:402:    cat <<EOF >>azblob_secret.yml
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:423:    kubectl create secret generic <azblob-secret> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:424:      --from-literal=AZBLOB_ACCOUNT_NAME=<your-azure-storage-account-name> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:425:      --from-literal=AZBLOB_ACCOUNT_KEY=<your-azure-storage-account-key> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:426:      --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:427:      --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:428:      --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:432:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:446:   - If you set `<endpoint-suffix>` in the URL, the default endpoint suffix will be `core.windows.net`.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:447:   - If you set `AZBLOB_ENDPOINT` in the secret, Longhorn will use `AZBLOB_ENDPOINT` as your storage URL, and `<endpoint-suffix>` will not be used even if it has been set. 
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:52:To see the BackingImage, click **Setting > Backing Image** in the top navigation bar and click the BackingImage mentioned in the `VolumeSnapshotContent.snapshotHandle`.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:108:The parameters can be retrieved from the **Setting > Backing Image** page in the Longhorn UI.
content/docs/1.5.3/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.3/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.3/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:82:    > - `snapshot` with retain value set to 5
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:83:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:84:    >
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:85:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:121:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:129:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:137:kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:149:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/source=enabled
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:157:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:165:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:173:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.3/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.5.3/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.5.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.3/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.5.3/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.5.3/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.5.3/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.5.3/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.5.3/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.5.3/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.5.3/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.5.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.3/volumes-and-nodes/expansion.md:101:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.3/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/1.5.3/volumes-and-nodes/expansion.md:103:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.3/volumes-and-nodes/expansion.md:104:    resize2fs /dev/longhorn/<volume name>
content/docs/1.5.3/volumes-and-nodes/expansion.md:105:    umount /dev/longhorn/<volume name>
content/docs/1.5.3/volumes-and-nodes/expansion.md:111:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.3/volumes-and-nodes/expansion.md:112:    xfs_growfs <the mount directory>
content/docs/1.5.3/volumes-and-nodes/expansion.md:113:    umount /dev/longhorn/<volume name>
content/docs/1.5.3/volumes-and-nodes/expansion.md:122:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.5.3/volumes-and-nodes/expansion.md:130:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.5.3/volumes-and-nodes/expansion.md:132:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.5.3/volumes-and-nodes/maintenance.md:27:   > **Note:**
content/docs/1.5.3/volumes-and-nodes/maintenance.md:28:   > By default, if there is one last healthy replica for a volume on
content/docs/1.5.3/volumes-and-nodes/maintenance.md:29:   > the node, Longhorn will prevent the node from completing the drain
content/docs/1.5.3/volumes-and-nodes/maintenance.md:30:   > operation, to protect the last replica and prevent the disruption of the
content/docs/1.5.3/volumes-and-nodes/maintenance.md:31:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/1.5.3/volumes-and-nodes/maintenance.md:32:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/1.5.3/volumes-and-nodes/maintenance.md:35:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/1.5.3/volumes-and-nodes/maintenance.md:36:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/1.5.3/volumes-and-nodes/maintenance.md:37:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/1.5.3/volumes-and-nodes/maintenance.md:38:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/1.5.3/volumes-and-nodes/maintenance.md:42:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/1.5.3/volumes-and-nodes/maintenance.md:43:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/1.5.3/volumes-and-nodes/maintenance.md:44:   > can be reused later, once the node back online and uncordoned.
content/docs/1.5.3/volumes-and-nodes/maintenance.md:77:        kubectl delete node <node-name>
content/docs/1.5.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.3/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.5.3/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.5.3/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.3/volumes-and-nodes/volume-size.md:43:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.5.3/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.5.3/volumes-and-nodes/volume-size.md:50:     >
content/docs/1.5.3/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.5.3/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.5.3/volumes-and-nodes/volume-size.md:71:    >
content/docs/1.5.3/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:31:When using `fstrim`, you must identify the mount point of the volume and then run the command `fstrim <the mount point>`.
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:34:- RWX volume: The mount point is the share manager pod of the volume. The share manager pod contains the NFS server and is typically named `share-manager-<volume name>`.
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:39:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:41:1. Identify the work directory of the NFS server (for example, `/export/<volume name>`).
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:43:    mount | grep <volume name>
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:44:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:48:    fstrim /export/<volume name>
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:80:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:82:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.5.3/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.3/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.3/v2-data-engine/prerequisites.md:13:  > **NOTICE**
content/docs/1.5.3/v2-data-engine/prerequisites.md:14:  >
content/docs/1.5.3/v2-data-engine/prerequisites.md:15:  > Currently, V2 Data Engine only supports `x86_64` platform.
content/docs/1.5.3/v2-data-engine/quick-start.md:73:  echo 512 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
content/docs/1.5.3/v2-data-engine/quick-start.md:78:  echo "vm.nr_hugepages=512" >> /etc/sysctl.conf
content/docs/1.5.3/v2-data-engine/quick-start.md:123:Or, you can enable it in `Setting > General > V2 Data Engine`.
content/docs/1.5.3/v2-data-engine/quick-start.md:157:You can observe the utilization of allocated huge pages on each node by running the command `kubectl get node <node name> -o yaml`.
content/docs/1.5.3/v2-data-engine/quick-start.md:204:kubectl -n longhorn-system edit node.longhorn.io <NODE NAME>
content/docs/1.5.3/v2-data-engine/quick-start.md:209:<DISK NAME>:
content/docs/1.5.4/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.5.4/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.5.4/advanced-resources/backing-image.md:114:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.5.4/advanced-resources/backing-image.md:135:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.5.4/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.5.4/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.5.4/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.5.4 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.5.4/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.5.4/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.5.4`
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.5.4/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.4/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.4/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.5.4/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.4/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.5.4/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.5.4/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.5.4/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.5.4/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.5.4/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.5.4/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.5.4/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:173:In v1.5.4, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.5.4/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.4/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.4/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.5.4/advanced-resources/deploy/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.5.4/advanced-resources/deploy/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.5.4/advanced-resources/deploy/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:168:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:174:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:179:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:188:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:191:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:195:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:39:   >
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.5.4/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.4/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.4/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.4/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.5.4/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.4/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.5.4/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.4/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.4/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.4/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.5.4/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.5.4/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.5.4/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.5.4/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.5.4/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.5.4/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.5.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.5.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.5.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.5.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.4/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.4/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.4/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.4/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.4/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.4/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.4/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:56:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:60:  > **Note:** Longhorn system restores volume with the latest backup. We recommend updating the last backup for all volumes. By taking volume backups, you ensure that the data is up-to-date with the system backup. For more information, please refer to the [Configuration - Volume Backup Policy](#volume-backup-policy) section.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:92:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:106:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:112:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:116:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.4/advanced-resources/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.5.4/advanced-resources/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.5.4/advanced-resources/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.5.4/advanced-resources/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.5.4/advanced-resources/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.5.4/advanced-resources/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.4/advanced-resources/rwx-workloads.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.5.4/advanced-resources/rwx-workloads.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.5.4/advanced-resources/rwx-workloads.md:24:      > ```
content/docs/1.5.4/advanced-resources/rwx-workloads.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.5.4/advanced-resources/rwx-workloads.md:26:      > ```
content/docs/1.5.4/advanced-resources/rwx-workloads.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.5.4/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.5.4/advanced-resources/rwx-workloads.md:36:> **Notice**  
content/docs/1.5.4/advanced-resources/rwx-workloads.md:37:> An RWX volume must have the access mode set to `ReadWriteMany` and the "migratable" flag disabled (*parameters.migratable: `false`*).
content/docs/1.5.4/advanced-resources/rwx-workloads.md:72:> **Important:**
content/docs/1.5.4/advanced-resources/rwx-workloads.md:73:> To create PVCs for RWX volumes using the sample StorageClass, replace the `nfsOptions` string with a customized comma-separated list of legal options.
content/docs/1.5.4/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.4/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.5.4/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.5.4/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.5.4/deploy/important-notes/index.md:71:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.5.4/deploy/important-notes/index.md:74:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.4/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.4/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.4/deploy/install/_index.md:27:-  Kubernetes >= v1.21
content/docs/1.5.4/deploy/install/_index.md:153:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.5.4/deploy/install/_index.md:258:The `Server Version` should be >= v1.21.
content/docs/1.5.4/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.5.4/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.4/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.4/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.4/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.4/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.4/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.4/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.4/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.4/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.4/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.4/deploy/uninstall/_index.md:85:      > helm list -n longhorn-system -a
content/docs/1.5.4/deploy/uninstall/_index.md:92:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.5.4/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.4/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.4/deploy/upgrade/_index.md:39:- To upgrade from v1.5.x (< v1.5.4) or v1.4.x, see [this section.](./longhorn-manager)
content/docs/1.5.4/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.5.x (< v1.5.4) or v1.4.x.
content/docs/1.5.4/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.5.x (< v1.5.4) or v1.4.x
content/docs/1.5.4/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.5.4/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.4/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.4/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.4/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.5.4/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.5.4/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.5.4/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.5.4/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.5.4/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:113:      - to: <the email address to send notifications to>
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:114:        from: <the sender address>
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:115:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:117:        auth_username: <the username>
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:118:        auth_identity: <the identity>
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:119:        auth_password: <the password>
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:131:      - api_url: <the Slack webhook URL>
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:132:        channel: <the channel or user to send notifications to>
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:145:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:173:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:197:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:299:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:301:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:331:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:332:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:403:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.5.4/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.5.4/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.5.4/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.5.4/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.5.4/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.5.4/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.5.4/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.5.4/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.5.4/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.4/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.4/references/networking.md:76:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.4/references/networking.md:88:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.5.4/references/settings.md:87:> Default: `block-if-contains-last-replica`
content/docs/1.5.4/references/settings.md:111:> Default: `true`
content/docs/1.5.4/references/settings.md:117:> Default: `true`
content/docs/1.5.4/references/settings.md:124:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.4/references/settings.md:125:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.4/references/settings.md:126:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.4/references/settings.md:130:> Default: `true`
content/docs/1.5.4/references/settings.md:136:> Default: `0`
content/docs/1.5.4/references/settings.md:144:> Default: `5`
content/docs/1.5.4/references/settings.md:154:> Default: `false`
content/docs/1.5.4/references/settings.md:164:> Default: `longhorn.io/v1beta2`
content/docs/1.5.4/references/settings.md:170:> Default: `disabled`
content/docs/1.5.4/references/settings.md:188:> Default: `/var/lib/longhorn/`
content/docs/1.5.4/references/settings.md:202:> Default: `longhorn-static`
content/docs/1.5.4/references/settings.md:208:> Default: `3`
content/docs/1.5.4/references/settings.md:219:> Default: `false`
content/docs/1.5.4/references/settings.md:223:> Default: `false`
content/docs/1.5.4/references/settings.md:229:> Default: `true`
content/docs/1.5.4/references/settings.md:237:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.4/references/settings.md:241:> Default: `true`
content/docs/1.5.4/references/settings.md:249:  > This value may not be accurate for virtual machines.
content/docs/1.5.4/references/settings.md:322:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.4/references/settings.md:326:> Default: `do-nothing`
content/docs/1.5.4/references/settings.md:341:> Default: `600`
content/docs/1.5.4/references/settings.md:349:> Default: `if-not-present`
content/docs/1.5.4/references/settings.md:365:> Default: `60`
content/docs/1.5.4/references/settings.md:370:> Default: `300`
content/docs/1.5.4/references/settings.md:374:> **Note:**
content/docs/1.5.4/references/settings.md:375:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.4/references/settings.md:376:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.4/references/settings.md:379:> Default: `8`
content/docs/1.5.4/references/settings.md:390:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.4/references/settings.md:394:> Default: `1`
content/docs/1.5.4/references/settings.md:404:> Default: `false`
content/docs/1.5.4/references/settings.md:410:> Default: `30`
content/docs/1.5.4/references/settings.md:417:> Default: `false`
content/docs/1.5.4/references/settings.md:421:> **Warning**
content/docs/1.5.4/references/settings.md:422:>
content/docs/1.5.4/references/settings.md:423:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.4/references/settings.md:424:>
content/docs/1.5.4/references/settings.md:425:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.4/references/settings.md:426:>
content/docs/1.5.4/references/settings.md:427:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.4/references/settings.md:431:> Default: `enabled`
content/docs/1.5.4/references/settings.md:443:> Default: `fast-check`
content/docs/1.5.4/references/settings.md:452:> Default: `false`
content/docs/1.5.4/references/settings.md:458:> Default: `0 0 */7 * *`
content/docs/1.5.4/references/settings.md:461:> **Warning**
content/docs/1.5.4/references/settings.md:462:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.4/references/settings.md:468:> Default: `false`
content/docs/1.5.4/references/settings.md:476:> Default: `false`
content/docs/1.5.4/references/settings.md:480:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.4/references/settings.md:484:> Examples:  
content/docs/1.5.4/references/settings.md:485:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.4/references/settings.md:486:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.4/references/settings.md:487:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.4/references/settings.md:493:> Example: `s3-secret`
content/docs/1.5.4/references/settings.md:499:> Default: `300`
content/docs/1.5.4/references/settings.md:507:> Default: `1440`
content/docs/1.5.4/references/settings.md:515:> Default: `1`
content/docs/1.5.4/references/settings.md:524:> Default: `1`
content/docs/1.5.4/references/settings.md:532:> Default: `false`
content/docs/1.5.4/references/settings.md:537:> Default: `ignored`
content/docs/1.5.4/references/settings.md:547:> Default: `lz4`
content/docs/1.5.4/references/settings.md:559:> Default: `2`
content/docs/1.5.4/references/settings.md:565:> Default: `2`
content/docs/1.5.4/references/settings.md:573:> Default: `true`
content/docs/1.5.4/references/settings.md:577:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.4/references/settings.md:581:> Default: `true`
content/docs/1.5.4/references/settings.md:589:> Default: `false`
content/docs/1.5.4/references/settings.md:597:> Default: `true`
content/docs/1.5.4/references/settings.md:603:> **Note:**
content/docs/1.5.4/references/settings.md:604:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.4/references/settings.md:605:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.4/references/settings.md:609:> Default: `disabled`
content/docs/1.5.4/references/settings.md:625:> Default: `ignored`
content/docs/1.5.4/references/settings.md:639:> Default: `25`
content/docs/1.5.4/references/settings.md:647:> Default: `100`
content/docs/1.5.4/references/settings.md:657:> Default: `30`
content/docs/1.5.4/references/settings.md:667:> Default: `5`
content/docs/1.5.4/references/settings.md:673:> **WARNING:**
content/docs/1.5.4/references/settings.md:674:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.4/references/settings.md:675:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.4/references/settings.md:676:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.4/references/settings.md:681:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.4/references/settings.md:700:> Example: `high-priority`
content/docs/1.5.4/references/settings.md:711:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.4/references/settings.md:717:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.4/references/settings.md:724:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.4/references/settings.md:730:> Default: `false`
content/docs/1.5.4/references/settings.md:736:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.4/references/settings.md:740:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.4/references/settings.md:744:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.4/references/settings.md:750:> Example: `false`
content/docs/1.5.4/references/settings.md:762:> Default: `12`
content/docs/1.5.4/references/settings.md:774:> **Warning:**
content/docs/1.5.4/references/settings.md:775:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.4/references/settings.md:776:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.4/references/settings.md:777:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.4/references/settings.md:778:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.4/references/settings.md:779:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.4/references/settings.md:783:> Default: `false`
content/docs/1.5.4/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.4/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.4/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.4/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.4/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.4/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.4/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.4/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.4/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.4/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.4/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.4/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.4/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.4/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.4/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.4/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.4/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.4/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.4/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.4/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.4/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.4/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.4/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.4/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.4/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.4/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.4/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.4/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.4/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.4/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.4/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.4/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.4/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.4/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.4/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.4/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.4/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.4/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.4/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.4/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.4/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.4/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.4/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.4/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.4/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.4/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.4/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.4/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.4/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.4/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#configuring-volume-mount-options)
content/docs/1.5.4/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.4/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.4/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:50:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:61:       kubectl create secret generic <aws-secret> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:62:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:111:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:       kubectl create secret generic <aws-secret> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:123:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:139:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:kubectl create secret generic <aws-secret> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:159:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:160:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:161:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:264:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:334:  cat <<EOF >>cifs_secret.yml
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:349:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:402:    cat <<EOF >>azblob_secret.yml
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:423:    kubectl create secret generic <azblob-secret> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:424:      --from-literal=AZBLOB_ACCOUNT_NAME=<your-azure-storage-account-name> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:425:      --from-literal=AZBLOB_ACCOUNT_KEY=<your-azure-storage-account-key> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:426:      --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:427:      --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:428:      --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:432:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:446:   - If you set `<endpoint-suffix>` in the URL, the default endpoint suffix will be `core.windows.net`.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:447:   - If you set `AZBLOB_ENDPOINT` in the secret, Longhorn will use `AZBLOB_ENDPOINT` as your storage URL, and `<endpoint-suffix>` will not be used even if it has been set. 
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:52:To see the BackingImage, click **Setting > Backing Image** in the top navigation bar and click the BackingImage mentioned in the `VolumeSnapshotContent.snapshotHandle`.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:108:The parameters can be retrieved from the **Setting > Backing Image** page in the Longhorn UI.
content/docs/1.5.4/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.4/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.4/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:82:    > - `snapshot` with retain value set to 5
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:83:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:84:    >
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:85:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:121:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:129:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:137:kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:149:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/source=enabled
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:157:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:165:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:173:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.5.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.4/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.4/volumes-and-nodes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.5.4/volumes-and-nodes/create-volumes.md:159:Labels:          <none>
content/docs/1.5.4/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.4/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.4/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.5.4/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.5.4/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.5.4/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.5.4/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.5.4/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.5.4/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.5.4/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.5.4/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.4/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.4/volumes-and-nodes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.4/volumes-and-nodes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/1.5.4/volumes-and-nodes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.4/volumes-and-nodes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/1.5.4/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/1.5.4/volumes-and-nodes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.5.4/volumes-and-nodes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/1.5.4/volumes-and-nodes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/1.5.4/volumes-and-nodes/expansion.md:123:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.5.4/volumes-and-nodes/expansion.md:131:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.5.4/volumes-and-nodes/expansion.md:133:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.5.4/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.4/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.4/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.4/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.5.4/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.5.4/volumes-and-nodes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.4/volumes-and-nodes/volume-size.md:44:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.5.4/volumes-and-nodes/volume-size.md:50:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.5.4/volumes-and-nodes/volume-size.md:51:     >
content/docs/1.5.4/volumes-and-nodes/volume-size.md:52:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.5.4/volumes-and-nodes/volume-size.md:71:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.5.4/volumes-and-nodes/volume-size.md:72:    >
content/docs/1.5.4/volumes-and-nodes/volume-size.md:73:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.5.4/volumes-and-nodes/maintenance.md:28:   > **Note:** Volumes that are not attached through the CSI flow on the node (for example, manually attached using
content/docs/1.5.4/volumes-and-nodes/maintenance.md:29:   > UI) will not be automatically attached to new nodes by Kubernetes during the draining. Therefore, Longhorn will
content/docs/1.5.4/volumes-and-nodes/maintenance.md:30:   > prevent the node from completing the drain operation. The user will need to detach these volumes manually to
content/docs/1.5.4/volumes-and-nodes/maintenance.md:31:   > unblock the draining.
content/docs/1.5.4/volumes-and-nodes/maintenance.md:36:   > **Note:** By default, if there is one last healthy replica for a volume on the node, Longhorn will prevent the node
content/docs/1.5.4/volumes-and-nodes/maintenance.md:37:   > from completing the drain operation, to protect the last replica and prevent the disruption of the workload. You
content/docs/1.5.4/volumes-and-nodes/maintenance.md:38:   > can control this behavior with the setting [Node Drain Policy](../../references/settings#node-drain-policy), or
content/docs/1.5.4/volumes-and-nodes/maintenance.md:39:   > [evict the replica to other nodes before draining](../disks-or-nodes-eviction). See [Node Drain Policy
content/docs/1.5.4/volumes-and-nodes/maintenance.md:40:   > Recommendations](#node-drain-policy-recommendations) for considerations when selecting a policy.
content/docs/1.5.4/volumes-and-nodes/maintenance.md:48:   > **Note:** Normally you don't need to evict the replicas before the drain operation, as long as you have healthy
content/docs/1.5.4/volumes-and-nodes/maintenance.md:49:   > replicas on other nodes. The replicas can be reused later, once the node back online and uncordoned. See [Node
content/docs/1.5.4/volumes-and-nodes/maintenance.md:50:   > Drain Policy](#node-drain-policy-recommendations) for further guidance.
content/docs/1.5.4/volumes-and-nodes/maintenance.md:88:      kubectl delete node <node-name>
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:31:When using `fstrim`, you must identify the mount point of the volume and then run the command `fstrim <the mount point>`.
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:34:- RWX volume: The mount point is the share manager pod of the volume. The share manager pod contains the NFS server and is typically named `share-manager-<volume name>`.
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:39:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:41:1. Identify the work directory of the NFS server (for example, `/export/<volume name>`).
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:43:    mount | grep <volume name>
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:44:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:48:    fstrim /export/<volume name>
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:80:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:82:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.5.4/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.4/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.4/best-practices.md:124:> Recommend: `false`
content/docs/1.5.4/best-practices.md:130:> Recommend: `false`
content/docs/1.5.4/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.4/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.4/v2-data-engine/prerequisites.md:11:  > **NOTICE**
content/docs/1.5.4/v2-data-engine/prerequisites.md:12:  >
content/docs/1.5.4/v2-data-engine/prerequisites.md:13:  > Currently, V2 Data Engine only supports `x86_64` platform.
content/docs/1.5.4/v2-data-engine/quick-start.md:71:  echo 512 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
content/docs/1.5.4/v2-data-engine/quick-start.md:76:  echo "vm.nr_hugepages=512" >> /etc/sysctl.conf
content/docs/1.5.4/v2-data-engine/quick-start.md:121:Or, you can enable it in `Setting > General > V2 Data Engine`.
content/docs/1.5.4/v2-data-engine/quick-start.md:155:You can observe the utilization of allocated huge pages on each node by running the command `kubectl get node <node name> -o yaml`.
content/docs/1.5.4/v2-data-engine/quick-start.md:202:kubectl -n longhorn-system edit node.longhorn.io <NODE NAME>
content/docs/1.5.4/v2-data-engine/quick-start.md:207:<DISK NAME>:
content/docs/1.6.0/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.6.0`
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.6.0/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.6.0/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.6.0/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.6.0 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.6.0/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.6.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:99:> **NOTE:**
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:100:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:170:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:176:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:181:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:190:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:193:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:197:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.6.0/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.6.0/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.6.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.6.0/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:39:   >
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:48:> **Warning**:
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:49:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:50:> * When all Longhorn volumes are detached, the customized settings are immediately applied to the system-managed components (for example, Instance manager, CSI driver and Engine images).
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:51:> * When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:77:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.6.0/advanced-resources/deploy/priority-class.md:21:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.6.0/advanced-resources/deploy/priority-class.md:22:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.6.0/advanced-resources/deploy/priority-class.md:23:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.6.0/advanced-resources/deploy/priority-class.md:27:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:15:> **Important**: To ensure that your preferred settings are immediately applied, stop all workloads and detach all Longhorn volumes before configuring the settings.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:17:> When all volumes are detached, Longhorn attempts to restart all Instance Manager and Backing Image Manager pods to apply the setting.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:18:> When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:31:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:32:>
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:33:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:34:>
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:35:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:41:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:42:>
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:43:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:59:> To ensure that your preferred toleration settings are immediately applied, stop all workloads and detach all Longhorn volumes before configuring the settings.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:62:>
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:63:> When all Longhorn volumes are detached, the customized setting is immediately applied to the system-managed components.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:64:> When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:65:>
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:66:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:101:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.6.0/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.6.0/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.6.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.6.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.6.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.6.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.6.0/advanced-resources/os-distro-specific/okd-support.md:62:cat <<EOF >>auto-mount-machineconfig.yaml
content/docs/1.6.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.6.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.6.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:56:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:60:  > **Note:** Longhorn system restores volume with the latest backup. We recommend updating the last backup for all volumes. By taking volume backups, you ensure that the data is up-to-date with the system backup. For more information, please refer to the [Configuration - Volume Backup Policy](#volume-backup-policy) section.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:92:   > kubectl -n longhorn-system get systembackup
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:106:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:112:   > kubectl -n longhorn-system get systembackup
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:116:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.6.0/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.6.0/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.6.0/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.6.0/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:46:> **IMPORTANT:**
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:47:> - `name`: Use the same name for the backing image and its backup. If the names are not identical, Longhorn will not be able to find the backing image.
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:48:> - `userCreated`: Set the value to `true` to indicate that you created the backup custom resource, which enabled the creation of the backup in the backupstore. The value `false` indicates that the backup custom resource was synced from the backupstore.
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:49:> - `labels`: You can add labels to the backing image backup.
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:71:> **IMPORTANT:**
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:72:> - `sourceType`: Set the value to `restore`.
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:73:> - `sourceParameters`: Configure the following parameters:
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:74:>   - `backup-url`: URL of the backing image resource in the backupstore. You can find this information in the status of the backup custom resource `.Status.URL`.
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:75:>   - `concurrent-limit`: Maximum number of worker threads that can concurrently run for each restore operation. When unspecified, Longhorn uses the default value.
content/docs/1.6.0/advanced-resources/backing-image/backing-image-backup.md:76:> - `checksum`: You can specify the expected SHA-512 checksum of the backing image file, which Longhorn uses to validate the restored file. When unspecified, Longhorn uses the checksum of the restored file as the truth.
content/docs/1.6.0/advanced-resources/backing-image/backing-image.md:23:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.6.0/advanced-resources/backing-image/backing-image.md:30:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.6.0/advanced-resources/backing-image/backing-image.md:115:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.6.0/advanced-resources/backing-image/backing-image.md:136:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.6.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.6.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.6.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.6.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.6.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.6.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.6.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.6.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.6.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.6.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.6.0/advanced-resources/driver-migration/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.6.0/advanced-resources/rebuilding/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.6.0/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.6.0/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.6.0/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.6.0/deploy/important-notes/index.md:81:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.6.0/deploy/important-notes/index.md:84:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.6.0/deploy/important-notes/index.md:120:> **Warning:**
content/docs/1.6.0/deploy/important-notes/index.md:121:> Whenever engine upgrade enforcement causes upgrade failure, Longhorn allows you to revert to the previous version because Longhorn Manager will block the entire upgrade. However, Longhorn prohibits downgrading when an upgrade is successful. For more information, see [Upgrade Path Enforcement](../../deploy/upgrade/#upgrade-path-enforcement-and-downgrade-prevention).
content/docs/1.6.0/deploy/install/install-with-fleet.md:19:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.6.0/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.0/deploy/install/install-with-fleet.md:94:    cat > longhorn-gitrepo.yaml << "EOF"
content/docs/1.6.0/deploy/install/install-with-flux.md:15:    export GITHUB_TOKEN=<gh-token>
content/docs/1.6.0/deploy/install/install-with-flux.md:18:      --owner=<github_username> \
content/docs/1.6.0/deploy/install/install-with-flux.md:19:      --repository=<github_repo_name> \
content/docs/1.6.0/deploy/install/install-with-flux.md:20:      --branch=<branch_name> \
content/docs/1.6.0/deploy/install/install-with-flux.md:21:      --path=<folder_path_within_repo> \
content/docs/1.6.0/deploy/install/install-with-flux.md:25:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.6.0/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.0/deploy/install/install-with-flux.md:36:      --export > helmrepo.yaml
content/docs/1.6.0/deploy/install/install-with-flux.md:47:      --export > helmrelease.yaml
content/docs/1.6.0/deploy/install/install-with-flux.md:97:    git push origin <branch_name>
content/docs/1.6.0/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.6.0/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.6.0/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.6.0/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.6.0/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.6.0/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.6.0/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.6.0/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.6.0/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.6.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.6.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.6.0/deploy/install/_index.md:30:-  Kubernetes >= v1.21
content/docs/1.6.0/deploy/install/_index.md:157:  echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.6.0/deploy/install/_index.md:266:The `Server Version` should be >= v1.21.
content/docs/1.6.0/deploy/install/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.6.0/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.6.0/deploy/install/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.6.0/deploy/install/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.6.0/deploy/install/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.6.0/deploy/install/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.6.0/deploy/install/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.6.0/deploy/install/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.6.0/deploy/install/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.6.0/deploy/install/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.6.0/deploy/install/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.6.0/deploy/install/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.6.0/deploy/install/airgap.md:173:In v1.6.0, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.6.0/deploy/install/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.6.0/deploy/install/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.6.0/deploy/install/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.6.0/deploy/install/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.6.0/deploy/install/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.6.0/deploy/install/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.6.0/deploy/install/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.6.0/deploy/install/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.6.0/deploy/install/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.6.0/deploy/install/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.6.0/deploy/install/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.6.0/deploy/install/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.6.0/deploy/install/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.6.0/deploy/install/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.6.0/deploy/install/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.6.0/deploy/install/install-with-argocd.md:18:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.6.0/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.0/deploy/install/install-with-argocd.md:37:    cat > longhorn-application.yaml <<EOF
content/docs/1.6.0/deploy/uninstall/_index.md:69:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.6.0/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.6.0/deploy/uninstall/_index.md:88:      > helm list -n longhorn-system -a
content/docs/1.6.0/deploy/uninstall/_index.md:95:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.6.0/deploy/upgrade/_index.md:18:> **Warning**:
content/docs/1.6.0/deploy/upgrade/_index.md:19:> - Once you successfully upgrade to v1.6.0, you will not be allowed to revert to the previously installed version.
content/docs/1.6.0/deploy/upgrade/_index.md:20:> - The Downgrade Prevention feature was introduced in v1.5.0 so Longhorn is unable to prevent downgrade attempts in older versions.
content/docs/1.6.0/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.6.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.6.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.6.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.6.0/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.6.0/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.6.0/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.6.0/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.6.0/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.6.0/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:113:      - to: <the email address to send notifications to>
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:114:        from: <the sender address>
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:115:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:117:        auth_username: <the username>
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:118:        auth_identity: <the identity>
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:119:        auth_password: <the password>
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:131:      - api_url: <the Slack webhook URL>
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:132:        channel: <the channel or user to send notifications to>
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:145:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:173:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:197:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:299:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:301:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:331:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:332:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:403:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.6.0/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.6.0/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.6.0/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.6.0/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.6.0/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.6.0/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.6.0/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.6.0/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.6.0/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.6.0/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.6.0/references/networking.md:76:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.6.0/references/networking.md:88:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.6.0/references/helm-values.md:243:| defaultSettings.replicaZoneSoftAntiAffinity | Setting that allows Longhorn to schedule new replicas of a volume to nodes in the same zone as existing healthy replicas. Nodes that do not belong to any zone are treated as existing in the zone that contains healthy replicas. When identifying zones, Longhorn relies on the label "topology.kubernetes.io/zone=<Zone name of the node>" in the Kubernetes node object. |
content/docs/1.6.0/references/settings.md:94:> Default: `block-if-contains-last-replica`
content/docs/1.6.0/references/settings.md:118:> Default: `false`
content/docs/1.6.0/references/settings.md:125:> Default: `true`
content/docs/1.6.0/references/settings.md:131:> Default: `true`
content/docs/1.6.0/references/settings.md:139:> Default: `true`
content/docs/1.6.0/references/settings.md:146:> **Note:** This setting doesn't apply to below cases.
content/docs/1.6.0/references/settings.md:147:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.6.0/references/settings.md:148:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../nodes-and-volumes/volumes/rwx-volumes).
content/docs/1.6.0/references/settings.md:152:> Default: `true`
content/docs/1.6.0/references/settings.md:158:> Default: `0`
content/docs/1.6.0/references/settings.md:166:> Default: `5`
content/docs/1.6.0/references/settings.md:176:> Default: `false`
content/docs/1.6.0/references/settings.md:186:> Default: `longhorn.io/v1beta2`
content/docs/1.6.0/references/settings.md:192:> Default: `disabled`
content/docs/1.6.0/references/settings.md:210:> Default: `/var/lib/longhorn/`
content/docs/1.6.0/references/settings.md:224:> Default: `longhorn-static`
content/docs/1.6.0/references/settings.md:230:> Default: `3`
content/docs/1.6.0/references/settings.md:241:> Default: `false`
content/docs/1.6.0/references/settings.md:245:> Default: `false`
content/docs/1.6.0/references/settings.md:251:> Default: `true`
content/docs/1.6.0/references/settings.md:259:> Only available if `Upgrade Checker` is enabled.
content/docs/1.6.0/references/settings.md:263:> Default: `true`
content/docs/1.6.0/references/settings.md:271:  > This value may not be accurate for virtual machines.
content/docs/1.6.0/references/settings.md:348:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.6.0/references/settings.md:352:> Default: `do-nothing`
content/docs/1.6.0/references/settings.md:367:> Default: `600`
content/docs/1.6.0/references/settings.md:375:> Default: `if-not-present`
content/docs/1.6.0/references/settings.md:391:> Default: `60`
content/docs/1.6.0/references/settings.md:396:> Default: `300`
content/docs/1.6.0/references/settings.md:400:> **Note:**
content/docs/1.6.0/references/settings.md:401:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.6.0/references/settings.md:402:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.6.0/references/settings.md:405:> Default: `8`
content/docs/1.6.0/references/settings.md:416:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.6.0/references/settings.md:420:> Default: `1`
content/docs/1.6.0/references/settings.md:430:> Default: `false`
content/docs/1.6.0/references/settings.md:436:> Default: `30`
content/docs/1.6.0/references/settings.md:442:> Default: `true`
content/docs/1.6.0/references/settings.md:449:> Default: `false`
content/docs/1.6.0/references/settings.md:453:> **Warning**
content/docs/1.6.0/references/settings.md:454:>
content/docs/1.6.0/references/settings.md:455:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.6.0/references/settings.md:456:>
content/docs/1.6.0/references/settings.md:457:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.6.0/references/settings.md:461:> Default: `2048`
content/docs/1.6.0/references/settings.md:467:> Default: `1250`
content/docs/1.6.0/references/settings.md:471:> **Warning:**
content/docs/1.6.0/references/settings.md:472:>  - Specifying a value of 0 disables CPU requests for instance manager pods. You must specify an integer between 1000 and 8000. 
content/docs/1.6.0/references/settings.md:473:>  - This is a global setting. Modifying the value triggers an automatic restart of the Instance Manager pods. However, V2 Instance Manager pods that use this setting are restarted only when no instances are running.
content/docs/1.6.0/references/settings.md:477:> Default: `enabled`
content/docs/1.6.0/references/settings.md:489:> Default: `fast-check`
content/docs/1.6.0/references/settings.md:498:> Default: `false`
content/docs/1.6.0/references/settings.md:504:> Default: `0 0 */7 * *`
content/docs/1.6.0/references/settings.md:507:> **Warning**
content/docs/1.6.0/references/settings.md:508:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.6.0/references/settings.md:514:> Default: `false`
content/docs/1.6.0/references/settings.md:522:> Default: `false`
content/docs/1.6.0/references/settings.md:526:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.6.0/references/settings.md:530:> Examples:  
content/docs/1.6.0/references/settings.md:531:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.6.0/references/settings.md:532:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.6.0/references/settings.md:533:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.6.0/references/settings.md:539:> Example: `s3-secret`
content/docs/1.6.0/references/settings.md:545:> Default: `300`
content/docs/1.6.0/references/settings.md:553:> Default: `1440`
content/docs/1.6.0/references/settings.md:561:> Default: `1`
content/docs/1.6.0/references/settings.md:570:> Default: `1`
content/docs/1.6.0/references/settings.md:578:> Default: `false`
content/docs/1.6.0/references/settings.md:583:> Default: `ignored`
content/docs/1.6.0/references/settings.md:593:> Default: `lz4`
content/docs/1.6.0/references/settings.md:605:> Default: `2`
content/docs/1.6.0/references/settings.md:611:> Default: `2`
content/docs/1.6.0/references/settings.md:619:> Default: `true`
content/docs/1.6.0/references/settings.md:623:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.6.0/references/settings.md:627:> Default: `true`
content/docs/1.6.0/references/settings.md:635:> Default: `false`
content/docs/1.6.0/references/settings.md:641:> **Note:**
content/docs/1.6.0/references/settings.md:642:>   - This setting is superseded if replicas are forbidden to share a zone by the Replica Zone Level Anti-Affinity setting.
content/docs/1.6.0/references/settings.md:646:> Default: `true`
content/docs/1.6.0/references/settings.md:652:> **Note:**
content/docs/1.6.0/references/settings.md:653:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.6.0/references/settings.md:654:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.6.0/references/settings.md:658:> Default: `true`
content/docs/1.6.0/references/settings.md:664:> **Note:**
content/docs/1.6.0/references/settings.md:665:>   - Even if the setting is "true" and disk sharing is allowed, Longhorn will seek to use a different disk if possible, even if on the same node.
content/docs/1.6.0/references/settings.md:666:>   - This setting is superseded if replicas are forbidden to share a zone or a node by either of the other Soft Anti-Affinity settings.
content/docs/1.6.0/references/settings.md:670:> Default: `disabled`
content/docs/1.6.0/references/settings.md:686:> Default: `ignored`
content/docs/1.6.0/references/settings.md:700:> Default: `25`
content/docs/1.6.0/references/settings.md:708:> Default: `100`
content/docs/1.6.0/references/settings.md:718:> Default: `30`
content/docs/1.6.0/references/settings.md:726:> Default: `true`
content/docs/1.6.0/references/settings.md:732:> Default: `true`
content/docs/1.6.0/references/settings.md:760:> Default: `5`
content/docs/1.6.0/references/settings.md:766:> **WARNING:**
content/docs/1.6.0/references/settings.md:767:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.6.0/references/settings.md:768:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.6.0/references/settings.md:769:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.6.0/references/settings.md:774:> Example: `nodetype=storage:NoSchedule`
content/docs/1.6.0/references/settings.md:793:> Default: `longhorn-critical`
content/docs/1.6.0/references/settings.md:804:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.6.0/references/settings.md:810:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.6.0/references/settings.md:817:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.0/references/settings.md:823:> Default: `false`
content/docs/1.6.0/references/settings.md:829:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.6.0/references/settings.md:833:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.6.0/references/settings.md:837:> **Warning:** This setting should change after all Longhorn volumes are detached because some pods that run Longhorn system components are recreated to apply the setting. When all volumes are detached, Longhorn attempts to restart all Instance Manager and Backing Image Manager pods immediately. When volumes are in use, Longhorn components are not restarted, and you need to reconfigure the settings after detaching the remaining volumes; otherwise, you can wait for the setting change to be reconciled in an hour.
content/docs/1.6.0/references/settings.md:843:> Example: `false`
content/docs/1.6.0/references/settings.md:855:> Default: `12`
content/docs/1.6.0/references/settings.md:867:> **Warning:**
content/docs/1.6.0/references/settings.md:868:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.6.0/references/settings.md:869:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.6.0/references/settings.md:870:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.6.0/references/settings.md:871:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.6.0/references/settings.md:872:>  - After the setting is changed, the V1 Instance Manager pods that use this setting are automatically restarted when no instances are running.
content/docs/1.6.0/references/settings.md:876:> Default: `false`
content/docs/1.6.0/references/storage-class-parameters.md:52:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.6.0/references/storage-class-parameters.md:55:> Default: `true`  
content/docs/1.6.0/references/storage-class-parameters.md:56:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.6.0/references/storage-class-parameters.md:59:> Default: `Delete`  
content/docs/1.6.0/references/storage-class-parameters.md:60:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.6.0/references/storage-class-parameters.md:63:> Default `[]`  
content/docs/1.6.0/references/storage-class-parameters.md:64:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.6.0/references/storage-class-parameters.md:67:> Default `Immediate`  
content/docs/1.6.0/references/storage-class-parameters.md:68:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.6.0/references/storage-class-parameters.md:75:> Default: `3`  
content/docs/1.6.0/references/storage-class-parameters.md:81:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.6.0/references/storage-class-parameters.md:84:> Default: `30`
content/docs/1.6.0/references/storage-class-parameters.md:89:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:90:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.6.0/references/storage-class-parameters.md:95:> Default: `ext4`  
content/docs/1.6.0/references/storage-class-parameters.md:96:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../nodes-and-volumes/volumes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.6.0/references/storage-class-parameters.md:99:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:100:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../nodes-and-volumes/volumes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.6.0/references/storage-class-parameters.md:103:> Default: `false`  
content/docs/1.6.0/references/storage-class-parameters.md:108:> Default: `false`  
content/docs/1.6.0/references/storage-class-parameters.md:109:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.6.0/references/storage-class-parameters.md:112:> Default: `disabled`  
content/docs/1.6.0/references/storage-class-parameters.md:119:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.6.0/references/storage-class-parameters.md:120:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.6.0/references/storage-class-parameters.md:123:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:129:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.6.0/references/storage-class-parameters.md:130:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.6.0/references/storage-class-parameters.md:133:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:134:> Example: `"ssd,fast"`  
content/docs/1.6.0/references/storage-class-parameters.md:137:> More details in [Storage Tags](../../nodes-and-volumes/nodes/storage-tags)
content/docs/1.6.0/references/storage-class-parameters.md:140:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:141:> Example: `"storage,fast"`  
content/docs/1.6.0/references/storage-class-parameters.md:144:> More details in [Storage Tags](../../nodes-and-volumes/nodes/storage-tags)
content/docs/1.6.0/references/storage-class-parameters.md:147:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:148:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.6.0/references/storage-class-parameters.md:151:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.6.0/references/storage-class-parameters.md:154:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:155:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.0/references/storage-class-parameters.md:158:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:159:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.0/references/storage-class-parameters.md:162:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:163:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.0/references/storage-class-parameters.md:166:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:167:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.0/references/storage-class-parameters.md:170:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:175:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.6.0/references/storage-class-parameters.md:176:> More details in [Trim Filesystem](../../nodes-and-volumes/volumes/trim-filesystem).
content/docs/1.6.0/references/storage-class-parameters.md:179:> Default: `false`  
content/docs/1.6.0/references/storage-class-parameters.md:181:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.6.0/references/storage-class-parameters.md:182:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.6.0/references/storage-class-parameters.md:185:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:190:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.6.0/references/storage-class-parameters.md:191:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.6.0/references/storage-class-parameters.md:194:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:199:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.6.0/references/storage-class-parameters.md:200:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling).
content/docs/1.6.0/references/storage-class-parameters.md:203:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:208:> Global setting: [Replica Disk Level Soft Anti-Affinity](../settings#replica-disk-level-soft-anti-affinity).  
content/docs/1.6.0/references/storage-class-parameters.md:209:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling).
content/docs/1.6.0/references/storage-class-parameters.md:212:> Default: `""`
content/docs/1.6.0/references/storage-class-parameters.md:213:> Example: `"hard,sync"`  
content/docs/1.6.0/references/storage-class-parameters.md:218:> More details in [RWX Workloads](../../nodes-and-volumes/volumes/rwx-volumes#configuring-volume-mount-options)
content/docs/1.6.0/references/storage-class-parameters.md:221:> Default: `"v1"`  
content/docs/1.6.0/references/storage-class-parameters.md:225:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.6.0/references/storage-class-parameters.md:226:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:50:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:61:       kubectl create secret generic <aws-secret> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:62:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:111:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:       kubectl create secret generic <aws-secret> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:123:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:139:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:kubectl create secret generic <aws-secret> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:159:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:160:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:161:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:264:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:334:  cat <<EOF >>cifs_secret.yml
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:349:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:402:    cat <<EOF >>azblob_secret.yml
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:423:    kubectl create secret generic <azblob-secret> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:424:      --from-literal=AZBLOB_ACCOUNT_NAME=<your-azure-storage-account-name> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:425:      --from-literal=AZBLOB_ACCOUNT_KEY=<your-azure-storage-account-key> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:426:      --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:427:      --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:428:      --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:432:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:446:   - If you set `<endpoint-suffix>` in the URL, the default endpoint suffix will be `core.windows.net`.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:447:   - If you set `AZBLOB_ENDPOINT` in the secret, Longhorn will use `AZBLOB_ENDPOINT` as your storage URL, and `<endpoint-suffix>` will not be used even if it has been set. 
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:52:To see the BackingImage, click **Setting > Backing Image** in the top navigation bar and click the BackingImage mentioned in the `VolumeSnapshotContent.snapshotHandle`.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:108:The parameters can be retrieved from the **Setting > Backing Image** page in the Longhorn UI.
content/docs/1.6.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.6.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.6.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:82:    > - `snapshot` with retain value set to 5
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:83:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:84:    >
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:85:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:121:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:129:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:137:kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:149:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/source=enabled
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:157:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:165:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:173:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.6.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.6.0/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.6.0/troubleshoot/troubleshooting.md:28:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.6.0/v2-data-engine/prerequisites.md:13:  > **NOTICE**
content/docs/1.6.0/v2-data-engine/prerequisites.md:14:  >
content/docs/1.6.0/v2-data-engine/prerequisites.md:15:  >  AMD64 CPUs require SSE4.2 instruction support.
content/docs/1.6.0/v2-data-engine/prerequisites.md:20:  > **NOTICE**
content/docs/1.6.0/v2-data-engine/prerequisites.md:21:  >
content/docs/1.6.0/v2-data-engine/prerequisites.md:22:  > Host machines with Linux kernel 5.15 may unexpectedly reboot when volume-related IO errors occur. Update the Linux kernel on Longhorn nodes to version 5.19 or later to prevent such issues.
content/docs/1.6.0/v2-data-engine/quick-start.md:75:  echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
content/docs/1.6.0/v2-data-engine/quick-start.md:80:  echo "vm.nr_hugepages=1024" >> /etc/sysctl.conf
content/docs/1.6.0/v2-data-engine/quick-start.md:125:Or, you can enable it in `Setting > General > V2 Data Engine`.
content/docs/1.6.0/v2-data-engine/quick-start.md:156:You can observe the utilization of allocated huge pages on each node by running the command `kubectl get node <node name> -o yaml`.
content/docs/1.6.0/v2-data-engine/quick-start.md:203:kubectl -n longhorn-system edit node.longhorn.io <NODE NAME>
content/docs/1.6.0/v2-data-engine/quick-start.md:208:<DISK NAME>:
content/docs/1.6.0/best-practices.md:52:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.6.0/best-practices.md:176:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.6.0/best-practices.md:192:> Recommend: `false`
content/docs/1.6.0/best-practices.md:198:> Recommend: `false`
content/docs/1.6.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.6.0/maintenance/maintenance.md:34:   > **Note:** Volumes that are not attached through the CSI flow on the node (for example, manually attached using
content/docs/1.6.0/maintenance/maintenance.md:35:   > UI) will not be automatically attached to new nodes by Kubernetes during the draining. Therefore, Longhorn will
content/docs/1.6.0/maintenance/maintenance.md:36:   > prevent the node from completing the drain operation. The user will need to detach these volumes manually to
content/docs/1.6.0/maintenance/maintenance.md:37:   > unblock the draining.
content/docs/1.6.0/maintenance/maintenance.md:42:   > **Note:** By default, if there is one last healthy replica for a volume on the node, Longhorn will prevent the node
content/docs/1.6.0/maintenance/maintenance.md:43:   > from completing the drain operation, to protect the last replica and prevent the disruption of the workload. You
content/docs/1.6.0/maintenance/maintenance.md:44:   > can control this behavior with the setting [Node Drain Policy](../../references/settings#node-drain-policy), or
content/docs/1.6.0/maintenance/maintenance.md:45:   > [evict the replica to other nodes before draining](../../nodes-and-volumes/nodes/disks-or-nodes-eviction). See [Node Drain Policy
content/docs/1.6.0/maintenance/maintenance.md:46:   > Recommendations](#node-drain-policy-recommendations) for considerations when selecting a policy.
content/docs/1.6.0/maintenance/maintenance.md:54:   > **Note:** Normally you don't need to evict the replicas before the drain operation, as long as you have healthy
content/docs/1.6.0/maintenance/maintenance.md:55:   > replicas on other nodes. The replicas can be reused later, once the node back online and uncordoned. See [Node
content/docs/1.6.0/maintenance/maintenance.md:56:   > Drain Policy](#node-drain-policy-recommendations) for further guidance.
content/docs/1.6.0/maintenance/maintenance.md:94:      kubectl delete node <node-name>
content/docs/1.6.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.6.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.6.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.6.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.6.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.6.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.6.0/nodes-and-volumes/nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.6.0/nodes-and-volumes/nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.6.0/nodes-and-volumes/nodes/multidisk.md:24:> **Note**:
content/docs/1.6.0/nodes-and-volumes/nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.6.0/nodes-and-volumes/nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.6.0/nodes-and-volumes/nodes/scheduling.md:53:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.6.0/nodes-and-volumes/nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.6.0/nodes-and-volumes/volumes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.6.0/nodes-and-volumes/volumes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/create-volumes.md:159:Labels:          <none>
content/docs/1.6.0/nodes-and-volumes/volumes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.6.0/nodes-and-volumes/volumes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.6.0/nodes-and-volumes/volumes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:123:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:131:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/expansion.md:133:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:24:      > ```
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:26:      > ```
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.6.0/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:36:> **Notice**  
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:37:> An RWX volume must have the access mode set to `ReadWriteMany` and the "migratable" flag disabled (*parameters.migratable: `false`*).
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:72:> **Important:**
content/docs/1.6.0/nodes-and-volumes/volumes/rwx-volumes.md:73:> To create PVCs for RWX volumes using the sample StorageClass, replace the `nfsOptions` string with a customized comma-separated list of legal options.
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:8:> **Note:**
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:10:>
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:31:When using `fstrim`, you must identify the mount point of the volume and then run the command `fstrim <the mount point>`.
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:34:- RWX volume: The mount point is the share manager pod of the volume. The share manager pod contains the NFS server and is typically named `share-manager-<volume name>`.
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:39:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:41:1. Identify the work directory of the NFS server (for example, `/export/<volume name>`).
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:43:    mount | grep <volume name>
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:44:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:48:    fstrim /export/<volume name>
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:80:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.6.0/nodes-and-volumes/volumes/trim-filesystem.md:82:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:44:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:50:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:51:     >
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:52:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:71:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:72:    >
content/docs/1.6.0/nodes-and-volumes/volumes/volume-size.md:73:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:28:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:54:        registry-secret:  <SECRET_NAME>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:90:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:116:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:120:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:124:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:128:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:151:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:158:            - name: <SECRET_NAME>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:178:        engine: <REGISTRY_URL>/longhorn-engine
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:179:        engineTag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:180:        manager: <REGISTRY_URL>/longhorn-manager
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:181:        managerTag: LONGHORN_MANAGER_IMAGE_TAG<>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:182:        ui: <REGISTRY_URL>/longhorn-ui
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:183:        uiTag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:184:        instanceManager: <REGISTRY_URL>/longhorn-instance-manager
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:185:        instanceManagerTag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:193:      attacherImage: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:194:      provisionerImage: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:195:      driverRegistrarImage: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:196:      resizerImage: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:204:      registrySecret: <SECRET_NAME>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:207:        registryUrl: <REGISTRY_URL>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:208:        registryUser: <REGISTRY_USER>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:209:        registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:225:    * Longhorn Manager Image Name     e.g. `<REGISTRY_URL>/longhorn-manager`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:227:    * Longhorn Engine Image Name    e.g. `<REGISTRY_URL>/longhorn-engine`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:229:    * Longhorn UI Image Name    e.g. `<REGISTRY_URL>/longhorn-ui`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:231:    * Longhorn Instance Manager Image Name    e.g.  `<REGISTRY_URL>/longhorn-instance-manager`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:235:    * Longhorn CSI Attacher Image    e.g.  `<REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:236:    * Longhorn CSI Provisioner Image   e.g. `<REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:237:    * Longhorn CSI Driver Registrar Image    e.g. `<REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:238:    * Longhorn CSI Driver Resizer Image    e.g. `<REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:256:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:267:    value: <SECRET_NAME>
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:294:time="2020-03-13T22:49:22Z" level=fatal msg="Error deploying driver: CSI cannot be deployed because MountPropagation is not set: Node <NODE_NAME> is not support mount propagation"
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:299:> "Dropping Longhorn node longhorn-system/**NODE_NAME** out of the queue: fail to sync node for longhorn-system/**NODE_NAME**:
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:300:> InstanceManager.longhorn.io \"instance-manager-e-605e9473\" is invalid: metadata.labels: Invalid value:
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:301:> \"**PRIVATE_REGISTRY_URL**-**PREFIX**-longhorn-instance-manager-v1_20200301\": **must be no more than 63 characters**"
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:311:"instance-manager-e-xxxxxxxx" is invalid: metadata.labels: Invalid value: "<PRIVATE_REGISTRY_URL>-longhornio-longhorn-instance-manager-v1_20200301": must be no more than 63 characters
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:321:    longhorn.io/instance-manager-image: <PRIVATE_REGISTRY_URL>-longhornio-longhorn-instance-manager-v1_20200301
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:323:    longhorn.io/node: <NODE_NAME>
content/docs/archives/0.8.1/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/0.8.1/advanced-resources/deploy/taint-toleration.md:22:Setting -> General -> Kubernetes Taint Toleration
content/docs/archives/0.8.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/0.8.1/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/0.8.1/advanced-resources/os-distro-specific/csi-on-k3s.md:19:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/0.8.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/0.8.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/0.8.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/0.8.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/0.8.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/0.8.1/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/0.8.1/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/0.8.1/deploy/accessing-the-ui/_index.md:39:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/0.8.1/deploy/accessing-the-ui/_index.md:45:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/0.8.1/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/0.8.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there, 
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:12:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:53:    kubectl edit pv <The corresponding PV of the volume found in step 1>
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:59:    curl -s https://raw.githubusercontent.com/longhorn/longhorn/v0.8.1/scripts/migrate-for-pre-070-volumes.sh |bash -s -- <volume name>
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:71:If the migration prerequisites are not satisfied and there is no error log `failed to delete then recreate PV/PVC, users need to manually check the current PVC/PV then recreate them if needed: <error log>`, the script will do nothing for the PV and PVC. Users can check the migration prerequisites and steps and retry it. 
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:79:   kubectl edit pv <The PV name>
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:85:    kubectl delete pvc <The PVC name> && kubectl delete pv <The PV name>
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:206:> These steps should not be executed if you want to maintain the ability to [roll back](#rollbacks) from a v0.7.0 installation.
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:216:    > **Important:** You must make sure all the v0.6.2 pods have been deleted, otherwise the data will be lost.
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:225:> **Prerequisite:** To rollback from v0.7.0 installation, you must not have [cleaned up the v0.6.2 CRDs.](#clean-up-the-v062-crds)
content/docs/archives/0.8.1/high-availability/recover-volume.md:8:> This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/0.8.1/high-availability/recover-volume.md:22:Mounting one more layers with the `xfs` filesystem is not allowed and will trigger the error `XFS (sdb): Filesystem has duplicate UUID <filesystem UUID> - can't mount`.
content/docs/archives/0.8.1/high-availability/recover-volume.md:24:If you use the `xfs` filesystem, you will need to manually unmount, then mount the `xfs` filesystem on the host. The device path on the host for the attached volume is `/dev/longhorn/<volume name>` . 
content/docs/archives/0.8.1/high-availability/recover-volume.md:75:- The directory used in the `livenessProbe` will be `<volumeMount.mountPath>/lost+found`
content/docs/archives/0.8.1/high-availability/recover-volume.md:89:kubectl -n <namespace of your workload> get pods <workload's pod name> -o wide
content/docs/archives/0.8.1/high-availability/recover-volume.md:100:docker restart <the container ID of the workload>
content/docs/archives/0.8.1/high-availability/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/0.8.1/high-availability/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://backupbucket@us-east-1/backupstore?backup=backup-bd326da2c4414b02&volume=volumeexamplename`. Do not delete `''`.
content/docs/archives/0.8.1/high-availability/recover-without-system.md:16:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/0.8.1/high-availability/recover-without-system.md:18:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. Now support `raw` or `qcow2` only.
content/docs/archives/0.8.1/high-availability/recover-without-system.md:20:7. Set S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`. 
content/docs/archives/0.8.1/high-availability/recover-without-system.md:26:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/0.8.1/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/0.8.1/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/0.8.1/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/0.8.1/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/0.8.1/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/0.8.1/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/0.8.1/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/0.8.1/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/0.8.1/references/settings.md:32:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/0.8.1/references/settings.md:37:> Example: `s3-secret`
content/docs/archives/0.8.1/references/settings.md:42:> Example: `300`
content/docs/archives/0.8.1/references/settings.md:49:> Example: `false`
content/docs/archives/0.8.1/references/settings.md:58:> Example: `/var/lib/longhorn`
content/docs/archives/0.8.1/references/settings.md:65:> Example: `longhornio/longhorn-engine:v0.6.0`
content/docs/archives/0.8.1/references/settings.md:72:> Example: `true`
content/docs/archives/0.8.1/references/settings.md:77:> Example: `v0.6.0`
content/docs/archives/0.8.1/references/settings.md:84:> Example: `3`
content/docs/archives/0.8.1/references/settings.md:91:> Example: `0.2`
content/docs/archives/0.8.1/references/settings.md:95:> **Warning:** The system may fail to start or become stuck while using this feature due to the resource constraint. Disabled (\"0\") by default.
content/docs/archives/0.8.1/references/settings.md:100:>Example: `longhorn-static`
content/docs/archives/0.8.1/references/settings.md:105:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/0.8.1/references/settings.md:117:> Example: `true`
content/docs/archives/0.8.1/references/settings.md:124:> Example: `500`
content/docs/archives/0.8.1/references/settings.md:131:> Example: `10`
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:40:                    "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:41:                    "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:    AWS_ACCESS_KEY_ID: <your_aws_access_key_id>
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:52:    AWS_SECRET_ACCESS_KEY: <your_aws_secret_access_key>
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:   Also make sure you've set **`<your-aws-region>` in the URL**. For example, for Google Cloud Storage, you can find the region code [here.](https://cloud.google.com/storage/docs/locations)
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:110:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/0.8.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/0.8.1/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/0.8.1/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/0.8.1/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/0.8.1/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/0.8.1/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/0.8.1/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/0.8.1/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/0.8.1/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:105:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:107:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:108:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:109:    umount /dev/longhorn/<volume name>
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:115:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:116:    xfs_growfs <the mount directory>
content/docs/archives/0.8.1/volumes-and-nodes/expansion.md:117:    umount /dev/longhorn/<volume name>
content/docs/archives/0.8.1/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.0.0/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.0.0/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.0.0/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.0.0/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.0.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.0.0/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.0.0/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://backupbucket@us-east-1/backupstore?backup=backup-bd326da2c4414b02&volume=volumeexamplename`. Do not delete `''`.
content/docs/archives/1.0.0/advanced-resources/data-recovery/recover-without-system.md:16:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.0.0/advanced-resources/data-recovery/recover-without-system.md:18:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.0.0/advanced-resources/data-recovery/recover-without-system.md:20:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`. 
content/docs/archives/1.0.0/advanced-resources/data-recovery/recover-without-system.md:30:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:28:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:54:        registry-secret:  <SECRET_NAME>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:90:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:116:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:120:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:124:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:128:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:151:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:158:            - name: <SECRET_NAME>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:178:        engine: <REGISTRY_URL>/longhorn-engine
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:179:        engineTag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:180:        manager: <REGISTRY_URL>/longhorn-manager
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:181:        managerTag: LONGHORN_MANAGER_IMAGE_TAG<>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:182:        ui: <REGISTRY_URL>/longhorn-ui
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:183:        uiTag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:184:        instanceManager: <REGISTRY_URL>/longhorn-instance-manager
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:185:        instanceManagerTag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:193:      attacherImage: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:194:      provisionerImage: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:195:      driverRegistrarImage: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:196:      resizerImage: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:204:      registrySecret: <SECRET_NAME>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:207:        registryUrl: <REGISTRY_URL>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:208:        registryUser: <REGISTRY_USER>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:209:        registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:225:    * Longhorn Manager Image Name     e.g. `<REGISTRY_URL>/longhorn-manager`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:227:    * Longhorn Engine Image Name    e.g. `<REGISTRY_URL>/longhorn-engine`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:229:    * Longhorn UI Image Name    e.g. `<REGISTRY_URL>/longhorn-ui`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:231:    * Longhorn Instance Manager Image Name    e.g.  `<REGISTRY_URL>/longhorn-instance-manager`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:235:    * Longhorn CSI Attacher Image    e.g.  `<REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:236:    * Longhorn CSI Provisioner Image   e.g. `<REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:237:    * Longhorn CSI Driver Registrar Image    e.g. `<REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:238:    * Longhorn CSI Driver Resizer Image    e.g. `<REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:256:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:267:    value: <SECRET_NAME>
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:294:time="2020-03-13T22:49:22Z" level=fatal msg="Error deploying driver: CSI cannot be deployed because MountPropagation is not set: Node <NODE_NAME> is not support mount propagation"
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:299:> "Dropping Longhorn node longhorn-system/**NODE_NAME** out of the queue: fail to sync node for longhorn-system/**NODE_NAME**:
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:300:> InstanceManager.longhorn.io \"instance-manager-e-605e9473\" is invalid: metadata.labels: Invalid value:
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:301:> \"**PRIVATE_REGISTRY_URL**-**PREFIX**-longhorn-instance-manager-v1_20200301\": **must be no more than 63 characters**"
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:311:"instance-manager-e-xxxxxxxx" is invalid: metadata.labels: Invalid value: "<PRIVATE_REGISTRY_URL>-longhornio-longhorn-instance-manager-v1_20200301": must be no more than 63 characters
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:321:    longhorn.io/instance-manager-image: <PRIVATE_REGISTRY_URL>-longhornio-longhorn-instance-manager-v1_20200301
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:323:    longhorn.io/node: <NODE_NAME>
content/docs/archives/1.0.0/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.0.0/advanced-resources/deploy/taint-toleration.md:22:Setting -> General -> Kubernetes Taint Toleration
content/docs/archives/1.0.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.0.0/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.0.0/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.0.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.0.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.0.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.0.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.0.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.0.0/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.0.0/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.0.0/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:50:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:104:> GET / HTTP/1.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:105:> Host: 97.107.142.125
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:106:> User-Agent: curl/7.64.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:107:> Accept: */*
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:108:>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:109:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:110:< Server: openresty/1.15.8.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:111:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:112:< Content-Type: text/html
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:113:< Content-Length: 185
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:114:< Connection: keep-alive
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:115:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:116:<
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:117:<html>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:118:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:119:<body>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:120:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:121:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:122:</body>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:123:</html>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:132:> GET / HTTP/1.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:133:> Host: 97.107.142.125
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:134:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:135:> User-Agent: curl/7.64.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:136:> Accept: */*
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:137:>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:138:< HTTP/1.1 200 OK
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:139:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:140:< Content-Type: text/html
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:141:< Content-Length: 1118
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:142:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:143:< ETag: "5ecf084d-3fd"
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:144:< Cache-Control: max-age=0
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:145:<
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:146:<!DOCTYPE html>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:147:<html lang="en">
content/docs/archives/1.0.0/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there, 
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:22:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:36:    > Please make sure you have at least 2 vCPUs per node before updating this setting to 0.25. See the [settings reference](../../../references/settings/#guaranteed-engine-cpu) for details.
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:40:    > Please make sure you have more nodes than the default replica count before updating this setting.
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:47:clientconn.go:1120] grpc: addrConn.createTransport failed to connect to {/var/lib/kubelet/plugins/io.rancher.longhorn-reg.sock 0  <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /var/lib/kubelet/plugins/io.rancher.longhorn-reg.sock: connect: connection refused". Reconnecting...
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:52:> **Note**: Please make sure there is no PV running with driver `io.rancher.longhorn`.
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:66:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.0.0/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.0/high-availability/recover-volume.md:22:Mounting one more layers with the `xfs` filesystem is not allowed and will trigger the error `XFS (sdb): Filesystem has duplicate UUID <filesystem UUID> - can't mount`.
content/docs/archives/1.0.0/high-availability/recover-volume.md:24:If you use the `xfs` filesystem, you will need to manually unmount, then mount the `xfs` filesystem on the host. The device path on the host for the attached volume is `/dev/longhorn/<volume name>` . 
content/docs/archives/1.0.0/high-availability/recover-volume.md:75:- The directory used in the `livenessProbe` will be `<volumeMount.mountPath>/lost+found`
content/docs/archives/1.0.0/high-availability/recover-volume.md:89:kubectl -n <namespace of your workload> get pods <workload's pod name> -o wide
content/docs/archives/1.0.0/high-availability/recover-volume.md:100:docker restart <the container ID of the workload>
content/docs/archives/1.0.0/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.0.0/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.0.0/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.0.0/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.0.0/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.0.0/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.0/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.0/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.0/references/settings.md:46:> Default: `false`
content/docs/archives/1.0.0/references/settings.md:55:> Default: `/var/lib/longhorn/`
content/docs/archives/1.0.0/references/settings.md:62:> Default: `longhornio/longhorn-engine:varchives` for Longhorn varchives
content/docs/archives/1.0.0/references/settings.md:69:> Default: `longhornio/longhorn-instance-manager:v1_20200514` for Longhorn varchives
content/docs/archives/1.0.0/references/settings.md:74:> Default: `true`
content/docs/archives/1.0.0/references/settings.md:79:> Default: `varchives` for Longhorn varchives
content/docs/archives/1.0.0/references/settings.md:86:> Default: `3`
content/docs/archives/1.0.0/references/settings.md:93:> Default: `longhorn-static`
content/docs/archives/1.0.0/references/settings.md:98:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.0.0/references/settings.md:103:> Default: `true`
content/docs/archives/1.0.0/references/settings.md:112:> Default: `wait`
content/docs/archives/1.0.0/references/settings.md:127:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.0.0/references/settings.md:132:> Example: `s3-secret`
content/docs/archives/1.0.0/references/settings.md:137:> Default: `300`
content/docs/archives/1.0.0/references/settings.md:147:> Default: `false`
content/docs/archives/1.0.0/references/settings.md:154:> Default: `200`
content/docs/archives/1.0.0/references/settings.md:163:> Default: `25`
content/docs/archives/1.0.0/references/settings.md:170:> Default: `true`
content/docs/archives/1.0.0/references/settings.md:177:> Default: `true`
content/docs/archives/1.0.0/references/settings.md:183:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.0.0/references/settings.md:188:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.0.0/references/settings.md:200:> Default: `0.25`
content/docs/archives/1.0.0/references/settings.md:206:> **Warning:** This setting should be changed only when all the volumes on the nodes are detached. Changing the setting will result in all the Instance Manager Pods restarting, which will automatically detach all the attached volumes, and could cause a workload outage.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:41:                    "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:42:                    "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:52:    AWS_ACCESS_KEY_ID: <your_aws_access_key_id>
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:53:    AWS_SECRET_ACCESS_KEY: <your_aws_secret_access_key>
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:59:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:65:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:68:   Also make sure you've set **`<your-aws-region>` in the URL**. For example, for Google Cloud Storage, you can find the region code [here.](https://cloud.google.com/storage/docs/locations)
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:115:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.0.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.0.0/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.0.0/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.0.0/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.0.0/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.0.0/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.0.0/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.0.0/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.0.0/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:105:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:107:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:108:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:109:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:115:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:116:    xfs_growfs <the mount directory>
content/docs/archives/1.0.0/volumes-and-nodes/expansion.md:117:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:27:    > **Upcoming feature:** After adding the support of `Replica eviction`, you will be able to evict the replicas on the node gracefully.
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:35:    > **Upcoming features:**
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:36:    >
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:37:    > - After adding the support of the **Reuse existing replica data for rebuild** feature, the replica rebuild will be faster and take less space.
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:38:    > - After adding the support of the **Disable replica rebuild** feature, there will not be an unnecessary replica rebuild caused by the node maintenance.
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:57:    > **Upcoming feature:** The replica eviction feature can also help here.
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:71:    > **Upcoming feature:** The replica eviction feature can also help here.
content/docs/archives/1.0.0/volumes-and-nodes/maintenance.md:79:        kubectl delete node <node-name>
content/docs/archives/1.0.0/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.0.1/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.0.1/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.0.1/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.0.1/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.0.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.0.1/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.0.1/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://backupbucket@us-east-1/backupstore?backup=backup-bd326da2c4414b02&volume=volumeexamplename`. Do not delete `''`.
content/docs/archives/1.0.1/advanced-resources/data-recovery/recover-without-system.md:16:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.0.1/advanced-resources/data-recovery/recover-without-system.md:18:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.0.1/advanced-resources/data-recovery/recover-without-system.md:20:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`. 
content/docs/archives/1.0.1/advanced-resources/data-recovery/recover-without-system.md:30:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:21:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.0.1/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:25:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:47:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:73:        registry-secret:  <SECRET_NAME>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:109:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:123:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:125:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:127:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:129:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:135:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:139:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:143:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:147:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:170:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:177:            - name: <SECRET_NAME>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:197:        engine: <REGISTRY_URL>/longhorn-engine
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:198:        engineTag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:199:        manager: <REGISTRY_URL>/longhorn-manager
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:200:        managerTag: LONGHORN_MANAGER_IMAGE_TAG<>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:201:        ui: <REGISTRY_URL>/longhorn-ui
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:202:        uiTag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:203:        instanceManager: <REGISTRY_URL>/longhorn-instance-manager
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:204:        instanceManagerTag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:212:      attacherImage: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:213:      provisionerImage: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:214:      driverRegistrarImage: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:215:      resizerImage: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:223:      registrySecret: <SECRET_NAME>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:226:        registryUrl: <REGISTRY_URL>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:227:        registryUser: <REGISTRY_USER>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:228:        registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:244:    * Longhorn Manager Image Name     e.g. `<REGISTRY_URL>/longhorn-manager`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:246:    * Longhorn Engine Image Name    e.g. `<REGISTRY_URL>/longhorn-engine`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:248:    * Longhorn UI Image Name    e.g. `<REGISTRY_URL>/longhorn-ui`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:250:    * Longhorn Instance Manager Image Name    e.g.  `<REGISTRY_URL>/longhorn-instance-manager`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:254:    * Longhorn CSI Attacher Image    e.g.  `<REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:255:    * Longhorn CSI Provisioner Image   e.g. `<REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:256:    * Longhorn CSI Driver Registrar Image    e.g. `<REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:257:    * Longhorn CSI Driver Resizer Image    e.g. `<REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:275:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.0.1/advanced-resources/deploy/airgap.md:286:    value: <SECRET_NAME>
content/docs/archives/1.0.1/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.0.1/advanced-resources/deploy/priority-class.md:15:> **Warning:** Longhorn will not start if Priority Class setting is invalid (such as the Priority Class not existing). You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`. You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.0.1/advanced-resources/deploy/priority-class.md:21:Setting -> General -> Priority Class
content/docs/archives/1.0.1/advanced-resources/deploy/taint-toleration.md:22:Setting -> General -> Kubernetes Taint Toleration
content/docs/archives/1.0.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.0.1/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.0.1/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.0.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.0.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.0.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.0.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.0.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.0.1/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.0.1/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.0.1/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:50:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:104:> GET / HTTP/1.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:105:> Host: 97.107.142.125
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:106:> User-Agent: curl/7.64.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:107:> Accept: */*
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:108:>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:109:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:110:< Server: openresty/1.15.8.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:111:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:112:< Content-Type: text/html
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:113:< Content-Length: 185
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:114:< Connection: keep-alive
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:115:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:116:<
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:117:<html>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:118:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:119:<body>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:120:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:121:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:122:</body>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:123:</html>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:132:> GET / HTTP/1.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:133:> Host: 97.107.142.125
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:134:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:135:> User-Agent: curl/7.64.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:136:> Accept: */*
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:137:>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:138:< HTTP/1.1 200 OK
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:139:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:140:< Content-Type: text/html
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:141:< Content-Length: 1118
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:142:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:143:< ETag: "5ecf084d-3fd"
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:144:< Cache-Control: max-age=0
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:145:<
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:146:<!DOCTYPE html>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:147:<html lang="en">
content/docs/archives/1.0.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there, 
content/docs/archives/1.0.1/deploy/upgrade/longhorn-manager.md:19:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.0.1/deploy/upgrade/longhorn-manager.md:26:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.0.1/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.1/high-availability/recover-volume.md:22:Mounting one more layers with the `xfs` filesystem is not allowed and will trigger the error `XFS (sdb): Filesystem has duplicate UUID <filesystem UUID> - can't mount`.
content/docs/archives/1.0.1/high-availability/recover-volume.md:24:If you use the `xfs` filesystem, you will need to manually unmount, then mount the `xfs` filesystem on the host. The device path on the host for the attached volume is `/dev/longhorn/<volume name>` . 
content/docs/archives/1.0.1/high-availability/recover-volume.md:75:- The directory used in the `livenessProbe` will be `<volumeMount.mountPath>/lost+found`
content/docs/archives/1.0.1/high-availability/recover-volume.md:89:kubectl -n <namespace of your workload> get pods <workload's pod name> -o wide
content/docs/archives/1.0.1/high-availability/recover-volume.md:100:docker restart <the container ID of the workload>
content/docs/archives/1.0.1/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.0.1/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.0.1/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.0.1/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.0.1/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.0.1/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.1/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.1/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.1/references/settings.md:47:> Default: `false`
content/docs/archives/1.0.1/references/settings.md:56:> Default: `/var/lib/longhorn/`
content/docs/archives/1.0.1/references/settings.md:63:> Default: `longhornio/longhorn-engine:varchives` for Longhorn varchives
content/docs/archives/1.0.1/references/settings.md:70:> Default: `longhornio/longhorn-instance-manager:v1_20200514` for Longhorn varchives
content/docs/archives/1.0.1/references/settings.md:75:> Default: `true`
content/docs/archives/1.0.1/references/settings.md:80:> Default: `varchives` for Longhorn varchives
content/docs/archives/1.0.1/references/settings.md:87:> Default: `3`
content/docs/archives/1.0.1/references/settings.md:94:> Default: `longhorn-static`
content/docs/archives/1.0.1/references/settings.md:99:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.0.1/references/settings.md:104:> Default: `true`
content/docs/archives/1.0.1/references/settings.md:113:> Default: `wait`
content/docs/archives/1.0.1/references/settings.md:128:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.0.1/references/settings.md:133:> Example: `s3-secret`
content/docs/archives/1.0.1/references/settings.md:138:> Default: `300`
content/docs/archives/1.0.1/references/settings.md:148:> Default: `false`
content/docs/archives/1.0.1/references/settings.md:155:> Default: `200`
content/docs/archives/1.0.1/references/settings.md:164:> Default: `25`
content/docs/archives/1.0.1/references/settings.md:171:> Default: `true`
content/docs/archives/1.0.1/references/settings.md:178:> Default: `true`
content/docs/archives/1.0.1/references/settings.md:184:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.0.1/references/settings.md:189:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.0.1/references/settings.md:201:> Default: `0.25`
content/docs/archives/1.0.1/references/settings.md:207:> **Warning:** This setting should be changed only when all the volumes on the nodes are detached. Changing the setting will result in all the Instance Manager Pods restarting, which will automatically detach all the attached volumes, and could cause a workload outage.
content/docs/archives/1.0.1/references/settings.md:222:> Example: `high-priority`
content/docs/archives/1.0.1/references/settings.md:228:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:40:                    "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:41:                    "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:    AWS_ACCESS_KEY_ID: <your_aws_access_key_id>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:52:    AWS_SECRET_ACCESS_KEY: <your_aws_secret_access_key>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:64:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:67:   Also make sure you've set **`<your-aws-region>` in the URL**. For example, for Google Cloud Storage, you can find the region code [here.](https://cloud.google.com/storage/docs/locations)
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:80:    AWS_ACCESS_KEY_ID: <your_aws_access_key_id>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:81:    AWS_SECRET_ACCESS_KEY: <your_aws_secret_access_key>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:    HTTP_PROXY: <your_proxy_ip_and_port>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:    HTTPS_PROXY: <your_proxy_ip_and_port>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:84:    NO_PROXY: <excluded-ip-list>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.0.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.0.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.0.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.0.1/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.0.1/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.0.1/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.0.1/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.0.1/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.0.1/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.0.1/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.0.1/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:105:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:107:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:108:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:109:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:115:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:116:    xfs_growfs <the mount directory>
content/docs/archives/1.0.1/volumes-and-nodes/expansion.md:117:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:27:    > **Upcoming feature:** After adding the support of `Replica eviction`, you will be able to evict the replicas on the node gracefully.
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:35:    > **Upcoming features:**
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:36:    >
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:37:    > - After adding the support of the **Reuse existing replica data for rebuild** feature, the replica rebuild will be faster and take less space.
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:38:    > - After adding the support of the **Disable replica rebuild** feature, there will not be an unnecessary replica rebuild caused by the node maintenance.
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:57:    > **Upcoming feature:** The replica eviction feature can also help here.
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:71:    > **Upcoming feature:** The replica eviction feature can also help here.
content/docs/archives/1.0.1/volumes-and-nodes/maintenance.md:79:        kubectl delete node <node-name>
content/docs/archives/1.0.1/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.0.2/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.0.2/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.0.2/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.0.2/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.0.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.0.2/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.0.2/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://backupbucket@us-east-1/backupstore?backup=backup-bd326da2c4414b02&volume=volumeexamplename`. Do not delete `''`.
content/docs/archives/1.0.2/advanced-resources/data-recovery/recover-without-system.md:16:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.0.2/advanced-resources/data-recovery/recover-without-system.md:18:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.0.2/advanced-resources/data-recovery/recover-without-system.md:20:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`. 
content/docs/archives/1.0.2/advanced-resources/data-recovery/recover-without-system.md:30:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.0.2/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:80:          registry-secret:  <SECRET_NAME>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:116:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:130:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:132:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:134:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:136:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:142:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:146:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:150:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:154:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:177:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:184:            - name: <SECRET_NAME>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:193:In v1.0.2, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:195:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:211:        registrySecret: <SECRET_NAME>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:214:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:215:          registryUser: <REGISTRY_USER>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:216:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:229:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:237:            engineTag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:239:            managerTag: LONGHORN_MANAGER_IMAGE_TAG<>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:241:            uiTag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:243:            instanceManagerTag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:251:          attacherImageTag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:253:          provisionerImageTag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:255:          nodeDriverRegistrarImageTag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:257:          resizerImageTag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:264:          registrySecret: <SECRET_NAME>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:267:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:268:            registryUser: <REGISTRY_USER>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:269:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:295:> The `Private registry secret` must be specified in the default settings so that Longhorn can create a secret under the name. The process for setting up the private registry will be fixed to be less confusing, but not in the v1.0.2 release. More detail is at [this comment.](https://github.com/longhorn/longhorn/issues/1670#issuecomment-670723484)
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:301:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:309:> The `Private registry secret` must be specified in the default settings so that Longhorn can create a secret under the name. The process for setting up the private registry will be fixed to be less confusing, but not in the v1.0.2 release. More detail is at [this comment.](https://github.com/longhorn/longhorn/issues/1670#issuecomment-670723484)
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:318:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:329:    value: <SECRET_NAME>
content/docs/archives/1.0.2/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.0.2/advanced-resources/deploy/priority-class.md:15:> **Warning:** Longhorn will not start if Priority Class setting is invalid (such as the Priority Class not existing). You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`. You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.0.2/advanced-resources/deploy/priority-class.md:21:Setting -> General -> Priority Class
content/docs/archives/1.0.2/advanced-resources/deploy/taint-toleration.md:22:Setting -> General -> Kubernetes Taint Toleration
content/docs/archives/1.0.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.0.2/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.0.2/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.0.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.0.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.0.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.0.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.0.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.0.2/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.0.2/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.0.2/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:50:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:104:> GET / HTTP/1.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:105:> Host: 97.107.142.125
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:106:> User-Agent: curl/7.64.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:107:> Accept: */*
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:108:>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:109:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:110:< Server: openresty/1.15.8.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:111:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:112:< Content-Type: text/html
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:113:< Content-Length: 185
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:114:< Connection: keep-alive
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:115:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:116:<
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:117:<html>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:118:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:119:<body>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:120:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:121:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:122:</body>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:123:</html>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:132:> GET / HTTP/1.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:133:> Host: 97.107.142.125
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:134:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:135:> User-Agent: curl/7.64.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:136:> Accept: */*
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:137:>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:138:< HTTP/1.1 200 OK
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:139:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:140:< Content-Type: text/html
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:141:< Content-Length: 1118
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:142:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:143:< ETag: "5ecf084d-3fd"
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:144:< Cache-Control: max-age=0
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:145:<
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:146:<!DOCTYPE html>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:147:<html lang="en">
content/docs/archives/1.0.2/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.0.2/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.0.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.0.2/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.2/high-availability/recover-volume.md:22:Mounting one more layers with the `xfs` filesystem is not allowed and will trigger the error `XFS (sdb): Filesystem has duplicate UUID <filesystem UUID> - can't mount`.
content/docs/archives/1.0.2/high-availability/recover-volume.md:24:If you use the `xfs` filesystem, you will need to manually unmount, then mount the `xfs` filesystem on the host. The device path on the host for the attached volume is `/dev/longhorn/<volume name>` . 
content/docs/archives/1.0.2/high-availability/recover-volume.md:75:- The directory used in the `livenessProbe` will be `<volumeMount.mountPath>/lost+found`
content/docs/archives/1.0.2/high-availability/recover-volume.md:89:kubectl -n <namespace of your workload> get pods <workload's pod name> -o wide
content/docs/archives/1.0.2/high-availability/recover-volume.md:100:docker restart <the container ID of the workload>
content/docs/archives/1.0.2/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.0.2/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.0.2/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.0.2/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.0.2/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.0.2/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.2/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.2/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.0.2/references/settings.md:47:> Default: `false`
content/docs/archives/1.0.2/references/settings.md:56:> Default: `/var/lib/longhorn/`
content/docs/archives/1.0.2/references/settings.md:63:> Default: `longhornio/longhorn-engine:varchives` for Longhorn varchives
content/docs/archives/1.0.2/references/settings.md:70:> Default: `longhornio/longhorn-instance-manager:v1_20200514` for Longhorn varchives
content/docs/archives/1.0.2/references/settings.md:75:> Default: `true`
content/docs/archives/1.0.2/references/settings.md:80:> Default: `varchives` for Longhorn varchives
content/docs/archives/1.0.2/references/settings.md:87:> Default: `3`
content/docs/archives/1.0.2/references/settings.md:94:> Default: `longhorn-static`
content/docs/archives/1.0.2/references/settings.md:99:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.0.2/references/settings.md:104:> Default: `true`
content/docs/archives/1.0.2/references/settings.md:113:> Default: `wait`
content/docs/archives/1.0.2/references/settings.md:128:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.0.2/references/settings.md:133:> Example: `s3-secret`
content/docs/archives/1.0.2/references/settings.md:138:> Default: `300`
content/docs/archives/1.0.2/references/settings.md:148:> Default: `false`
content/docs/archives/1.0.2/references/settings.md:155:> Default: `200`
content/docs/archives/1.0.2/references/settings.md:164:> Default: `25`
content/docs/archives/1.0.2/references/settings.md:171:> Default: `true`
content/docs/archives/1.0.2/references/settings.md:178:> Default: `true`
content/docs/archives/1.0.2/references/settings.md:184:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.0.2/references/settings.md:189:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.0.2/references/settings.md:201:> Default: `0.25`
content/docs/archives/1.0.2/references/settings.md:207:> **Warning:** This setting should be changed only when all the volumes on the nodes are detached. Changing the setting will result in all the Instance Manager Pods restarting, which will automatically detach all the attached volumes, and could cause a workload outage.
content/docs/archives/1.0.2/references/settings.md:222:> Example: `high-priority`
content/docs/archives/1.0.2/references/settings.md:228:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:40:                    "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:41:                    "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:    AWS_ACCESS_KEY_ID: <your_aws_access_key_id>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:52:    AWS_SECRET_ACCESS_KEY: <your_aws_secret_access_key>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:64:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:67:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:84:    AWS_ACCESS_KEY_ID: <your_aws_access_key_id>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:85:    AWS_SECRET_ACCESS_KEY: <your_aws_secret_access_key>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:86:    HTTP_PROXY: <your_proxy_ip_and_port>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:    HTTPS_PROXY: <your_proxy_ip_and_port>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:    NO_PROXY: <excluded-ip-list>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.0.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.0.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.0.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.0.2/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.0.2/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.0.2/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.0.2/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.0.2/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.0.2/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.0.2/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.0.2/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:105:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:107:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:108:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:109:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:115:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:116:    xfs_growfs <the mount directory>
content/docs/archives/1.0.2/volumes-and-nodes/expansion.md:117:    umount /dev/longhorn/<volume name>
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:27:    > **Upcoming feature:** After adding the support of `Replica eviction`, you will be able to evict the replicas on the node gracefully.
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:35:    > **Upcoming features:**
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:36:    >
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:37:    > - After adding the support of the **Reuse existing replica data for rebuild** feature, the replica rebuild will be faster and take less space.
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:38:    > - After adding the support of the **Disable replica rebuild** feature, there will not be an unnecessary replica rebuild caused by the node maintenance.
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:57:    > **Upcoming feature:** The replica eviction feature can also help here.
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:71:    > **Upcoming feature:** The replica eviction feature can also help here.
content/docs/archives/1.0.2/volumes-and-nodes/maintenance.md:79:        kubectl delete node <node-name>
content/docs/archives/1.0.2/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.1.0/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.1.0/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.1.0/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.1.0/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.1.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.1.0/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.1.0/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.1.0/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.1.0/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.1.0/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.1.0/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.1.0/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:298:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:314:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:325:    value: <SECRET_NAME>
content/docs/archives/1.1.0/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.1.0/advanced-resources/deploy/priority-class.md:15:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing). You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`. You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.1.0/advanced-resources/deploy/priority-class.md:19:The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.1.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.1.0/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.1.0/advanced-resources/deploy/taint-toleration.md:20:The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.1.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.1.0/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.1.0/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.1.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.1.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.1.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.1.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.1.0/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.1.0/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.1.0/best-practices.md:103:> Recommend: `false`
content/docs/archives/1.1.0/best-practices.md:108:> Recommend: `false`
content/docs/archives/1.1.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.1.0/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.1.0/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.1.0/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:50:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:104:> GET / HTTP/1.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:105:> Host: 97.107.142.125
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:106:> User-Agent: curl/7.64.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:107:> Accept: */*
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:108:>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:109:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:110:< Server: openresty/1.15.8.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:111:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:112:< Content-Type: text/html
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:113:< Content-Length: 185
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:114:< Connection: keep-alive
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:115:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:116:<
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:117:<html>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:118:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:119:<body>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:120:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:121:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:122:</body>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:123:</html>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:132:> GET / HTTP/1.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:133:> Host: 97.107.142.125
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:134:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:135:> User-Agent: curl/7.64.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:136:> Accept: */*
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:137:>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:138:< HTTP/1.1 200 OK
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:139:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:140:< Content-Type: text/html
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:141:< Content-Length: 1118
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:142:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:143:< ETag: "5ecf084d-3fd"
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:144:< Cache-Control: max-age=0
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:145:<
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:146:<!DOCTYPE html>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:147:<html lang="en">
content/docs/archives/1.1.0/deploy/install/_index.md:109:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.1.0/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.1.0/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.1.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:283:        from: <the sender address>
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:286:        auth_username: <the username>
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:288:        auth_password: <the password>
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.1.0/monitoring/prometheus_and_grafana_setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.1.0/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.1.0/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.1.0/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.1.0/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.1.0/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.1.0/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.1.0/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.1.0/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.1.0/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.1.0/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.1.0/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.1.0/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.0/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.0/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.0/references/settings.md:59:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:68:> Default: `/var/lib/longhorn/`
content/docs/archives/1.1.0/references/settings.md:75:> Default: `longhornio/longhorn-engine:varchives` for Longhorn varchives
content/docs/archives/1.1.0/references/settings.md:82:> Default: `longhornio/longhorn-instance-manager:v1_20200514` for Longhorn varchives
content/docs/archives/1.1.0/references/settings.md:87:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:92:> Default: `varchives` for Longhorn varchives
content/docs/archives/1.1.0/references/settings.md:99:> Default: `3`
content/docs/archives/1.1.0/references/settings.md:106:> Default: `disabled`
content/docs/archives/1.1.0/references/settings.md:120:> Default: `longhorn-static`
content/docs/archives/1.1.0/references/settings.md:125:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.1.0/references/settings.md:130:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:135:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:149:> Default: `wait`
content/docs/archives/1.1.0/references/settings.md:158:> Default: `do-nothing`
content/docs/archives/1.1.0/references/settings.md:172:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:177:> Default: `if-not-present`
content/docs/archives/1.1.0/references/settings.md:188:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:195:> Default: `600`
content/docs/archives/1.1.0/references/settings.md:202:> Default: `if-not-present`
content/docs/archives/1.1.0/references/settings.md:219:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.1.0/references/settings.md:224:> Example: `s3-secret`
content/docs/archives/1.1.0/references/settings.md:229:> Default: `300`
content/docs/archives/1.1.0/references/settings.md:236:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:245:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:252:> Default: `200`
content/docs/archives/1.1.0/references/settings.md:261:> Default: `25`
content/docs/archives/1.1.0/references/settings.md:268:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:275:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:281:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.1.0/references/settings.md:284:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:288:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.1.0/references/settings.md:293:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.1.0/references/settings.md:305:> Default: `0.25`
content/docs/archives/1.1.0/references/settings.md:319:> **Warning:** This setting should be changed only when all the volumes on the nodes are detached. Changing the setting will result in all the Instance Manager Pods restarting, which will automatically detach all the attached volumes, and could cause a workload outage.
content/docs/archives/1.1.0/references/settings.md:334:> Example: `high-priority`
content/docs/archives/1.1.0/references/settings.md:340:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.1.0/references/settings.md:345:> Default: `false`
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:40:                    "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:41:                    "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:    kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:52:        --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:53:        --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:60:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:66:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:69:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:86:kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:89:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:90:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:91:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:138:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> CSI snapshot support is available for Kubernetes versions >= **1.17**.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:12:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:13:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:36:> **Prerequisite**
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:37:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:41:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:42:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:22:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't new data for a long time, Longhorn does the following:
content/docs/archives/1.1.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:23:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.1.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:24:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.1.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.1.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.1.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.0/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.1.0/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.1.0/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.1.0/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.1.0/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.1.0/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.1.0/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.1.0/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.1.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.1.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/archives/1.1.0/volumes-and-nodes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.0/volumes-and-nodes/maintenance.md:78:        kubectl delete node <node-name>
content/docs/archives/1.1.0/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.1.0/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.1.1/advanced-resources/backing-image.md:16:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.1.1/advanced-resources/backing-image.md:67:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.1.1/advanced-resources/backing-image.md:70:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.1.1/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.1.1/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.1.1/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.1.1/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.1.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.1.1/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.1.1/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.1.1/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.1.1/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.1.1/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.1.1/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.1.1/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:298:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:314:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:325:    value: <SECRET_NAME>
content/docs/archives/1.1.1/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.1.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.1.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.1.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.1.1/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.1.1/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.1.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.1.1/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.1.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.1.1/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.1.1/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.1.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.1.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.1.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.1.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.1.1/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.1.1/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.1.1/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.1.1/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.1.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.1.1/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.1.1/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.1.1/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:53:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:110:> GET / HTTP/1.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:111:> Host: 97.107.142.125
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:112:> User-Agent: curl/7.64.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:113:> Accept: */*
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:114:>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:115:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:116:< Server: openresty/1.15.8.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:117:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:118:< Content-Type: text/html
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:119:< Content-Length: 185
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:120:< Connection: keep-alive
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:121:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:122:<
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:123:<html>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:124:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:125:<body>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:126:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:127:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:128:</body>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:129:</html>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:138:> GET / HTTP/1.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:139:> Host: 97.107.142.125
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:140:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:141:> User-Agent: curl/7.64.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> Accept: */*
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:143:>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:144:< HTTP/1.1 200 OK
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:145:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:146:< Content-Type: text/html
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:147:< Content-Length: 1118
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:148:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:149:< ETag: "5ecf084d-3fd"
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:150:< Cache-Control: max-age=0
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:151:<
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:152:<!DOCTYPE html>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:153:<html lang="en">
content/docs/archives/1.1.1/deploy/install/_index.md:109:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.1.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.1.1/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.1.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.1.1/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.1.1/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.1.1/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.1.1/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.1.1/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.1.1/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.1.1/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.1.1/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.1.1/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.1.1/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.1.1/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.1.1/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.1.1/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.1/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.1/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.1/references/settings.md:62:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:70:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:76:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:87:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:93:> Default: `0`
content/docs/archives/1.1.1/references/settings.md:101:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:115:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.1.1/references/settings.md:121:> Default: `disabled`
content/docs/archives/1.1.1/references/settings.md:136:> Default: `/var/lib/longhorn/`
content/docs/archives/1.1.1/references/settings.md:154:> Default: `longhorn-static`
content/docs/archives/1.1.1/references/settings.md:160:> Default: `3`
content/docs/archives/1.1.1/references/settings.md:172:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:178:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:190:> Default: `do-nothing`
content/docs/archives/1.1.1/references/settings.md:205:> Default: `600`
content/docs/archives/1.1.1/references/settings.md:213:> Default: `if-not-present`
content/docs/archives/1.1.1/references/settings.md:229:> Default: `wait`
content/docs/archives/1.1.1/references/settings.md:238:> Default: `60`
content/docs/archives/1.1.1/references/settings.md:246:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:254:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.1.1/references/settings.md:260:> Example: `s3-secret`
content/docs/archives/1.1.1/references/settings.md:266:> Default: `300`
content/docs/archives/1.1.1/references/settings.md:276:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:280:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.1.1/references/settings.md:284:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:292:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:300:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:306:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.1.1/references/settings.md:310:> Default: `25`
content/docs/archives/1.1.1/references/settings.md:318:> Default: `200`
content/docs/archives/1.1.1/references/settings.md:330:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:336:> Default: `12`
content/docs/archives/1.1.1/references/settings.md:348:> **Warning:**
content/docs/archives/1.1.1/references/settings.md:349:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.1.1/references/settings.md:350:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.1.1/references/settings.md:351:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.1/references/settings.md:352:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.1.1/references/settings.md:353:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.1/references/settings.md:357:> Default: `12`
content/docs/archives/1.1.1/references/settings.md:369:> **Warning:**
content/docs/archives/1.1.1/references/settings.md:370:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.1.1/references/settings.md:371:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.1.1/references/settings.md:372:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.1/references/settings.md:373:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.1.1/references/settings.md:374:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.1/references/settings.md:379:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.1.1/references/settings.md:398:> Example: `high-priority`
content/docs/archives/1.1.1/references/settings.md:407:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.1.1/references/settings.md:413:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.1.1/references/settings.md:420:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.1/references/settings.md:428:> Default:
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:            --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:            --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:                "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:            --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:      4. The Longhorn will automatically annotates `iam.amazonaws.com/role=<your-aws-iam-role-arn>` to the longhorn manager as well as replica instance manager Pods. Please make sure the kube2iam or kiam Pods running on the same nodes as longhorn manager and replica instance manager Pods.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:127:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:133:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:205:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> CSI snapshot support is available for Kubernetes versions >= **1.17**.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:12:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:13:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:36:> **Prerequisite**
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:37:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:41:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:42:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:22:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't new data for a long time, Longhorn does the following:
content/docs/archives/1.1.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:23:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.1.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:24:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.1.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.1.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.1.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.1/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.1.1/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.1.1/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.1.1/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.1.1/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.1.1/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.1.1/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.1.1/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.1.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.1.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/archives/1.1.1/volumes-and-nodes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.1/volumes-and-nodes/maintenance.md:80:        kubectl delete node <node-name>
content/docs/archives/1.1.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.1.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.1.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.1.1/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.1.1/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.1.2/advanced-resources/backing-image.md:16:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.1.2/advanced-resources/backing-image.md:67:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.1.2/advanced-resources/backing-image.md:70:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.1.2/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.1.2/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.1.2/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.1.2/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.1.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.1.2/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.1.2/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.1.2/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.1.2/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.1.2/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.1.2/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.1.2/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:298:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:314:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:325:    value: <SECRET_NAME>
content/docs/archives/1.1.2/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.1.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.1.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.1.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.1.2/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.1.2/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.1.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.1.2/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:48:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:49:   >
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:50:   > For example:
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:51:   > ```yaml
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:52:   > defaultSettings:
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:53:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:54:   >  ```
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:58:> **Warning**:
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:59:>
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:60:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:61:>
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:62:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:63:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:64:>
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:65:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:100:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.1.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.1.2/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.1.2/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.1.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.1.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.1.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.1.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.1.2/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.1.2/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.1.2/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.1.2/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.1.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.1.2/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.1.2/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.1.2/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:53:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:110:> GET / HTTP/1.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:111:> Host: 97.107.142.125
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:112:> User-Agent: curl/7.64.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:113:> Accept: */*
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:114:>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:115:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:116:< Server: openresty/1.15.8.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:117:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:118:< Content-Type: text/html
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:119:< Content-Length: 185
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:120:< Connection: keep-alive
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:121:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:122:<
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:123:<html>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:124:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:125:<body>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:126:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:127:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:128:</body>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:129:</html>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:138:> GET / HTTP/1.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:139:> Host: 97.107.142.125
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:140:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:141:> User-Agent: curl/7.64.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> Accept: */*
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:143:>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:144:< HTTP/1.1 200 OK
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:145:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:146:< Content-Type: text/html
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:147:< Content-Length: 1118
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:148:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:149:< ETag: "5ecf084d-3fd"
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:150:< Cache-Control: max-age=0
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:151:<
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:152:<!DOCTYPE html>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:153:<html lang="en">
content/docs/archives/1.1.2/deploy/install/_index.md:109:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.1.2/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.1.2/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.1.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:14:> **Note:**
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:15:> There is a bug in the instance manager image `v1_20201216` shipped in Longhorn v1.1.0 and v1.1.1
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:16:> which can lead to a deadlock in a big cluster with hundreds of volumes.
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:17:> See more details [here](https://github.com/longhorn/longhorn/issues/2697).
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:18:> Longhorn v1.1.2 is shipped with a new instance manager image `v1_20210621` which fixed the deadlock
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:19:> but engine/replica processes of volumes don't migrate from old instance manager to new instance manager
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:20:> until the next time the volumes are detached/attached because Longhorn doesn't want to interrupt the
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:21:> data plane of the volumes.
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:22:>
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:23:> To reduce the chance of hitting the deadlock while the engine/replica processes are still in the old instance manager,
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:24:> you should upgrade engine for volumes in small batches, e.g., upgrading 2 or 3 volumes at a time.
content/docs/archives/1.1.2/deploy/upgrade/_index.md:23:> **Note:**
content/docs/archives/1.1.2/deploy/upgrade/_index.md:24:> There is a bug in the instance manager image `v1_20201216` shipped in Longhorn v1.1.0 and v1.1.1
content/docs/archives/1.1.2/deploy/upgrade/_index.md:25:> which can lead to a deadlock in a big cluster with hundreds of volumes.
content/docs/archives/1.1.2/deploy/upgrade/_index.md:26:> See more details [here](https://github.com/longhorn/longhorn/issues/2697).
content/docs/archives/1.1.2/deploy/upgrade/_index.md:27:> Longhorn v1.1.2 is shipped with a new instance manager image `v1_20210621` which fixed the deadlock
content/docs/archives/1.1.2/deploy/upgrade/_index.md:28:> but engine/replica processes of volumes don't migrate from the old instance manager to the new instance manager
content/docs/archives/1.1.2/deploy/upgrade/_index.md:29:> until the next time the volumes are detached/attached. Longhorn does it because we don't want to interrupt the
content/docs/archives/1.1.2/deploy/upgrade/_index.md:30:> data plane of the volumes.
content/docs/archives/1.1.2/deploy/upgrade/_index.md:31:>
content/docs/archives/1.1.2/deploy/upgrade/_index.md:32:> If you hit the deadlock in the old instance manager, please follow the recovering steps [here](https://github.com/longhorn/longhorn/issues/2697#issuecomment-879374809)
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.1.2/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.1.2/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.1.2/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.1.2/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.1.2/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.1.2/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.1.2/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.1.2/references/examples.md:250:      nodeName: <NODE_NAME>
content/docs/archives/1.1.2/references/examples.md:258:          '<BACKUP_URL>'
content/docs/archives/1.1.2/references/examples.md:259:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.1.2/references/examples.md:260:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.1.2/references/examples.md:268:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.1.2/references/examples.md:274:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.2/references/examples.md:279:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.2/references/examples.md:284:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.2/references/settings.md:61:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:69:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:75:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:86:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:92:> Default: `0`
content/docs/archives/1.1.2/references/settings.md:100:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:114:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.1.2/references/settings.md:120:> Default: `disabled`
content/docs/archives/1.1.2/references/settings.md:135:> Default: `/var/lib/longhorn/`
content/docs/archives/1.1.2/references/settings.md:153:> Default: `longhorn-static`
content/docs/archives/1.1.2/references/settings.md:159:> Default: `3`
content/docs/archives/1.1.2/references/settings.md:171:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:177:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:189:> Default: `do-nothing`
content/docs/archives/1.1.2/references/settings.md:204:> Default: `600`
content/docs/archives/1.1.2/references/settings.md:212:> Default: `if-not-present`
content/docs/archives/1.1.2/references/settings.md:227:> Default: `60`
content/docs/archives/1.1.2/references/settings.md:235:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:243:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.1.2/references/settings.md:249:> Example: `s3-secret`
content/docs/archives/1.1.2/references/settings.md:255:> Default: `300`
content/docs/archives/1.1.2/references/settings.md:265:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:269:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.1.2/references/settings.md:273:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:281:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:289:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:295:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.1.2/references/settings.md:299:> Default: `25`
content/docs/archives/1.1.2/references/settings.md:307:> Default: `200`
content/docs/archives/1.1.2/references/settings.md:319:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:325:> Default: `12`
content/docs/archives/1.1.2/references/settings.md:337:> **Warning:**
content/docs/archives/1.1.2/references/settings.md:338:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.1.2/references/settings.md:339:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.1.2/references/settings.md:340:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.2/references/settings.md:341:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.1.2/references/settings.md:342:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.2/references/settings.md:346:> Default: `12`
content/docs/archives/1.1.2/references/settings.md:358:> **Warning:**
content/docs/archives/1.1.2/references/settings.md:359:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.1.2/references/settings.md:360:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.1.2/references/settings.md:361:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.2/references/settings.md:362:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.1.2/references/settings.md:363:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.2/references/settings.md:368:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.1.2/references/settings.md:387:> Example: `high-priority`
content/docs/archives/1.1.2/references/settings.md:396:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.1.2/references/settings.md:402:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.1.2/references/settings.md:409:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.2/references/settings.md:417:> Default:
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:            --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:            --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:                "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:            --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> CSI snapshot support is available for Kubernetes versions >= **1.17**.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:12:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:13:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:36:> **Prerequisite**
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:37:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:41:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:42:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:22:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't new data for a long time, Longhorn does the following:
content/docs/archives/1.1.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:23:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.1.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:24:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.1.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.1.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.1.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.1.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.2/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.1.2/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.1.2/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.1.2/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.1.2/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.1.2/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.1.2/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.1.2/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.1.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.1.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/archives/1.1.2/volumes-and-nodes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.2/volumes-and-nodes/maintenance.md:80:        kubectl delete node <node-name>
content/docs/archives/1.1.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.1.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.1.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.1.2/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.1.2/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.1.3/advanced-resources/backing-image.md:17:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.1.3/advanced-resources/backing-image.md:68:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.1.3/advanced-resources/backing-image.md:71:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.1.3/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.1.3/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.1.3/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.1.3/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.1.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.1.3/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.1.3/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.1.3/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.1.3/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.1.3/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.1.3/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.1.3/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.1.3/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.1.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.1.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.1.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.1.3/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.1.3/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.1.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.1.3/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.1.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.1.3/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.1.3/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.1.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.1.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.1.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.1.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.1.3/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.1.3/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.1.3/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.1.3/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.1.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.1.3/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.1.3/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.1.3/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:53:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:110:> GET / HTTP/1.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:111:> Host: 97.107.142.125
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:112:> User-Agent: curl/7.64.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:113:> Accept: */*
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:114:>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:115:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:116:< Server: openresty/1.15.8.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:117:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:118:< Content-Type: text/html
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:119:< Content-Length: 185
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:120:< Connection: keep-alive
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:121:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:122:<
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:123:<html>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:124:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:125:<body>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:126:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:127:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:128:</body>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:129:</html>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:138:> GET / HTTP/1.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:139:> Host: 97.107.142.125
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:140:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:141:> User-Agent: curl/7.64.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> Accept: */*
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:143:>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:144:< HTTP/1.1 200 OK
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:145:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:146:< Content-Type: text/html
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:147:< Content-Length: 1118
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:148:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:149:< ETag: "5ecf084d-3fd"
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:150:< Cache-Control: max-age=0
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:151:<
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:152:<!DOCTYPE html>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:153:<html lang="en">
content/docs/archives/1.1.3/deploy/install/_index.md:109:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.1.3/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.1.3/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.1.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:14:> **Note:**
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:15:> There is a bug in the instance manager image `v1_20201216` shipped in Longhorn v1.1.0 and v1.1.1
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:16:> which can lead to a deadlock in a big cluster with hundreds of volumes.
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:17:> See more details [here](https://github.com/longhorn/longhorn/issues/2697).
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:18:> Longhorn v1.1.2 is shipped with a new instance manager image `v1_20210621` which fixed the deadlock
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:19:> but engine/replica processes of volumes don't migrate from old instance manager to new instance manager
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:20:> until the next time the volumes are detached/attached because Longhorn doesn't want to interrupt the
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:21:> data plane of the volumes.
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:22:>
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:23:> To reduce the chance of hitting the deadlock while the engine/replica processes are still in the old instance manager,
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:24:> you should upgrade engine for volumes in small batches, e.g., upgrading 2 or 3 volumes at a time.
content/docs/archives/1.1.3/deploy/upgrade/_index.md:23:> **Note:**
content/docs/archives/1.1.3/deploy/upgrade/_index.md:24:> There is a bug in the instance manager image `v1_20201216` shipped in Longhorn v1.1.0 and v1.1.1
content/docs/archives/1.1.3/deploy/upgrade/_index.md:25:> which can lead to a deadlock in a big cluster with hundreds of volumes.
content/docs/archives/1.1.3/deploy/upgrade/_index.md:26:> See more details [here](https://github.com/longhorn/longhorn/issues/2697).
content/docs/archives/1.1.3/deploy/upgrade/_index.md:27:> Longhorn v1.1.2 is shipped with a new instance manager image `v1_20210621` which fixed the deadlock
content/docs/archives/1.1.3/deploy/upgrade/_index.md:28:> but engine/replica processes of volumes don't migrate from the old instance manager to the new instance manager
content/docs/archives/1.1.3/deploy/upgrade/_index.md:29:> until the next time the volumes are detached/attached. Longhorn does it because we don't want to interrupt the
content/docs/archives/1.1.3/deploy/upgrade/_index.md:30:> data plane of the volumes.
content/docs/archives/1.1.3/deploy/upgrade/_index.md:31:>
content/docs/archives/1.1.3/deploy/upgrade/_index.md:32:> If you hit the deadlock in the old instance manager, please follow the recovering steps [here](https://github.com/longhorn/longhorn/issues/2697#issuecomment-879374809)
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.1.3/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.1.3/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.1.3/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.1.3/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.1.3/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.1.3/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.1.3/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.1.3/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/archives/1.1.3/references/examples.md:252:          '<BACKUP_URL>'
content/docs/archives/1.1.3/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.1.3/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.1.3/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.1.3/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.3/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.3/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/archives/1.1.3/references/settings.md:61:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:69:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:75:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:86:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:92:> Default: `0`
content/docs/archives/1.1.3/references/settings.md:100:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:114:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.1.3/references/settings.md:120:> Default: `disabled`
content/docs/archives/1.1.3/references/settings.md:135:> Default: `/var/lib/longhorn/`
content/docs/archives/1.1.3/references/settings.md:153:> Default: `longhorn-static`
content/docs/archives/1.1.3/references/settings.md:159:> Default: `3`
content/docs/archives/1.1.3/references/settings.md:171:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:177:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:189:> Default: `do-nothing`
content/docs/archives/1.1.3/references/settings.md:204:> Default: `600`
content/docs/archives/1.1.3/references/settings.md:212:> Default: `if-not-present`
content/docs/archives/1.1.3/references/settings.md:227:> Default: `60`
content/docs/archives/1.1.3/references/settings.md:235:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:243:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.1.3/references/settings.md:249:> Example: `s3-secret`
content/docs/archives/1.1.3/references/settings.md:255:> Default: `300`
content/docs/archives/1.1.3/references/settings.md:265:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:269:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.1.3/references/settings.md:273:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:281:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:289:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:295:> **Note:**
content/docs/archives/1.1.3/references/settings.md:296:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.1.3/references/settings.md:297:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.1.3/references/settings.md:301:> Default: `25`
content/docs/archives/1.1.3/references/settings.md:309:> Default: `200`
content/docs/archives/1.1.3/references/settings.md:321:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:327:> Default: `12`
content/docs/archives/1.1.3/references/settings.md:339:> **Warning:**
content/docs/archives/1.1.3/references/settings.md:340:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.1.3/references/settings.md:341:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.1.3/references/settings.md:342:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.3/references/settings.md:343:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.1.3/references/settings.md:344:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.3/references/settings.md:348:> Default: `12`
content/docs/archives/1.1.3/references/settings.md:360:> **Warning:**
content/docs/archives/1.1.3/references/settings.md:361:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.1.3/references/settings.md:362:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.1.3/references/settings.md:363:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.3/references/settings.md:364:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.1.3/references/settings.md:365:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.3/references/settings.md:370:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.1.3/references/settings.md:389:> Example: `high-priority`
content/docs/archives/1.1.3/references/settings.md:398:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.1.3/references/settings.md:404:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.1.3/references/settings.md:411:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.3/references/settings.md:419:> Default:
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:            --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:            --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:                "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:            --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> CSI snapshot support is available for Kubernetes versions >= **1.17**.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:12:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:13:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:36:> **Prerequisite**
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:37:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:41:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:42:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:22:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't new data for a long time, Longhorn does the following:
content/docs/archives/1.1.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:23:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.1.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:24:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.1.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.1.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.1.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.1.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.3/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.1.3/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.1.3/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.1.3/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.1.3/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.1.3/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.1.3/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.1.3/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.1.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.1.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/archives/1.1.3/volumes-and-nodes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/archives/1.1.3/volumes-and-nodes/maintenance.md:81:        kubectl delete node <node-name>
content/docs/archives/1.1.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.1.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.1.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.1.3/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.1.3/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.1.3/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.0/advanced-resources/backing-image.md:75:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.2.0/advanced-resources/backing-image.md:93:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.2.0/advanced-resources/backing-image.md:96:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.2.0/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.2.0/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.2.0/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.2.0/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.2.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.0/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.2.0/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.2.0/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.2.0/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.2.0/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.2.0/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.2.0/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.2.0/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.2.0/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.0/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.0/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.0/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.2.0/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.0/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.2.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.0/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.2.0/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.2.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.2.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.2.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.2.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.0/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.2.0/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.2.0/advanced-resources/volume-encryption.md:97:For `EXT4` one can use `resize2fs /dev/mapper/<volume-name>`
content/docs/archives/1.2.0/advanced-resources/volume-encryption.md:98:For `XFS` one can use `xfs_growfs /dev/mapper/<volume-name>`
content/docs/archives/1.2.0/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.2.0/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.2.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.0/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.2.0/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.2.0/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.2.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.0/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.2.0/deploy/install/_index.md:111:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.2.0/deploy/install/_index.md:211:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.2.0/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.0/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.2.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.2.0/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.2.0/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.2.0/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.2.0/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.2.0/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.2.0/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.2.0/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.2.0/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.2.0/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.2.0/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.2.0/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.2.0/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.2.0/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.0/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.0/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.0/references/settings.md:62:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:70:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:76:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:87:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:93:> Default: `0`
content/docs/archives/1.2.0/references/settings.md:101:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:115:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.0/references/settings.md:121:> Default: `disabled`
content/docs/archives/1.2.0/references/settings.md:136:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.0/references/settings.md:154:> Default: `longhorn-static`
content/docs/archives/1.2.0/references/settings.md:160:> Default: `3`
content/docs/archives/1.2.0/references/settings.md:172:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:178:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:190:> Default: `do-nothing`
content/docs/archives/1.2.0/references/settings.md:205:> Default: `600`
content/docs/archives/1.2.0/references/settings.md:213:> Default: `if-not-present`
content/docs/archives/1.2.0/references/settings.md:228:> Default: `60`
content/docs/archives/1.2.0/references/settings.md:236:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:244:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.0/references/settings.md:250:> Example: `s3-secret`
content/docs/archives/1.2.0/references/settings.md:256:> Default: `300`
content/docs/archives/1.2.0/references/settings.md:266:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:270:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.0/references/settings.md:274:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:282:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:290:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:296:> **Note:**
content/docs/archives/1.2.0/references/settings.md:297:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.0/references/settings.md:298:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.0/references/settings.md:302:> Default: `disabled`
content/docs/archives/1.2.0/references/settings.md:318:> Default: `ignored`
content/docs/archives/1.2.0/references/settings.md:332:> Default: `25`
content/docs/archives/1.2.0/references/settings.md:340:> Default: `200`
content/docs/archives/1.2.0/references/settings.md:352:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:358:> Default: `12`
content/docs/archives/1.2.0/references/settings.md:370:> **Warning:**
content/docs/archives/1.2.0/references/settings.md:371:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.0/references/settings.md:372:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.0/references/settings.md:373:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.0/references/settings.md:374:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.0/references/settings.md:375:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.0/references/settings.md:379:> Default: `12`
content/docs/archives/1.2.0/references/settings.md:391:> **Warning:**
content/docs/archives/1.2.0/references/settings.md:392:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.0/references/settings.md:393:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.0/references/settings.md:394:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.0/references/settings.md:395:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.0/references/settings.md:396:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.0/references/settings.md:401:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.0/references/settings.md:420:> Example: `high-priority`
content/docs/archives/1.2.0/references/settings.md:429:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.0/references/settings.md:435:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.0/references/settings.md:442:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.0/references/settings.md:450:> Default:
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:            --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:            --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:                "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:            --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.2.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.0/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.2.0/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.2.0/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.2.0/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.2.0/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.2.0/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.2.0/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.2.0/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.2.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.2.0/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.0/volumes-and-nodes/maintenance.md:80:        kubectl delete node <node-name>
content/docs/archives/1.2.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.0/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.2.0/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.2.0/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.1/advanced-resources/backing-image.md:75:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.2.1/advanced-resources/backing-image.md:93:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.2.1/advanced-resources/backing-image.md:96:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.2.1/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.2.1/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.2.1/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.2.1/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.2.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.1/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.2.1/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.2.1/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.2.1/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.2.1/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.2.1/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.2.1/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.2.1/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.2.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.1/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.2.1/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.1/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.2.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.1/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.2.1/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.2.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.2.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.2.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.2.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.1/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.2.1/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.2.1/advanced-resources/volume-encryption.md:97:For `EXT4` one can use `resize2fs /dev/mapper/<volume-name>`
content/docs/archives/1.2.1/advanced-resources/volume-encryption.md:98:For `XFS` one can use `xfs_growfs /dev/mapper/<volume-name>`
content/docs/archives/1.2.1/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.2.1/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.2.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.1/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.2.1/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.2.1/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.2.1/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn v1.2.1 because the supported Kubernetes version has been updated (>= v1.18) in v1.2.1.
content/docs/archives/1.2.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.1/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.2.1/deploy/install/_index.md:111:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.2.1/deploy/install/_index.md:211:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.2.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.1/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.2.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.2.1/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.2.1/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.2.1/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.2.1/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.2.1/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.2.1/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.2.1/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.2.1/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.2.1/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.2.1/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.2.1/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.2.1/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.2.1/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.1/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.1/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.1/references/settings.md:64:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:72:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:78:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:89:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:95:> Default: `0`
content/docs/archives/1.2.1/references/settings.md:103:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:117:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.1/references/settings.md:123:> Default: `disabled`
content/docs/archives/1.2.1/references/settings.md:138:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.1/references/settings.md:156:> Default: `longhorn-static`
content/docs/archives/1.2.1/references/settings.md:162:> Default: `3`
content/docs/archives/1.2.1/references/settings.md:174:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:180:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:192:> Default: `do-nothing`
content/docs/archives/1.2.1/references/settings.md:207:> Default: `600`
content/docs/archives/1.2.1/references/settings.md:215:> Default: `if-not-present`
content/docs/archives/1.2.1/references/settings.md:230:> Default: `60`
content/docs/archives/1.2.1/references/settings.md:235:> Default: `300`
content/docs/archives/1.2.1/references/settings.md:238:> **Note:**
content/docs/archives/1.2.1/references/settings.md:239:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.1/references/settings.md:240:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.1/references/settings.md:246:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:254:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.1/references/settings.md:260:> Example: `s3-secret`
content/docs/archives/1.2.1/references/settings.md:266:> Default: `300`
content/docs/archives/1.2.1/references/settings.md:276:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:280:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.1/references/settings.md:284:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:292:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:300:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:306:> **Note:**
content/docs/archives/1.2.1/references/settings.md:307:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.1/references/settings.md:308:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.1/references/settings.md:312:> Default: `disabled`
content/docs/archives/1.2.1/references/settings.md:328:> Default: `ignored`
content/docs/archives/1.2.1/references/settings.md:342:> Default: `25`
content/docs/archives/1.2.1/references/settings.md:350:> Default: `200`
content/docs/archives/1.2.1/references/settings.md:362:> Default: `5`
content/docs/archives/1.2.1/references/settings.md:368:> **WARNING:**
content/docs/archives/1.2.1/references/settings.md:369:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.1/references/settings.md:370:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.1/references/settings.md:371:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.1/references/settings.md:375:> Default: `12`
content/docs/archives/1.2.1/references/settings.md:387:> **Warning:**
content/docs/archives/1.2.1/references/settings.md:388:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.1/references/settings.md:389:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.1/references/settings.md:390:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.1/references/settings.md:391:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.1/references/settings.md:392:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.1/references/settings.md:396:> Default: `12`
content/docs/archives/1.2.1/references/settings.md:408:> **Warning:**
content/docs/archives/1.2.1/references/settings.md:409:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.1/references/settings.md:410:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.1/references/settings.md:411:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.1/references/settings.md:412:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.1/references/settings.md:413:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.1/references/settings.md:418:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.1/references/settings.md:437:> Example: `high-priority`
content/docs/archives/1.2.1/references/settings.md:446:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.1/references/settings.md:452:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.1/references/settings.md:459:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.1/references/settings.md:467:> Default:
content/docs/archives/1.2.1/references/settings.md:473:> Default: `false`
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:            --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:            --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:                "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:            --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:92:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:97:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:102:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.2.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.1/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.2.1/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.2.1/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.2.1/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.2.1/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.2.1/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.2.1/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.2.1/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.2.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.2.1/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.1/volumes-and-nodes/maintenance.md:80:        kubectl delete node <node-name>
content/docs/archives/1.2.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.1/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.2.1/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.2.1/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.2/advanced-resources/backing-image.md:75:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.2.2/advanced-resources/backing-image.md:93:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.2.2/advanced-resources/backing-image.md:96:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.2.2/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.2.2/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.2.2/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.2.2/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.2.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.2/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.2.2/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.2.2/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.2.2/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.2.2/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.2.2/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.2.2/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.2.2/advanced-resources/deploy/customizing-default-settings.md:18:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.2.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.2/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.2.2/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.2/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.2.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.2/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.2.2/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.2.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.2.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.2.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.2.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.2/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.2.2/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.2.2/advanced-resources/volume-encryption.md:97:For `EXT4` one can use `resize2fs /dev/mapper/<volume-name>`
content/docs/archives/1.2.2/advanced-resources/volume-encryption.md:98:For `XFS` one can use `xfs_growfs /dev/mapper/<volume-name>`
content/docs/archives/1.2.2/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.2.2/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.2.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.2/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.2.2/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.2.2/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.2.2/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18) in varchives.
content/docs/archives/1.2.2/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.2/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.2/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.2.2/deploy/install/_index.md:111:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.2.2/deploy/install/_index.md:211:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.2.2/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.2/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.2.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.2.2/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.2.2/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.2.2/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.2.2/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.2.2/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.2.2/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.2.2/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.2.2/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.2.2/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.2.2/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.2.2/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.2.2/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.2.2/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.2/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.2/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.2/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.2/references/networking.md:70:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.2/references/networking.md:82:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.2/references/settings.md:64:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:72:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:78:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:89:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:95:> Default: `0`
content/docs/archives/1.2.2/references/settings.md:103:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:117:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.2/references/settings.md:123:> Default: `disabled`
content/docs/archives/1.2.2/references/settings.md:138:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.2/references/settings.md:156:> Default: `longhorn-static`
content/docs/archives/1.2.2/references/settings.md:162:> Default: `3`
content/docs/archives/1.2.2/references/settings.md:174:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:180:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:192:> Default: `do-nothing`
content/docs/archives/1.2.2/references/settings.md:207:> Default: `600`
content/docs/archives/1.2.2/references/settings.md:215:> Default: `if-not-present`
content/docs/archives/1.2.2/references/settings.md:231:> Default: `60`
content/docs/archives/1.2.2/references/settings.md:236:> Default: `300`
content/docs/archives/1.2.2/references/settings.md:239:> **Note:**
content/docs/archives/1.2.2/references/settings.md:240:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.2/references/settings.md:241:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.2/references/settings.md:247:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:255:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.2/references/settings.md:261:> Example: `s3-secret`
content/docs/archives/1.2.2/references/settings.md:267:> Default: `300`
content/docs/archives/1.2.2/references/settings.md:277:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:281:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.2/references/settings.md:285:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:293:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:301:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:307:> **Note:**
content/docs/archives/1.2.2/references/settings.md:308:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.2/references/settings.md:309:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.2/references/settings.md:313:> Default: `disabled`
content/docs/archives/1.2.2/references/settings.md:329:> Default: `ignored`
content/docs/archives/1.2.2/references/settings.md:343:> Default: `25`
content/docs/archives/1.2.2/references/settings.md:351:> Default: `200`
content/docs/archives/1.2.2/references/settings.md:363:> Default: `5`
content/docs/archives/1.2.2/references/settings.md:369:> **WARNING:**
content/docs/archives/1.2.2/references/settings.md:370:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.2/references/settings.md:371:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.2/references/settings.md:372:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.2/references/settings.md:376:> Default: `12`
content/docs/archives/1.2.2/references/settings.md:388:> **Warning:**
content/docs/archives/1.2.2/references/settings.md:389:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.2/references/settings.md:390:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.2/references/settings.md:391:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.2/references/settings.md:392:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.2/references/settings.md:393:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.2/references/settings.md:397:> Default: `12`
content/docs/archives/1.2.2/references/settings.md:409:> **Warning:**
content/docs/archives/1.2.2/references/settings.md:410:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.2/references/settings.md:411:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.2/references/settings.md:412:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.2/references/settings.md:413:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.2/references/settings.md:414:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.2/references/settings.md:419:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.2/references/settings.md:438:> Example: `high-priority`
content/docs/archives/1.2.2/references/settings.md:447:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.2/references/settings.md:453:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.2/references/settings.md:460:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.2/references/settings.md:468:> Default:
content/docs/archives/1.2.2/references/settings.md:474:> Default: `false`
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:            --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:            --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:                "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:                "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:                "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:        kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:            --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:    > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.2/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.2/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.2/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.2.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.2/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.2.2/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.2.2/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.2.2/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.2.2/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.2.2/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.2.2/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.2.2/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.2.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.2.2/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.2/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/archives/1.2.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.2/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.2.2/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.2.2/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.3/advanced-resources/backing-image.md:76:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.2.3/advanced-resources/backing-image.md:94:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.2.3/advanced-resources/backing-image.md:97:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.2.3/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.2.3/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.2.3/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.2.3/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.2.3/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.2.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.3/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.2.3/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.2.3/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.2.3/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.2.3/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.2.3/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.2.3/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.2.3/advanced-resources/deploy/customizing-default-settings.md:24:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.2.3/advanced-resources/deploy/customizing-default-settings.md:171:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.2.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.3/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.2.3/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.3/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.2.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.3/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.2.3/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.2.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.2.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.2.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.2.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.3/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.2.3/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.2.3/best-practices.md:107:> Recommend: `false`
content/docs/archives/1.2.3/best-practices.md:112:> Recommend: `false`
content/docs/archives/1.2.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.3/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.2.3/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.2.3/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.2.3/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18) in varchives.
content/docs/archives/1.2.3/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.3/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.3/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.2.3/deploy/install/_index.md:150:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.2.3/deploy/install/_index.md:250:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.2.3/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.3/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.2.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.2.3/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.2.3/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.2.3/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.2.3/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.2.3/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.2.3/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.2.3/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.2.3/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.2.3/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.2.3/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.2.3/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.2.3/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.2.3/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.3/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.3/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.3/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.3/references/networking.md:70:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.3/references/networking.md:82:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.3/references/settings.md:64:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:72:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:78:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:89:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:95:> Default: `0`
content/docs/archives/1.2.3/references/settings.md:103:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:117:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.3/references/settings.md:123:> Default: `disabled`
content/docs/archives/1.2.3/references/settings.md:138:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.3/references/settings.md:156:> Default: `longhorn-static`
content/docs/archives/1.2.3/references/settings.md:162:> Default: `3`
content/docs/archives/1.2.3/references/settings.md:174:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:180:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:192:> Default: `do-nothing`
content/docs/archives/1.2.3/references/settings.md:207:> Default: `600`
content/docs/archives/1.2.3/references/settings.md:215:> Default: `if-not-present`
content/docs/archives/1.2.3/references/settings.md:231:> Default: `60`
content/docs/archives/1.2.3/references/settings.md:236:> Default: `300`
content/docs/archives/1.2.3/references/settings.md:239:> **Note:**
content/docs/archives/1.2.3/references/settings.md:240:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.3/references/settings.md:241:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.3/references/settings.md:247:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:255:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.3/references/settings.md:261:> Example: `s3-secret`
content/docs/archives/1.2.3/references/settings.md:267:> Default: `300`
content/docs/archives/1.2.3/references/settings.md:277:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:281:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.3/references/settings.md:285:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:293:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:301:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:307:> **Note:**
content/docs/archives/1.2.3/references/settings.md:308:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.3/references/settings.md:309:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.3/references/settings.md:313:> Default: `disabled`
content/docs/archives/1.2.3/references/settings.md:329:> Default: `ignored`
content/docs/archives/1.2.3/references/settings.md:343:> Default: `25`
content/docs/archives/1.2.3/references/settings.md:351:> Default: `200`
content/docs/archives/1.2.3/references/settings.md:363:> Default: `5`
content/docs/archives/1.2.3/references/settings.md:369:> **WARNING:**
content/docs/archives/1.2.3/references/settings.md:370:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.3/references/settings.md:371:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.3/references/settings.md:372:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.3/references/settings.md:376:> Default: `12`
content/docs/archives/1.2.3/references/settings.md:388:> **Warning:**
content/docs/archives/1.2.3/references/settings.md:389:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.3/references/settings.md:390:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.3/references/settings.md:391:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.3/references/settings.md:392:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.3/references/settings.md:393:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.3/references/settings.md:397:> Default: `12`
content/docs/archives/1.2.3/references/settings.md:409:> **Warning:**
content/docs/archives/1.2.3/references/settings.md:410:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.3/references/settings.md:411:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.3/references/settings.md:412:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.3/references/settings.md:413:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.3/references/settings.md:414:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.3/references/settings.md:419:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.3/references/settings.md:438:> Example: `high-priority`
content/docs/archives/1.2.3/references/settings.md:447:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.3/references/settings.md:453:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.3/references/settings.md:460:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.3/references/settings.md:468:> Default:
content/docs/archives/1.2.3/references/settings.md:474:> Default: `false`
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.3/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.3/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.3/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.2.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.3/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.2.3/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.2.3/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.2.3/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.2.3/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.2.3/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.2.3/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.2.3/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.2.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.2.3/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.3/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/archives/1.2.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.3/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.2.3/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.2.3/volumes-and-nodes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.4/advanced-resources/backing-image.md:75:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.2.4/advanced-resources/backing-image.md:93:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.2.4/advanced-resources/backing-image.md:96:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.2.4/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.2.4/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.2.4/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.2.4/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.2.4/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.2.4/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.4/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.2.4/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.2.4/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.2.4/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.2.4/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.2.4/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.2.4/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.2.4/advanced-resources/deploy/customizing-default-settings.md:24:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.2.4/advanced-resources/deploy/customizing-default-settings.md:171:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.2.4/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.4/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.4/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.4/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.2.4/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.4/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.4/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.2.4/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.4/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.2.4/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.2.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.2.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.2.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.2.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.4/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.2.4/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.2.4/best-practices.md:109:> Recommend: `false`
content/docs/archives/1.2.4/best-practices.md:114:> Recommend: `false`
content/docs/archives/1.2.4/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.4/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.2.4/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.2.4/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.2.4/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18) in varchives.
content/docs/archives/1.2.4/deploy/important-notes/index.md:12:3. When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to varchives, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890)
content/docs/archives/1.2.4/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.4/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.4/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.2.4/deploy/install/_index.md:150:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.2.4/deploy/install/_index.md:250:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.2.4/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.4/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.2.4/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.2.4/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.2.4/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.2.4/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.2.4/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.2.4/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.2.4/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.2.4/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.2.4/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.2.4/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.2.4/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.2.4/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.2.4/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.2.4/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.4/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.4/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.4/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.4/references/networking.md:70:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.4/references/networking.md:82:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.4/references/settings.md:64:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:72:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:78:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:89:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:95:> Default: `0`
content/docs/archives/1.2.4/references/settings.md:103:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:117:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.4/references/settings.md:123:> Default: `disabled`
content/docs/archives/1.2.4/references/settings.md:138:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.4/references/settings.md:156:> Default: `longhorn-static`
content/docs/archives/1.2.4/references/settings.md:162:> Default: `3`
content/docs/archives/1.2.4/references/settings.md:174:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:180:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:192:> Default: `do-nothing`
content/docs/archives/1.2.4/references/settings.md:207:> Default: `600`
content/docs/archives/1.2.4/references/settings.md:215:> Default: `if-not-present`
content/docs/archives/1.2.4/references/settings.md:231:> Default: `60`
content/docs/archives/1.2.4/references/settings.md:236:> Default: `300`
content/docs/archives/1.2.4/references/settings.md:239:> **Note:**
content/docs/archives/1.2.4/references/settings.md:240:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.4/references/settings.md:241:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.4/references/settings.md:247:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:255:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.4/references/settings.md:261:> Example: `s3-secret`
content/docs/archives/1.2.4/references/settings.md:267:> Default: `300`
content/docs/archives/1.2.4/references/settings.md:277:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:281:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.4/references/settings.md:285:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:293:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:301:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:307:> **Note:**
content/docs/archives/1.2.4/references/settings.md:308:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.4/references/settings.md:309:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.4/references/settings.md:313:> Default: `disabled`
content/docs/archives/1.2.4/references/settings.md:329:> Default: `ignored`
content/docs/archives/1.2.4/references/settings.md:343:> Default: `25`
content/docs/archives/1.2.4/references/settings.md:351:> Default: `200`
content/docs/archives/1.2.4/references/settings.md:363:> Default: `5`
content/docs/archives/1.2.4/references/settings.md:369:> **WARNING:**
content/docs/archives/1.2.4/references/settings.md:370:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.4/references/settings.md:371:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.4/references/settings.md:372:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.4/references/settings.md:376:> Default: `12`
content/docs/archives/1.2.4/references/settings.md:388:> **Warning:**
content/docs/archives/1.2.4/references/settings.md:389:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.4/references/settings.md:390:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.4/references/settings.md:391:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.4/references/settings.md:392:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.4/references/settings.md:393:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.4/references/settings.md:397:> Default: `12`
content/docs/archives/1.2.4/references/settings.md:409:> **Warning:**
content/docs/archives/1.2.4/references/settings.md:410:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.4/references/settings.md:411:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.4/references/settings.md:412:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.4/references/settings.md:413:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.4/references/settings.md:414:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.4/references/settings.md:419:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.4/references/settings.md:438:> Example: `high-priority`
content/docs/archives/1.2.4/references/settings.md:447:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.4/references/settings.md:453:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.4/references/settings.md:460:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.4/references/settings.md:468:> Default:
content/docs/archives/1.2.4/references/settings.md:474:> Default: `false`
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.4/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.4/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.4/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.2.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.4/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.4/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.4/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.4/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.2.4/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.2.4/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.2.4/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.2.4/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.2.4/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.2.4/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.2.4/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.2.4/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.4/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.2.4/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.4/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/archives/1.2.4/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.4/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.4/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.4/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.2.4/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:43:    > Users can see the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn does not support TRIM/UNMAP operations, so the `discard` mount option or `fstrim` in the filesystem layer cannot reclaim the unused blocks. In consequence, the actual size of Longhorn volumes cannot be shrunk in this case.
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:50:     >
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:71:    >
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/1.2.5/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.2.5/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.2.5/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.2.5/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.2.5/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.2.5/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.5/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.2.5/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.2.5/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.2.5/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.2.5/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.2.5/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.2.5/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.2.5/advanced-resources/deploy/customizing-default-settings.md:24:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.2.5/advanced-resources/deploy/customizing-default-settings.md:171:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.2.5/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.5/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.5/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.5/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.2.5/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.5/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/archives/1.2.5/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.5/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.2.5/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.5/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.2.5/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.2.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.2.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.2.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.2.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.5/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.2.5/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.2.5/advanced-resources/backing-image.md:75:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.2.5/advanced-resources/backing-image.md:93:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.2.5/advanced-resources/backing-image.md:96:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.2.5/best-practices.md:107:> Recommend: `false`
content/docs/archives/1.2.5/best-practices.md:112:> Recommend: `false`
content/docs/archives/1.2.5/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.5/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.2.5/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.2.5/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.2.5/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18) in varchives.
content/docs/archives/1.2.5/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.5/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.5/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.2.5/deploy/install/_index.md:119:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.2.5/deploy/install/_index.md:219:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.2.5/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.5/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.2.5/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.2.5/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.2.5/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.2.5/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.2.5/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.2.5/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.2.5/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.2.5/references/examples.md:247:      nodeName: <NODE_NAME>
content/docs/archives/1.2.5/references/examples.md:255:          '<BACKUP_URL>'
content/docs/archives/1.2.5/references/examples.md:256:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.2.5/references/examples.md:257:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.2.5/references/examples.md:265:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.2.5/references/examples.md:271:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.5/references/examples.md:276:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.5/references/examples.md:281:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.5/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.5/references/networking.md:70:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.5/references/networking.md:82:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.5/references/settings.md:65:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:73:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:79:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:90:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:96:> Default: `0`
content/docs/archives/1.2.5/references/settings.md:104:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:118:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.5/references/settings.md:124:> Default: `disabled`
content/docs/archives/1.2.5/references/settings.md:139:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.5/references/settings.md:157:> Default: `longhorn-static`
content/docs/archives/1.2.5/references/settings.md:163:> Default: `3`
content/docs/archives/1.2.5/references/settings.md:175:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:181:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:193:> Default: `do-nothing`
content/docs/archives/1.2.5/references/settings.md:208:> Default: `600`
content/docs/archives/1.2.5/references/settings.md:216:> Default: `if-not-present`
content/docs/archives/1.2.5/references/settings.md:232:> Default: `60`
content/docs/archives/1.2.5/references/settings.md:237:> Default: `300`
content/docs/archives/1.2.5/references/settings.md:240:> **Note:**
content/docs/archives/1.2.5/references/settings.md:241:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.5/references/settings.md:242:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.5/references/settings.md:248:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:256:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.5/references/settings.md:262:> Example: `s3-secret`
content/docs/archives/1.2.5/references/settings.md:268:> Default: `300`
content/docs/archives/1.2.5/references/settings.md:276:> Default: `1440`
content/docs/archives/1.2.5/references/settings.md:286:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:290:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.5/references/settings.md:294:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:302:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:310:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:316:> **Note:**
content/docs/archives/1.2.5/references/settings.md:317:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.5/references/settings.md:318:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.5/references/settings.md:322:> Default: `disabled`
content/docs/archives/1.2.5/references/settings.md:338:> Default: `ignored`
content/docs/archives/1.2.5/references/settings.md:352:> Default: `25`
content/docs/archives/1.2.5/references/settings.md:360:> Default: `200`
content/docs/archives/1.2.5/references/settings.md:372:> Default: `5`
content/docs/archives/1.2.5/references/settings.md:378:> **WARNING:**
content/docs/archives/1.2.5/references/settings.md:379:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.5/references/settings.md:380:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.5/references/settings.md:381:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.5/references/settings.md:385:> Default: `12`
content/docs/archives/1.2.5/references/settings.md:397:> **Warning:**
content/docs/archives/1.2.5/references/settings.md:398:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.5/references/settings.md:399:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.5/references/settings.md:400:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.5/references/settings.md:401:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.5/references/settings.md:402:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.5/references/settings.md:406:> Default: `12`
content/docs/archives/1.2.5/references/settings.md:418:> **Warning:**
content/docs/archives/1.2.5/references/settings.md:419:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.5/references/settings.md:420:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.5/references/settings.md:421:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.5/references/settings.md:422:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.5/references/settings.md:423:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.5/references/settings.md:428:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.5/references/settings.md:447:> Example: `high-priority`
content/docs/archives/1.2.5/references/settings.md:456:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.5/references/settings.md:462:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.5/references/settings.md:469:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.5/references/settings.md:477:> Default:
content/docs/archives/1.2.5/references/settings.md:483:> Default: `false`
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.5/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.5/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.5/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.2.5/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.5/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.5/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.5/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.5/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.2.5/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.2.5/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.2.5/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.2.5/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.2.5/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.2.5/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.2.5/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.2.5/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.5/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.2.5/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.5/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/archives/1.2.5/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.5/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.5/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.5/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.2.5/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:43:    > Users can see the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn does not support TRIM/UNMAP operations, so the `discard` mount option or `fstrim` in the filesystem layer cannot reclaim the unused blocks. In consequence, the actual size of Longhorn volumes cannot be shrunk in this case.
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:50:     >
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:71:    >
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/1.2.6/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.2.6/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.2.6/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.2.6/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.2.6/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.2.6/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.6/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.2.6/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.2.6/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.2.6/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.2.6/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.2.6/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.2.6/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.2.6/advanced-resources/deploy/customizing-default-settings.md:24:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.2.6/advanced-resources/deploy/customizing-default-settings.md:171:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.2.6/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.6/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.6/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.6/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.2.6/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.6/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/archives/1.2.6/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.6/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.2.6/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.6/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.2.6/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.2.6/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.2.6/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.2.6/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.2.6/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.6/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.2.6/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.2.6/advanced-resources/backing-image.md:75:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.2.6/advanced-resources/backing-image.md:93:- The unused backing images can be cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > CleanupBackingImage** of one backing image. Then choose disks.
content/docs/archives/1.2.6/advanced-resources/backing-image.md:96:- There is one pod with the naming schema `<Backing image name>-<first 8 characters of the disk UUID>` handling the actual file for each backing image in each disk.
content/docs/archives/1.2.6/best-practices.md:107:> Recommend: `false`
content/docs/archives/1.2.6/best-practices.md:112:> Recommend: `false`
content/docs/archives/1.2.6/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.6/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.2.6/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.2.6/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.2.6/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18) in varchives.
content/docs/archives/1.2.6/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.6/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.6/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.2.6/deploy/install/_index.md:119:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.2.6/deploy/install/_index.md:219:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.2.6/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.6/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.2.6/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:282:      - to: <the email address to send notifications to>
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:283:        from: <the sender address>
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:284:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:286:        auth_username: <the username>
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:287:        auth_identity: <the identity>
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:288:        auth_password: <the password>
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:300:      - api_url: <the Slack webhook URL>
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:301:        channel: <the channel or user to send notifications to>
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:342:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Alertmanager over TLS connection.
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:366:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:468:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:470:    > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to `ClusterIP`, and set up an Ingress-controller to expose the web UI of Prometheus server over TLS connection.
content/docs/archives/1.2.6/monitoring/prometheus-and-grafana-setup.md:565:   > Use the above NodePort service for quick verification only because it doesn't communicate over TLS connection. You may want to change the service type to ClusterIP, and setup an Ingress-controller to expose Grafana over TLS connection.
content/docs/archives/1.2.6/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.2.6/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.2.6/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.2.6/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.2.6/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.2.6/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/archives/1.2.6/references/examples.md:252:          '<BACKUP_URL>'
content/docs/archives/1.2.6/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.2.6/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.2.6/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.2.6/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.6/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.6/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/archives/1.2.6/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.6/references/networking.md:70:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.6/references/networking.md:82:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.2.6/references/settings.md:65:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:73:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:79:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:90:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:96:> Default: `0`
content/docs/archives/1.2.6/references/settings.md:104:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:118:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.6/references/settings.md:124:> Default: `disabled`
content/docs/archives/1.2.6/references/settings.md:139:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.6/references/settings.md:157:> Default: `longhorn-static`
content/docs/archives/1.2.6/references/settings.md:163:> Default: `3`
content/docs/archives/1.2.6/references/settings.md:175:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:181:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:193:> Default: `do-nothing`
content/docs/archives/1.2.6/references/settings.md:208:> Default: `600`
content/docs/archives/1.2.6/references/settings.md:216:> Default: `if-not-present`
content/docs/archives/1.2.6/references/settings.md:232:> Default: `60`
content/docs/archives/1.2.6/references/settings.md:237:> Default: `300`
content/docs/archives/1.2.6/references/settings.md:240:> **Note:**
content/docs/archives/1.2.6/references/settings.md:241:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.6/references/settings.md:242:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.6/references/settings.md:248:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:256:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.6/references/settings.md:262:> Example: `s3-secret`
content/docs/archives/1.2.6/references/settings.md:268:> Default: `300`
content/docs/archives/1.2.6/references/settings.md:276:> Default: `1440`
content/docs/archives/1.2.6/references/settings.md:286:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:290:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.6/references/settings.md:294:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:302:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:310:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:316:> **Note:**
content/docs/archives/1.2.6/references/settings.md:317:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.6/references/settings.md:318:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.6/references/settings.md:322:> Default: `disabled`
content/docs/archives/1.2.6/references/settings.md:338:> Default: `ignored`
content/docs/archives/1.2.6/references/settings.md:352:> Default: `25`
content/docs/archives/1.2.6/references/settings.md:360:> Default: `200`
content/docs/archives/1.2.6/references/settings.md:372:> Default: `5`
content/docs/archives/1.2.6/references/settings.md:378:> **WARNING:**
content/docs/archives/1.2.6/references/settings.md:379:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.6/references/settings.md:380:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.6/references/settings.md:381:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.6/references/settings.md:385:> Default: `12`
content/docs/archives/1.2.6/references/settings.md:397:> **Warning:**
content/docs/archives/1.2.6/references/settings.md:398:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.6/references/settings.md:399:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.6/references/settings.md:400:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.6/references/settings.md:401:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.6/references/settings.md:402:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.6/references/settings.md:406:> Default: `12`
content/docs/archives/1.2.6/references/settings.md:418:> **Warning:**
content/docs/archives/1.2.6/references/settings.md:419:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.6/references/settings.md:420:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.6/references/settings.md:421:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.6/references/settings.md:422:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.6/references/settings.md:423:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.6/references/settings.md:428:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.6/references/settings.md:447:> Example: `high-priority`
content/docs/archives/1.2.6/references/settings.md:456:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.6/references/settings.md:462:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.6/references/settings.md:469:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.6/references/settings.md:477:> Default:
content/docs/archives/1.2.6/references/settings.md:483:> Default: `false`
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.6/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.6/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.6/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.2.6/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.6/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.6/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.6/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.6/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.2.6/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.2.6/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.2.6/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.2.6/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.2.6/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.2.6/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.2.6/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.2.6/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.6/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.2.6/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.2.6/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/archives/1.2.6/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.6/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.6/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.6/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.2.6/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:43:    > Users can see the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn does not support TRIM/UNMAP operations, so the `discard` mount option or `fstrim` in the filesystem layer cannot reclaim the unused blocks. In consequence, the actual size of Longhorn volumes cannot be shrunk in this case.
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:50:     >
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:71:    >
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/1.3.0/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/archives/1.3.0/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/archives/1.3.0/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.3.0/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/archives/1.3.0/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.0/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.3.0/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.3.0/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.3.0/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.3.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.0/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.3.0/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.3.0/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.3.0/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.3.0/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.3.0/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.3.0/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:184:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:190:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:195:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:204:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:207:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:211:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.3.0/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.0/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.0/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.0/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.3.0/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.0/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.0/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/archives/1.3.0/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/archives/1.3.0/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/archives/1.3.0/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/archives/1.3.0/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/archives/1.3.0/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.3.0/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.3.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.3.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.3.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.3.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.0/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.3.0/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.3.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.0/best-practices.md:121:> Recommend: `false`
content/docs/archives/1.3.0/best-practices.md:127:> Recommend: `false`
content/docs/archives/1.3.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.0/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.3.0/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.3.0/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.3.0/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18 and <= v1.24) in varchives.
content/docs/archives/1.3.0/deploy/important-notes/index.md:12:3. When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to varchives, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890)
content/docs/archives/1.3.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.0/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.3.0/deploy/install/_index.md:147:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.3.0/deploy/install/_index.md:251:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.3.0/deploy/install/install-with-helm.md:15:> [This script](https://github.com/longhorn/longhorn/blob/varchives/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/archives/1.3.0/deploy/install/install-with-helm.md:20:> **Note**: The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.3.0/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.0/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.3.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/archives/1.3.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/archives/1.3.0/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.3.0/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.3.0/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.3.0/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.3.0/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.3.0/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.3.0/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/archives/1.3.0/references/examples.md:252:          '<BACKUP_URL>'
content/docs/archives/1.3.0/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.3.0/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.3.0/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.3.0/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.0/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.0/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.0/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.0/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.0/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.0/references/settings.md:68:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:76:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:82:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:93:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:99:> Default: `0`
content/docs/archives/1.3.0/references/settings.md:107:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:121:> Default: `longhorn.io/v1beta2`
content/docs/archives/1.3.0/references/settings.md:127:> Default: `disabled`
content/docs/archives/1.3.0/references/settings.md:142:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.0/references/settings.md:160:> Default: `longhorn-static`
content/docs/archives/1.3.0/references/settings.md:166:> Default: `3`
content/docs/archives/1.3.0/references/settings.md:178:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:184:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:196:> Default: `do-nothing`
content/docs/archives/1.3.0/references/settings.md:211:> Default: `600`
content/docs/archives/1.3.0/references/settings.md:219:> Default: `if-not-present`
content/docs/archives/1.3.0/references/settings.md:235:> Default: `60`
content/docs/archives/1.3.0/references/settings.md:240:> Default: `300`
content/docs/archives/1.3.0/references/settings.md:243:> **Note:**
content/docs/archives/1.3.0/references/settings.md:244:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.0/references/settings.md:245:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.0/references/settings.md:250:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:258:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:266:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.0/references/settings.md:272:> Example: `s3-secret`
content/docs/archives/1.3.0/references/settings.md:278:> Default: `300`
content/docs/archives/1.3.0/references/settings.md:288:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:292:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.0/references/settings.md:296:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:304:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:312:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:318:> **Note:**
content/docs/archives/1.3.0/references/settings.md:319:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.0/references/settings.md:320:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.0/references/settings.md:324:> Default: `disabled`
content/docs/archives/1.3.0/references/settings.md:340:> Default: `ignored`
content/docs/archives/1.3.0/references/settings.md:354:> Default: `25`
content/docs/archives/1.3.0/references/settings.md:362:> Default: `200`
content/docs/archives/1.3.0/references/settings.md:374:> Default: `5`
content/docs/archives/1.3.0/references/settings.md:380:> **WARNING:**
content/docs/archives/1.3.0/references/settings.md:381:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.0/references/settings.md:382:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.0/references/settings.md:383:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.0/references/settings.md:387:> Default: `12`
content/docs/archives/1.3.0/references/settings.md:399:> **Warning:**
content/docs/archives/1.3.0/references/settings.md:400:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.0/references/settings.md:401:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.0/references/settings.md:402:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.0/references/settings.md:403:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.0/references/settings.md:404:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.0/references/settings.md:408:> Default: `12`
content/docs/archives/1.3.0/references/settings.md:420:> **Warning:**
content/docs/archives/1.3.0/references/settings.md:421:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.0/references/settings.md:422:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.0/references/settings.md:423:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.0/references/settings.md:424:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.0/references/settings.md:425:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.0/references/settings.md:430:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.0/references/settings.md:449:> Example: `high-priority`
content/docs/archives/1.3.0/references/settings.md:458:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.0/references/settings.md:464:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.0/references/settings.md:471:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.0/references/settings.md:477:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:483:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.0/references/settings.md:487:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.0/references/settings.md:491:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.0/references/settings.md:500:> Default:
content/docs/archives/1.3.0/references/settings.md:506:> Default: `false`
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.3.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.0/volumes-and-nodes/create-volumes.md:151:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/archives/1.3.0/volumes-and-nodes/create-volumes.md:155:Labels:          <none>
content/docs/archives/1.3.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.0/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.3.0/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.3.0/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.3.0/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.3.0/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.3.0/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.3.0/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.3.0/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.3.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.3.0/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.0/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/archives/1.3.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.0/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.3.0/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:43:    > Users can see the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn does not support TRIM/UNMAP operations, so the `discard` mount option or `fstrim` in the filesystem layer cannot reclaim the unused blocks. In consequence, the actual size of Longhorn volumes cannot be shrunk in this case.
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:50:     >
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:71:    >
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/1.3.1/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/archives/1.3.1/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/archives/1.3.1/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.3.1/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/archives/1.3.1/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.1/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.3.1/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.3.1/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.3.1/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.3.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.1/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.3.1/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.3.1/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.3.1/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.3.1/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.3.1/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.3.1/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.3.1/advanced-resources/deploy/customizing-default-settings.md:24:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.3.1/advanced-resources/deploy/customizing-default-settings.md:173:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.3.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.1/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.3.1/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.1/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/archives/1.3.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.1/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.1/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/archives/1.3.1/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/archives/1.3.1/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/archives/1.3.1/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/archives/1.3.1/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/archives/1.3.1/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.3.1/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.3.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.3.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.3.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.3.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.1/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.3.1/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.3.1/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.1/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.1/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.1/best-practices.md:118:> Recommend: `false`
content/docs/archives/1.3.1/best-practices.md:123:> Recommend: `false`
content/docs/archives/1.3.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.1/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.3.1/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.3.1/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.3.1/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18 and <= v1.24) in varchives.
content/docs/archives/1.3.1/deploy/important-notes/index.md:12:3. When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to varchives, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890)
content/docs/archives/1.3.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.1/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.3.1/deploy/install/_index.md:147:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.3.1/deploy/install/_index.md:251:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.3.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.1/deploy/upgrade/_index.md:18:- To upgrade from v1.3.x (< varchives) or v1.2.x, see [this section.](./longhorn-manager)
content/docs/archives/1.3.1/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.3.x (< varchives) or v1.2.x
content/docs/archives/1.3.1/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.3.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.1/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.3.x (< varchives) or v1.2.x to varchives.
content/docs/archives/1.3.1/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/archives/1.3.1/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.3.1/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.3.1/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.3.1/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.3.1/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.3.1/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.3.1/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/archives/1.3.1/references/examples.md:252:          '<BACKUP_URL>'
content/docs/archives/1.3.1/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.3.1/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.3.1/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.3.1/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.1/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.1/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.1/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.1/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.1/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.1/references/settings.md:68:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:76:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:82:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:93:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:99:> Default: `0`
content/docs/archives/1.3.1/references/settings.md:107:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:121:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.3.1/references/settings.md:127:> Default: `disabled`
content/docs/archives/1.3.1/references/settings.md:142:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.1/references/settings.md:160:> Default: `longhorn-static`
content/docs/archives/1.3.1/references/settings.md:166:> Default: `3`
content/docs/archives/1.3.1/references/settings.md:178:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:184:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:196:> Default: `do-nothing`
content/docs/archives/1.3.1/references/settings.md:211:> Default: `600`
content/docs/archives/1.3.1/references/settings.md:219:> Default: `if-not-present`
content/docs/archives/1.3.1/references/settings.md:235:> Default: `60`
content/docs/archives/1.3.1/references/settings.md:240:> Default: `300`
content/docs/archives/1.3.1/references/settings.md:243:> **Note:**
content/docs/archives/1.3.1/references/settings.md:244:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.1/references/settings.md:245:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.1/references/settings.md:250:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:258:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:266:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.1/references/settings.md:272:> Example: `s3-secret`
content/docs/archives/1.3.1/references/settings.md:278:> Default: `300`
content/docs/archives/1.3.1/references/settings.md:288:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:292:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.1/references/settings.md:296:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:304:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:312:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:318:> **Note:**
content/docs/archives/1.3.1/references/settings.md:319:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.1/references/settings.md:320:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.1/references/settings.md:324:> Default: `disabled`
content/docs/archives/1.3.1/references/settings.md:340:> Default: `ignored`
content/docs/archives/1.3.1/references/settings.md:354:> Default: `25`
content/docs/archives/1.3.1/references/settings.md:362:> Default: `200`
content/docs/archives/1.3.1/references/settings.md:374:> Default: `5`
content/docs/archives/1.3.1/references/settings.md:380:> **WARNING:**
content/docs/archives/1.3.1/references/settings.md:381:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.1/references/settings.md:382:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.1/references/settings.md:383:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.1/references/settings.md:387:> Default: `12`
content/docs/archives/1.3.1/references/settings.md:399:> **Warning:**
content/docs/archives/1.3.1/references/settings.md:400:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.1/references/settings.md:401:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.1/references/settings.md:402:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.1/references/settings.md:403:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.1/references/settings.md:404:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.1/references/settings.md:408:> Default: `12`
content/docs/archives/1.3.1/references/settings.md:420:> **Warning:**
content/docs/archives/1.3.1/references/settings.md:421:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.1/references/settings.md:422:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.1/references/settings.md:423:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.1/references/settings.md:424:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.1/references/settings.md:425:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.1/references/settings.md:430:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.1/references/settings.md:449:> Example: `high-priority`
content/docs/archives/1.3.1/references/settings.md:458:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.1/references/settings.md:464:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.1/references/settings.md:471:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.1/references/settings.md:477:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:483:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.1/references/settings.md:487:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.1/references/settings.md:491:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.1/references/settings.md:500:> Default:
content/docs/archives/1.3.1/references/settings.md:506:> Default: `false`
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.3.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.1/volumes-and-nodes/create-volumes.md:151:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/archives/1.3.1/volumes-and-nodes/create-volumes.md:155:Labels:          <none>
content/docs/archives/1.3.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.1/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.3.1/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.3.1/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.3.1/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.3.1/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.3.1/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.3.1/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.3.1/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.3.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.3.1/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.1/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/archives/1.3.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.1/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.3.1/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:43:    > Users can see the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn does not support TRIM/UNMAP operations, so the `discard` mount option or `fstrim` in the filesystem layer cannot reclaim the unused blocks. In consequence, the actual size of Longhorn volumes cannot be shrunk in this case.
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:50:     >
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:71:    >
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/1.3.2/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/archives/1.3.2/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/archives/1.3.2/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.3.2/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/archives/1.3.2/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.2/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.3.2/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.3.2/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.3.2/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.3.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.2/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.3.2/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.3.2/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.3.2/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.3.2/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.3.2/advanced-resources/data-recovery/recover-without-system.md:26:7. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.3.2/advanced-resources/data-recovery/recover-without-system.md:38:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.3.2/advanced-resources/deploy/customizing-default-settings.md:24:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.3.2/advanced-resources/deploy/customizing-default-settings.md:173:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.3.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.2/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.3.2/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.2/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/archives/1.3.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.2/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.2/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/archives/1.3.2/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/archives/1.3.2/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/archives/1.3.2/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/archives/1.3.2/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/archives/1.3.2/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.3.2/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.3.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.3.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.3.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.3.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.2/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.3.2/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.3.2/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.2/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.2/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.2/best-practices.md:118:> Recommend: `false`
content/docs/archives/1.3.2/best-practices.md:123:> Recommend: `false`
content/docs/archives/1.3.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.2/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.3.2/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.3.2/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.3.2/deploy/important-notes/index.md:10:1. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18 and <= v1.24) in varchives.
content/docs/archives/1.3.2/deploy/important-notes/index.md:12:3. When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to varchives, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890)
content/docs/archives/1.3.2/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.2/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.2/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.3.2/deploy/install/_index.md:147:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.3.2/deploy/install/_index.md:251:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.3.2/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.2/deploy/upgrade/_index.md:18:- To upgrade from v1.3.x (< varchives) or v1.2.x, see [this section.](./longhorn-manager)
content/docs/archives/1.3.2/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.3.x (< varchives) or v1.2.x
content/docs/archives/1.3.2/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.3.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.2/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.3.x (< varchives) or v1.2.x to varchives.
content/docs/archives/1.3.2/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/archives/1.3.2/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.3.2/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.3.2/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.3.2/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.3.2/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.3.2/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.3.2/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/archives/1.3.2/references/examples.md:252:          '<BACKUP_URL>'
content/docs/archives/1.3.2/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.3.2/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.3.2/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.3.2/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.2/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.2/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.2/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.2/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.2/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.2/references/settings.md:69:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:77:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:83:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:94:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:100:> Default: `0`
content/docs/archives/1.3.2/references/settings.md:108:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:122:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.3.2/references/settings.md:128:> Default: `disabled`
content/docs/archives/1.3.2/references/settings.md:143:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.2/references/settings.md:161:> Default: `longhorn-static`
content/docs/archives/1.3.2/references/settings.md:167:> Default: `3`
content/docs/archives/1.3.2/references/settings.md:179:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:185:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:197:> Default: `do-nothing`
content/docs/archives/1.3.2/references/settings.md:212:> Default: `600`
content/docs/archives/1.3.2/references/settings.md:220:> Default: `if-not-present`
content/docs/archives/1.3.2/references/settings.md:236:> Default: `60`
content/docs/archives/1.3.2/references/settings.md:241:> Default: `300`
content/docs/archives/1.3.2/references/settings.md:244:> **Note:**
content/docs/archives/1.3.2/references/settings.md:245:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.2/references/settings.md:246:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.2/references/settings.md:251:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:259:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:267:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.2/references/settings.md:273:> Example: `s3-secret`
content/docs/archives/1.3.2/references/settings.md:279:> Default: `300`
content/docs/archives/1.3.2/references/settings.md:287:> Default: `1440`
content/docs/archives/1.3.2/references/settings.md:297:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:301:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.2/references/settings.md:305:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:313:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:321:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:327:> **Note:**
content/docs/archives/1.3.2/references/settings.md:328:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.2/references/settings.md:329:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.2/references/settings.md:333:> Default: `disabled`
content/docs/archives/1.3.2/references/settings.md:349:> Default: `ignored`
content/docs/archives/1.3.2/references/settings.md:363:> Default: `25`
content/docs/archives/1.3.2/references/settings.md:371:> Default: `200`
content/docs/archives/1.3.2/references/settings.md:383:> Default: `5`
content/docs/archives/1.3.2/references/settings.md:389:> **WARNING:**
content/docs/archives/1.3.2/references/settings.md:390:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.2/references/settings.md:391:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.2/references/settings.md:392:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.2/references/settings.md:396:> Default: `12`
content/docs/archives/1.3.2/references/settings.md:408:> **Warning:**
content/docs/archives/1.3.2/references/settings.md:409:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.2/references/settings.md:410:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.2/references/settings.md:411:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.2/references/settings.md:412:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.2/references/settings.md:413:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.2/references/settings.md:417:> Default: `12`
content/docs/archives/1.3.2/references/settings.md:429:> **Warning:**
content/docs/archives/1.3.2/references/settings.md:430:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.2/references/settings.md:431:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.2/references/settings.md:432:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.2/references/settings.md:433:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.2/references/settings.md:434:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.2/references/settings.md:439:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.2/references/settings.md:458:> Example: `high-priority`
content/docs/archives/1.3.2/references/settings.md:467:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.2/references/settings.md:473:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.2/references/settings.md:480:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.2/references/settings.md:486:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:492:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.2/references/settings.md:496:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.2/references/settings.md:500:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.2/references/settings.md:509:> Default:
content/docs/archives/1.3.2/references/settings.md:515:> Default: `false`
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.2/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.2/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.2/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:93:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:98:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:103:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.3.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.2/volumes-and-nodes/create-volumes.md:151:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/archives/1.3.2/volumes-and-nodes/create-volumes.md:155:Labels:          <none>
content/docs/archives/1.3.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.2/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.3.2/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.3.2/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.3.2/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.3.2/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.3.2/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.3.2/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.3.2/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.3.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.3.2/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.2/volumes-and-nodes/maintenance.md:82:        kubectl delete node <node-name>
content/docs/archives/1.3.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.2/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.3.2/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:43:    > Users can see the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn does not support TRIM/UNMAP operations, so the `discard` mount option or `fstrim` in the filesystem layer cannot reclaim the unused blocks. In consequence, the actual size of Longhorn volumes cannot be shrunk in this case.
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:50:     >
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:71:    >
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/1.3.3/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/archives/1.3.3/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/archives/1.3.3/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.3.3/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/archives/1.3.3/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.3/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.3.3/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.3.3/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.3.3/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.3.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `varchives`
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.3.3/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.3.3/advanced-resources/deploy/customizing-default-settings.md:24:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.3.3/advanced-resources/deploy/customizing-default-settings.md:173:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.3.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.3/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.3.3/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.3/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/archives/1.3.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.3/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.3/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/archives/1.3.3/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/archives/1.3.3/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/archives/1.3.3/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/archives/1.3.3/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/archives/1.3.3/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.3.3/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.3.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.3.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.3.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.3.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.3/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.3.3/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.3.3/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.3/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.3/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.3/best-practices.md:118:> Recommend: `false`
content/docs/archives/1.3.3/best-practices.md:123:> Recommend: `false`
content/docs/archives/1.3.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.3/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.3.3/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.3.3/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.3.3/deploy/important-notes/index.md:11:2. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18 and <= v1.24) in varchives.
content/docs/archives/1.3.3/deploy/important-notes/index.md:13:3. When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to varchives, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890)
content/docs/archives/1.3.3/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.3/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.3/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.3.3/deploy/install/_index.md:147:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.3.3/deploy/install/_index.md:251:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.3.3/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.3/deploy/upgrade/_index.md:18:- To upgrade from v1.3.x (< varchives) or v1.2.x, see [this section.](./longhorn-manager)
content/docs/archives/1.3.3/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.3.x (< varchives) or v1.2.x
content/docs/archives/1.3.3/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.3.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.3/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.3.x (< varchives) or v1.2.x to varchives.
content/docs/archives/1.3.3/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/archives/1.3.3/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.3.3/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.3.3/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.3.3/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.3.3/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.3.3/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.3.3/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/archives/1.3.3/references/examples.md:252:          '<BACKUP_URL>'
content/docs/archives/1.3.3/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.3.3/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.3.3/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.3.3/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.3/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.3/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.3/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.3/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.3/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.3/references/settings.md:70:> Default: `block-if-contains-last-replica`
content/docs/archives/1.3.3/references/settings.md:81:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:87:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:98:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:104:> Default: `0`
content/docs/archives/1.3.3/references/settings.md:112:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:126:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.3.3/references/settings.md:132:> Default: `disabled`
content/docs/archives/1.3.3/references/settings.md:147:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.3/references/settings.md:165:> Default: `longhorn-static`
content/docs/archives/1.3.3/references/settings.md:171:> Default: `3`
content/docs/archives/1.3.3/references/settings.md:183:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:189:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:201:> Default: `do-nothing`
content/docs/archives/1.3.3/references/settings.md:216:> Default: `600`
content/docs/archives/1.3.3/references/settings.md:224:> Default: `if-not-present`
content/docs/archives/1.3.3/references/settings.md:240:> Default: `60`
content/docs/archives/1.3.3/references/settings.md:245:> Default: `300`
content/docs/archives/1.3.3/references/settings.md:248:> **Note:**
content/docs/archives/1.3.3/references/settings.md:249:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.3/references/settings.md:250:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.3/references/settings.md:255:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:263:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:271:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.3/references/settings.md:277:> Example: `s3-secret`
content/docs/archives/1.3.3/references/settings.md:283:> Default: `300`
content/docs/archives/1.3.3/references/settings.md:291:> Default: `1440`
content/docs/archives/1.3.3/references/settings.md:301:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:305:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.3/references/settings.md:309:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:317:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:325:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:331:> **Note:**
content/docs/archives/1.3.3/references/settings.md:332:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.3/references/settings.md:333:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.3/references/settings.md:337:> Default: `disabled`
content/docs/archives/1.3.3/references/settings.md:353:> Default: `ignored`
content/docs/archives/1.3.3/references/settings.md:367:> Default: `25`
content/docs/archives/1.3.3/references/settings.md:375:> Default: `200`
content/docs/archives/1.3.3/references/settings.md:387:> Default: `5`
content/docs/archives/1.3.3/references/settings.md:393:> **WARNING:**
content/docs/archives/1.3.3/references/settings.md:394:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.3/references/settings.md:395:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.3/references/settings.md:396:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.3/references/settings.md:400:> Default: `12`
content/docs/archives/1.3.3/references/settings.md:412:> **Warning:**
content/docs/archives/1.3.3/references/settings.md:413:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.3/references/settings.md:414:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.3/references/settings.md:415:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.3/references/settings.md:416:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.3/references/settings.md:417:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.3/references/settings.md:421:> Default: `12`
content/docs/archives/1.3.3/references/settings.md:433:> **Warning:**
content/docs/archives/1.3.3/references/settings.md:434:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.3/references/settings.md:435:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.3/references/settings.md:436:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.3/references/settings.md:437:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.3/references/settings.md:438:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.3/references/settings.md:443:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.3/references/settings.md:462:> Example: `high-priority`
content/docs/archives/1.3.3/references/settings.md:471:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.3/references/settings.md:477:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.3/references/settings.md:484:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.3/references/settings.md:490:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:496:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.3/references/settings.md:500:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.3/references/settings.md:504:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.3/references/settings.md:513:> Default:
content/docs/archives/1.3.3/references/settings.md:519:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:525:> Default: `false`
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.3/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.3/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.3/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:73:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:75:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    >
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:77:    > Using a volume with 2 recurring jobs as an example:
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:78:    > - `snapshot` with retain value set to 5
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > - `snapshot-delete`: with retain value set to 2
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:107:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:112:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:117:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.3.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.3/volumes-and-nodes/create-volumes.md:151:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/archives/1.3.3/volumes-and-nodes/create-volumes.md:155:Labels:          <none>
content/docs/archives/1.3.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.3/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.3.3/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.3.3/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.3.3/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.3.3/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.3.3/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.3.3/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.3.3/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.3.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.3.3/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:24:   > **Note:**
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:25:   > By default, if there is one last healthy replica for a volume on
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:26:   > the node, Longhorn will prevent the node from completing the drain
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:27:   > operation, to protect the last replica and prevent the disruption of the
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:28:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:29:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:32:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:33:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:34:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:35:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:39:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:40:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:41:   > can be reused later, once the node back online and uncordoned.
content/docs/archives/1.3.3/volumes-and-nodes/maintenance.md:74:        kubectl delete node <node-name>
content/docs/archives/1.3.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.3/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.3.3/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:43:    > Users can see the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn does not support TRIM/UNMAP operations, so the `discard` mount option or `fstrim` in the filesystem layer cannot reclaim the unused blocks. In consequence, the actual size of Longhorn volumes cannot be shrunk in this case.
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:50:     >
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:71:    >
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/1.3.4/advanced-resources/backing-image.md:22:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/archives/1.3.4/advanced-resources/backing-image.md:29:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/archives/1.3.4/advanced-resources/backing-image.md:97:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/archives/1.3.4/advanced-resources/backing-image.md:118:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/archives/1.3.4/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.4/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/archives/1.3.4/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/archives/1.3.4/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:varchives launch-simple-longhorn <volume_name> <volume_size>
content/docs/archives/1.3.4/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/archives/1.3.4/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `varchives`
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/archives/1.3.4/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/varchives/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:164:            - name: <SECRET_NAME>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:173:In varchives, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:300:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:316:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:327:    value: <SECRET_NAME>
content/docs/archives/1.3.4/advanced-resources/deploy/customizing-default-settings.md:24:From the project view in Rancher, go to **Apps > Launch > Longhorn** and edit the settings before launching the app.
content/docs/archives/1.3.4/advanced-resources/deploy/customizing-default-settings.md:173:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:39:   >
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:81:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/archives/1.3.4/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.4/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.4/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.4/advanced-resources/deploy/priority-class.md:23:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/archives/1.3.4/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.4/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/archives/1.3.4/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.4/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:99:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.4/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.4/advanced-resources/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/archives/1.3.4/advanced-resources/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/archives/1.3.4/advanced-resources/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/archives/1.3.4/advanced-resources/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/archives/1.3.4/advanced-resources/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/archives/1.3.4/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/archives/1.3.4/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/archives/1.3.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/archives/1.3.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/archives/1.3.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/archives/1.3.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.4/advanced-resources/rwx-workloads.md:12:For each actively in use RWX volume Longhorn will create a `share-manager-<volume-name>` Pod in the `longhorn-system` namespace.
content/docs/archives/1.3.4/advanced-resources/rwx-workloads.md:24:for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n
content/docs/archives/1.3.4/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.4/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.4/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.4/best-practices.md:118:> Recommend: `false`
content/docs/archives/1.3.4/best-practices.md:123:> Recommend: `false`
content/docs/archives/1.3.4/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.4/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/archives/1.3.4/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/archives/1.3.4/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/archives/1.3.4/deploy/important-notes/index.md:11:2. Please ensure your Kubernetes cluster is at least v1.18 and at most v1.24 before upgrading to Longhorn varchives because the supported Kubernetes version has been updated (>= v1.18 and <= v1.24) in varchives.
content/docs/archives/1.3.4/deploy/important-notes/index.md:13:3. When upgrading from a Longhorn version >= 1.2.0 and <= v1.2.3 to varchives, if your cluster has many backups, you may expect to have a long upgrade time (with 22000 backups, it could take a few hours). Helm upgrade may timeout and you may need to re-run the upgrade command or set a longer timeout. This is [a known issue](https://github.com/longhorn/longhorn/issues/3890)
content/docs/archives/1.3.4/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.4/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.4/deploy/install/_index.md:27:-  Kubernetes >= v1.18 and <= v1.24
content/docs/archives/1.3.4/deploy/install/_index.md:149:echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/archives/1.3.4/deploy/install/_index.md:253:The `Server Version` should be >= v1.18 and <= v1.24.
content/docs/archives/1.3.4/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/varchives/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.4/deploy/upgrade/_index.md:18:- To upgrade from v1.3.x (< varchives) or v1.2.x, see [this section.](./longhorn-manager)
content/docs/archives/1.3.4/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.3.x (< varchives) or v1.2.x
content/docs/archives/1.3.4/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/archives/1.3.4/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.4/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.3.x (< varchives) or v1.2.x to varchives.
content/docs/archives/1.3.4/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:94:      - to: <the email address to send notifications to>
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:95:        from: <the sender address>
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:96:        smarthost: <the SMTP host through which emails are sent>
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:98:        auth_username: <the username>
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:99:        auth_identity: <the identity>
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:100:        auth_password: <the password>
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:112:      - api_url: <the Slack webhook URL>
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:113:        channel: <the channel or user to send notifications to>
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:126:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:154:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:178:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:280:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:282:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:312:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:313:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:384:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/archives/1.3.4/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/archives/1.3.4/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/archives/1.3.4/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/archives/1.3.4/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/archives/1.3.4/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/archives/1.3.4/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/archives/1.3.4/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/archives/1.3.4/references/examples.md:252:          '<BACKUP_URL>'
content/docs/archives/1.3.4/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/archives/1.3.4/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/archives/1.3.4/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/archives/1.3.4/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.4/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.4/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/archives/1.3.4/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.4/references/networking.md:71:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.4/references/networking.md:83:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/archives/1.3.4/references/settings.md:70:> Default: `block-if-contains-last-replica`
content/docs/archives/1.3.4/references/settings.md:81:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:87:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:98:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:104:> Default: `0`
content/docs/archives/1.3.4/references/settings.md:112:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:126:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.3.4/references/settings.md:132:> Default: `disabled`
content/docs/archives/1.3.4/references/settings.md:147:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.4/references/settings.md:165:> Default: `longhorn-static`
content/docs/archives/1.3.4/references/settings.md:171:> Default: `3`
content/docs/archives/1.3.4/references/settings.md:183:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:189:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:201:> Default: `do-nothing`
content/docs/archives/1.3.4/references/settings.md:216:> Default: `600`
content/docs/archives/1.3.4/references/settings.md:224:> Default: `if-not-present`
content/docs/archives/1.3.4/references/settings.md:240:> Default: `60`
content/docs/archives/1.3.4/references/settings.md:245:> Default: `300`
content/docs/archives/1.3.4/references/settings.md:248:> **Note:**
content/docs/archives/1.3.4/references/settings.md:249:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.4/references/settings.md:250:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.4/references/settings.md:255:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:263:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:271:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.4/references/settings.md:277:> Example: `s3-secret`
content/docs/archives/1.3.4/references/settings.md:283:> Default: `300`
content/docs/archives/1.3.4/references/settings.md:291:> Default: `1440`
content/docs/archives/1.3.4/references/settings.md:301:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:305:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.4/references/settings.md:309:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:317:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:325:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:331:> **Note:**
content/docs/archives/1.3.4/references/settings.md:332:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.4/references/settings.md:333:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.4/references/settings.md:337:> Default: `disabled`
content/docs/archives/1.3.4/references/settings.md:353:> Default: `ignored`
content/docs/archives/1.3.4/references/settings.md:367:> Default: `25`
content/docs/archives/1.3.4/references/settings.md:375:> Default: `200`
content/docs/archives/1.3.4/references/settings.md:387:> Default: `5`
content/docs/archives/1.3.4/references/settings.md:393:> **WARNING:**
content/docs/archives/1.3.4/references/settings.md:394:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.4/references/settings.md:395:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.4/references/settings.md:396:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.4/references/settings.md:400:> Default: `12`
content/docs/archives/1.3.4/references/settings.md:412:> **Warning:**
content/docs/archives/1.3.4/references/settings.md:413:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.4/references/settings.md:414:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.4/references/settings.md:415:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.4/references/settings.md:416:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.4/references/settings.md:417:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.4/references/settings.md:421:> Default: `12`
content/docs/archives/1.3.4/references/settings.md:433:> **Warning:**
content/docs/archives/1.3.4/references/settings.md:434:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.4/references/settings.md:435:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.4/references/settings.md:436:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.4/references/settings.md:437:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.4/references/settings.md:438:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.4/references/settings.md:443:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.4/references/settings.md:462:> Example: `high-priority`
content/docs/archives/1.3.4/references/settings.md:471:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.4/references/settings.md:477:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.4/references/settings.md:484:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.4/references/settings.md:490:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:496:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.4/references/settings.md:500:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.4/references/settings.md:504:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.4/references/settings.md:513:> Default:
content/docs/archives/1.3.4/references/settings.md:519:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:525:> Default: `false`
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:45:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:46:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:56:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:57:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:58:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:82:               "arn:aws:s3:::<your-bucket-name>",
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:83:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:106:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:117:       kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:118:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:125:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:131:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:134:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:151:kubectl create secret generic <aws-secret> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:152:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:153:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:154:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:155:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:203:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.4/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.4/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.4/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:73:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:75:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    >
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:77:    > Using a volume with 2 recurring jobs as an example:
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:78:    > - `snapshot` with retain value set to 5
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > - `snapshot-delete`: with retain value set to 2
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:107:Label recurring job group with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled`
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:112:Label recurring job with `kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled`
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:117:Remove recurring job labels with `kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-`
content/docs/archives/1.3.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.4/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.4/volumes-and-nodes/create-volumes.md:151:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/archives/1.3.4/volumes-and-nodes/create-volumes.md:155:Labels:          <none>
content/docs/archives/1.3.4/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.4/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.4/volumes-and-nodes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/archives/1.3.4/volumes-and-nodes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/archives/1.3.4/volumes-and-nodes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/archives/1.3.4/volumes-and-nodes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/archives/1.3.4/volumes-and-nodes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/archives/1.3.4/volumes-and-nodes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/archives/1.3.4/volumes-and-nodes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/archives/1.3.4/volumes-and-nodes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/archives/1.3.4/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.4/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:98:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:99:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:100:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:101:    resize2fs /dev/longhorn/<volume name>
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:102:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:108:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:109:    xfs_growfs <the mount directory>
content/docs/archives/1.3.4/volumes-and-nodes/expansion.md:110:    umount /dev/longhorn/<volume name>
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:24:   > **Note:**
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:25:   > By default, if there is one last healthy replica for a volume on
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:26:   > the node, Longhorn will prevent the node from completing the drain
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:27:   > operation, to protect the last replica and prevent the disruption of the
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:28:   > workload. You can control this behavior in the setting [Node Drain Policy](../../references/settings#node-drain-policy), or [evict
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:29:   > the replica to other nodes before draining](../disks-or-nodes-eviction).
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:32:   > **Note:** For volumes that are not attached through the CSI flow on the node (for example, manually attached using UI),
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:33:   > they will not be automatically attached to new nodes by Kubernetes during the draining.
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:34:   > Therefore, Longhorn will prevent the node from completing the drain operation.
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:35:   > User would need to handle detachment for these volumes to unblock the draining.
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:39:   > **Note:** Normally you don't need to evict the replicas before the drain
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:40:   > operation, as long as you have healthy replicas on other nodes. The replicas
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:41:   > can be reused later, once the node back online and uncordoned.
content/docs/archives/1.3.4/volumes-and-nodes/maintenance.md:74:        kubectl delete node <node-name>
content/docs/archives/1.3.4/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.4/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.4/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.4/volumes-and-nodes/scheduling.md:48:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/archives/1.3.4/volumes-and-nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:43:    > Users can see the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn does not support TRIM/UNMAP operations, so the `discard` mount option or `fstrim` in the filesystem layer cannot reclaim the unused blocks. In consequence, the actual size of Longhorn volumes cannot be shrunk in this case.
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:49:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:50:     >
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:51:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:70:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:71:    >
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:72:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/archives/_index.md:9:<!--{{< version-list >}}-->
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:46:> **IMPORTANT:**
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:47:> - `name`: Use the same name for the backing image and its backup. If the names are not identical, Longhorn will not be able to find the backing image.
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:48:> - `userCreated`: Set the value to `true` to indicate that you created the backup custom resource, which enabled the creation of the backup in the backupstore. The value `false` indicates that the backup custom resource was synced from the backupstore.
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:49:> - `labels`: You can add labels to the backing image backup.
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:71:> **IMPORTANT:**
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:72:> - `sourceType`: Set the value to `restore`.
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:73:> - `sourceParameters`: Configure the following parameters:
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:74:>   - `backup-url`: URL of the backing image resource in the backupstore. You can find this information in the status of the backup custom resource `.Status.URL`.
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:75:>   - `concurrent-limit`: Maximum number of worker threads that can concurrently run for each restore operation. When unspecified, Longhorn uses the default value.
content/docs/1.6.1/advanced-resources/backing-image/backing-image-backup.md:76:> - `checksum`: You can specify the expected SHA-512 checksum of the backing image file, which Longhorn uses to validate the restored file. When unspecified, Longhorn uses the checksum of the restored file as the truth.
content/docs/1.6.1/advanced-resources/backing-image/backing-image.md:23:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.6.1/advanced-resources/backing-image/backing-image.md:30:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.6.1/advanced-resources/backing-image/backing-image.md:115:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.6.1/advanced-resources/backing-image/backing-image.md:136:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.6.1/advanced-resources/data-cleanup/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.6.1/advanced-resources/data-cleanup/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.6.1/advanced-resources/data-cleanup/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.6.1/advanced-resources/data-cleanup/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.6.1/advanced-resources/data-cleanup/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.6.1/advanced-resources/data-integrity/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.6.1/advanced-resources/data-integrity/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.6.1/advanced-resources/data-integrity/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.6.1/advanced-resources/data-integrity/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.6.1/advanced-resources/data-integrity/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.6.1/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.6.1/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.6.1/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.6.1 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.6.1/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.6.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.6.1`
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.6.1/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:99:> **NOTE:**
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:100:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:170:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:176:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:181:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:190:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:193:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.6.1/advanced-resources/deploy/customizing-default-settings.md:197:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:39:   >
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:48:> **Warning**:
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:49:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:50:> * When all Longhorn volumes are detached, the customized settings are immediately applied to the system-managed components (for example, Instance manager, CSI driver and Engine images).
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:51:> * When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.6.1/advanced-resources/deploy/node-selector.md:77:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.6.1/advanced-resources/deploy/priority-class.md:21:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.6.1/advanced-resources/deploy/priority-class.md:22:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.6.1/advanced-resources/deploy/priority-class.md:23:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.6.1/advanced-resources/deploy/priority-class.md:27:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.6.1/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.6.1/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.6.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.6.1/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:15:> **Important**: To ensure that your preferred settings are immediately applied, stop all workloads and detach all Longhorn volumes before configuring the settings.
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:17:> When all volumes are detached, Longhorn attempts to restart all Instance Manager and Backing Image Manager pods to apply the setting.
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:18:> When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:31:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:32:>
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:33:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:34:>
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:35:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:41:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:42:>
content/docs/1.6.1/advanced-resources/deploy/storage-network.md:43:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:59:> To ensure that your preferred toleration settings are immediately applied, stop all workloads and detach all Longhorn volumes before configuring the settings.
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:62:>
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:63:> When all Longhorn volumes are detached, the customized setting is immediately applied to the system-managed components.
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:64:> When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:65:>
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:66:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.6.1/advanced-resources/deploy/taint-toleration.md:101:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.6.1/advanced-resources/driver-migration/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.6.1/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.6.1/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.6.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.6.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.6.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.6.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.6.1/advanced-resources/os-distro-specific/okd-support.md:62:cat <<EOF >>auto-mount-machineconfig.yaml
content/docs/1.6.1/advanced-resources/rebuilding/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.6.1/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.6.1/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.6.1/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.6.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.6.1/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.6.1/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.6.1/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.6.1/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:56:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:60:  > **Note:** Longhorn system restores volume with the latest backup. We recommend updating the last backup for all volumes. By taking volume backups, you ensure that the data is up-to-date with the system backup. For more information, please refer to the [Configuration - Volume Backup Policy](#volume-backup-policy) section.
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:92:   > kubectl -n longhorn-system get systembackup
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:106:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:112:   > kubectl -n longhorn-system get systembackup
content/docs/1.6.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:116:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.6.1/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.6.1/best-practices.md:52:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.6.1/best-practices.md:176:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.6.1/best-practices.md:192:> Recommend: `false`
content/docs/1.6.1/best-practices.md:198:> Recommend: `false`
content/docs/1.6.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.6.1/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.6.1/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.6.1/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.6.1/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.6.1/deploy/important-notes/index.md:81:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.6.1/deploy/important-notes/index.md:84:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.6.1/deploy/important-notes/index.md:120:> **Warning:**
content/docs/1.6.1/deploy/important-notes/index.md:121:> Whenever engine upgrade enforcement causes upgrade failure, Longhorn allows you to revert to the previous version because Longhorn Manager will block the entire upgrade. However, Longhorn prohibits downgrading when an upgrade is successful. For more information, see [Upgrade Path Enforcement](../../deploy/upgrade/#upgrade-path-enforcement-and-downgrade-prevention).
content/docs/1.6.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.6.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.6.1/deploy/install/_index.md:30:-  Kubernetes >= v1.21
content/docs/1.6.1/deploy/install/_index.md:157:  echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.6.1/deploy/install/_index.md:266:The `Server Version` should be >= v1.21.
content/docs/1.6.1/deploy/install/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.6.1/deploy/install/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.6.1/deploy/install/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.6.1/deploy/install/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.6.1/deploy/install/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.6.1/deploy/install/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.6.1/deploy/install/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.6.1/deploy/install/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.6.1/deploy/install/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.6.1/deploy/install/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.6.1/deploy/install/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.6.1/deploy/install/airgap.md:173:In v1.6.1, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.6.1/deploy/install/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.6.1/deploy/install/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.6.1/deploy/install/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.6.1/deploy/install/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.6.1/deploy/install/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.6.1/deploy/install/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.6.1/deploy/install/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.6.1/deploy/install/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.6.1/deploy/install/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.6.1/deploy/install/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.6.1/deploy/install/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.6.1/deploy/install/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.6.1/deploy/install/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.6.1/deploy/install/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.6.1/deploy/install/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.6.1/deploy/install/install-with-argocd.md:18:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.6.1/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.1/deploy/install/install-with-argocd.md:37:    cat > longhorn-application.yaml <<EOF
content/docs/1.6.1/deploy/install/install-with-fleet.md:19:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.6.1/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.1/deploy/install/install-with-fleet.md:94:    cat > longhorn-gitrepo.yaml << "EOF"
content/docs/1.6.1/deploy/install/install-with-flux.md:15:    export GITHUB_TOKEN=<gh-token>
content/docs/1.6.1/deploy/install/install-with-flux.md:18:      --owner=<github_username> \
content/docs/1.6.1/deploy/install/install-with-flux.md:19:      --repository=<github_repo_name> \
content/docs/1.6.1/deploy/install/install-with-flux.md:20:      --branch=<branch_name> \
content/docs/1.6.1/deploy/install/install-with-flux.md:21:      --path=<folder_path_within_repo> \
content/docs/1.6.1/deploy/install/install-with-flux.md:25:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.6.1/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.1/deploy/install/install-with-flux.md:36:      --export > helmrepo.yaml
content/docs/1.6.1/deploy/install/install-with-flux.md:47:      --export > helmrelease.yaml
content/docs/1.6.1/deploy/install/install-with-flux.md:97:    git push origin <branch_name>
content/docs/1.6.1/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.6.1/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.6.1/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.6.1/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.6.1/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.6.1/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.6.1/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.6.1/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.6.1/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.6.1/deploy/uninstall/_index.md:69:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.6.1/deploy/uninstall/_index.md:88:      > helm list -n longhorn-system -a
content/docs/1.6.1/deploy/uninstall/_index.md:95:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.6.1/deploy/upgrade/_index.md:18:> **Warning**:
content/docs/1.6.1/deploy/upgrade/_index.md:19:> - Once you successfully upgrade to v1.6.0, you will not be allowed to revert to the previously installed version.
content/docs/1.6.1/deploy/upgrade/_index.md:20:> - The Downgrade Prevention feature was introduced in v1.5.0 so Longhorn is unable to prevent downgrade attempts in older versions.
content/docs/1.6.1/deploy/upgrade/_index.md:43:- To upgrade from v1.6.x (< v1.6.1) or v1.5.x, see [this section.](./longhorn-manager)
content/docs/1.6.1/deploy/upgrade/longhorn-manager.md:6:### Upgrading from v1.6.x (< v1.6.1) or v1.5.x
content/docs/1.6.1/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.6.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.6.1/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.6.1/deploy/upgrade/upgrade-engine.md:24:Live upgrade is supported for upgrading from v1.6.x (< v1.6.1) or v1.5.x.
content/docs/1.6.1/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.6.1/maintenance/maintenance.md:34:   > **Note:** Volumes that are not attached through the CSI flow on the node (for example, manually attached using
content/docs/1.6.1/maintenance/maintenance.md:35:   > UI) will not be automatically attached to new nodes by Kubernetes during the draining. Therefore, Longhorn will
content/docs/1.6.1/maintenance/maintenance.md:36:   > prevent the node from completing the drain operation. The user will need to detach these volumes manually to
content/docs/1.6.1/maintenance/maintenance.md:37:   > unblock the draining.
content/docs/1.6.1/maintenance/maintenance.md:42:   > **Note:** By default, if there is one last healthy replica for a volume on the node, Longhorn will prevent the node
content/docs/1.6.1/maintenance/maintenance.md:43:   > from completing the drain operation, to protect the last replica and prevent the disruption of the workload. You
content/docs/1.6.1/maintenance/maintenance.md:44:   > can control this behavior with the setting [Node Drain Policy](../../references/settings#node-drain-policy), or
content/docs/1.6.1/maintenance/maintenance.md:45:   > [evict the replica to other nodes before draining](../../nodes-and-volumes/nodes/disks-or-nodes-eviction). See [Node Drain Policy
content/docs/1.6.1/maintenance/maintenance.md:46:   > Recommendations](#node-drain-policy-recommendations) for considerations when selecting a policy.
content/docs/1.6.1/maintenance/maintenance.md:54:   > **Note:** Normally you don't need to evict the replicas before the drain operation, as long as you have healthy
content/docs/1.6.1/maintenance/maintenance.md:55:   > replicas on other nodes. The replicas can be reused later, once the node back online and uncordoned. See [Node
content/docs/1.6.1/maintenance/maintenance.md:56:   > Drain Policy](#node-drain-policy-recommendations) for further guidance.
content/docs/1.6.1/maintenance/maintenance.md:94:      kubectl delete node <node-name>
content/docs/1.6.1/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.6.1/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.6.1/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.6.1/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.6.1/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.6.1/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:113:      - to: <the email address to send notifications to>
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:114:        from: <the sender address>
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:115:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:117:        auth_username: <the username>
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:118:        auth_identity: <the identity>
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:119:        auth_password: <the password>
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:131:      - api_url: <the Slack webhook URL>
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:132:        channel: <the channel or user to send notifications to>
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:145:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:173:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:197:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:299:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:301:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:331:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:332:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.6.1/monitoring/prometheus-and-grafana-setup.md:403:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.6.1/nodes-and-volumes/nodes/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.6.1/nodes-and-volumes/nodes/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.6.1/nodes-and-volumes/nodes/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.6.1/nodes-and-volumes/nodes/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.6.1/nodes-and-volumes/nodes/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.6.1/nodes-and-volumes/nodes/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.6.1/nodes-and-volumes/nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.6.1/nodes-and-volumes/nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.6.1/nodes-and-volumes/nodes/multidisk.md:24:> **Note**:
content/docs/1.6.1/nodes-and-volumes/nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.6.1/nodes-and-volumes/nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.6.1/nodes-and-volumes/nodes/scheduling.md:53:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.6.1/nodes-and-volumes/nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.6.1/nodes-and-volumes/volumes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.6.1/nodes-and-volumes/volumes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/create-volumes.md:159:Labels:          <none>
content/docs/1.6.1/nodes-and-volumes/volumes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.6.1/nodes-and-volumes/volumes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.6.1/nodes-and-volumes/volumes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:123:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:131:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/expansion.md:133:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:24:      > ```
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:26:      > ```
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:36:> **Notice**  
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:37:> An RWX volume must have the access mode set to `ReadWriteMany` and the "migratable" flag disabled (*parameters.migratable: `false`*).
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:72:> **Important:**
content/docs/1.6.1/nodes-and-volumes/volumes/rwx-volumes.md:73:> To create PVCs for RWX volumes using the sample StorageClass, replace the `nfsOptions` string with a customized comma-separated list of legal options.
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:8:> **Note:**
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:10:>
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:31:When using `fstrim`, you must identify the mount point of the volume and then run the command `fstrim <the mount point>`.
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:34:- RWX volume: The mount point is the share manager pod of the volume. The share manager pod contains the NFS server and is typically named `share-manager-<volume name>`.
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:39:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:41:1. Identify the work directory of the NFS server (for example, `/export/<volume name>`).
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:43:    mount | grep <volume name>
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:44:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:48:    fstrim /export/<volume name>
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:80:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.6.1/nodes-and-volumes/volumes/trim-filesystem.md:82:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:44:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:50:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:51:     >
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:52:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:71:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:72:    >
content/docs/1.6.1/nodes-and-volumes/volumes/volume-size.md:73:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.6.1/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.6.1/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.6.1/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.6.1/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.6.1/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.6.1/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.6.1/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.6.1/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.6.1/references/helm-values.md:243:| defaultSettings.replicaZoneSoftAntiAffinity | Setting that allows Longhorn to schedule new replicas of a volume to nodes in the same zone as existing healthy replicas. Nodes that do not belong to any zone are treated as existing in the zone that contains healthy replicas. When identifying zones, Longhorn relies on the label "topology.kubernetes.io/zone=<Zone name of the node>" in the Kubernetes node object. |
content/docs/1.6.1/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.6.1/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.6.1/references/networking.md:76:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.6.1/references/networking.md:88:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.6.1/references/settings.md:94:> Default: `block-if-contains-last-replica`
content/docs/1.6.1/references/settings.md:118:> Default: `false`
content/docs/1.6.1/references/settings.md:125:> Default: `true`
content/docs/1.6.1/references/settings.md:131:> Default: `true`
content/docs/1.6.1/references/settings.md:139:> Default: `true`
content/docs/1.6.1/references/settings.md:146:> **Note:** This setting doesn't apply to below cases.
content/docs/1.6.1/references/settings.md:147:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.6.1/references/settings.md:148:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../nodes-and-volumes/volumes/rwx-volumes).
content/docs/1.6.1/references/settings.md:152:> Default: `true`
content/docs/1.6.1/references/settings.md:158:> Default: `0`
content/docs/1.6.1/references/settings.md:166:> Default: `5`
content/docs/1.6.1/references/settings.md:176:> Default: `false`
content/docs/1.6.1/references/settings.md:186:> Default: `longhorn.io/v1beta2`
content/docs/1.6.1/references/settings.md:192:> Default: `disabled`
content/docs/1.6.1/references/settings.md:210:> Default: `/var/lib/longhorn/`
content/docs/1.6.1/references/settings.md:224:> Default: `longhorn-static`
content/docs/1.6.1/references/settings.md:230:> Default: `3`
content/docs/1.6.1/references/settings.md:241:> Default: `false`
content/docs/1.6.1/references/settings.md:245:> Default: `false`
content/docs/1.6.1/references/settings.md:251:> Default: `true`
content/docs/1.6.1/references/settings.md:259:> Only available if `Upgrade Checker` is enabled.
content/docs/1.6.1/references/settings.md:263:> Default: `true`
content/docs/1.6.1/references/settings.md:271:  > This value may not be accurate for virtual machines.
content/docs/1.6.1/references/settings.md:348:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.6.1/references/settings.md:352:> Default: `do-nothing`
content/docs/1.6.1/references/settings.md:367:> Default: `600`
content/docs/1.6.1/references/settings.md:375:> Default: `if-not-present`
content/docs/1.6.1/references/settings.md:391:> Default: `60`
content/docs/1.6.1/references/settings.md:396:> Default: `300`
content/docs/1.6.1/references/settings.md:400:> **Note:**
content/docs/1.6.1/references/settings.md:401:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.6.1/references/settings.md:402:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.6.1/references/settings.md:405:> Default: `8`
content/docs/1.6.1/references/settings.md:416:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.6.1/references/settings.md:420:> Default: `1`
content/docs/1.6.1/references/settings.md:430:> Default: `false`
content/docs/1.6.1/references/settings.md:436:> Default: `30`
content/docs/1.6.1/references/settings.md:442:> Default: `true`
content/docs/1.6.1/references/settings.md:449:> Default: `false`
content/docs/1.6.1/references/settings.md:453:> **Warning**
content/docs/1.6.1/references/settings.md:454:>
content/docs/1.6.1/references/settings.md:455:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.6.1/references/settings.md:456:>
content/docs/1.6.1/references/settings.md:457:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.6.1/references/settings.md:461:> Default: `2048`
content/docs/1.6.1/references/settings.md:467:> Default: `1250`
content/docs/1.6.1/references/settings.md:471:> **Warning:**
content/docs/1.6.1/references/settings.md:472:>  - Specifying a value of 0 disables CPU requests for instance manager pods. You must specify an integer between 1000 and 8000. 
content/docs/1.6.1/references/settings.md:473:>  - This is a global setting. Modifying the value triggers an automatic restart of the Instance Manager pods. However, V2 Instance Manager pods that use this setting are restarted only when no instances are running.
content/docs/1.6.1/references/settings.md:477:> Default: `enabled`
content/docs/1.6.1/references/settings.md:489:> Default: `fast-check`
content/docs/1.6.1/references/settings.md:498:> Default: `false`
content/docs/1.6.1/references/settings.md:504:> Default: `0 0 */7 * *`
content/docs/1.6.1/references/settings.md:507:> **Warning**
content/docs/1.6.1/references/settings.md:508:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.6.1/references/settings.md:514:> Default: `false`
content/docs/1.6.1/references/settings.md:522:> Default: `false`
content/docs/1.6.1/references/settings.md:526:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.6.1/references/settings.md:530:> Examples:  
content/docs/1.6.1/references/settings.md:531:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.6.1/references/settings.md:532:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.6.1/references/settings.md:533:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.6.1/references/settings.md:539:> Example: `s3-secret`
content/docs/1.6.1/references/settings.md:545:> Default: `300`
content/docs/1.6.1/references/settings.md:553:> Default: `1440`
content/docs/1.6.1/references/settings.md:561:> Default: `1`
content/docs/1.6.1/references/settings.md:570:> Default: `1`
content/docs/1.6.1/references/settings.md:578:> Default: `false`
content/docs/1.6.1/references/settings.md:583:> Default: `ignored`
content/docs/1.6.1/references/settings.md:593:> Default: `lz4`
content/docs/1.6.1/references/settings.md:605:> Default: `2`
content/docs/1.6.1/references/settings.md:611:> Default: `2`
content/docs/1.6.1/references/settings.md:619:> Default: `true`
content/docs/1.6.1/references/settings.md:623:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.6.1/references/settings.md:627:> Default: `true`
content/docs/1.6.1/references/settings.md:635:> Default: `false`
content/docs/1.6.1/references/settings.md:641:> **Note:**
content/docs/1.6.1/references/settings.md:642:>   - This setting is superseded if replicas are forbidden to share a zone by the Replica Zone Level Anti-Affinity setting.
content/docs/1.6.1/references/settings.md:646:> Default: `true`
content/docs/1.6.1/references/settings.md:652:> **Note:**
content/docs/1.6.1/references/settings.md:653:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.6.1/references/settings.md:654:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.6.1/references/settings.md:658:> Default: `true`
content/docs/1.6.1/references/settings.md:664:> **Note:**
content/docs/1.6.1/references/settings.md:665:>   - Even if the setting is "true" and disk sharing is allowed, Longhorn will seek to use a different disk if possible, even if on the same node.
content/docs/1.6.1/references/settings.md:666:>   - This setting is superseded if replicas are forbidden to share a zone or a node by either of the other Soft Anti-Affinity settings.
content/docs/1.6.1/references/settings.md:670:> Default: `disabled`
content/docs/1.6.1/references/settings.md:686:> Default: `ignored`
content/docs/1.6.1/references/settings.md:700:> Default: `25`
content/docs/1.6.1/references/settings.md:708:> Default: `100`
content/docs/1.6.1/references/settings.md:718:> Default: `30`
content/docs/1.6.1/references/settings.md:726:> Default: `true`
content/docs/1.6.1/references/settings.md:732:> Default: `true`
content/docs/1.6.1/references/settings.md:760:> Default: `5`
content/docs/1.6.1/references/settings.md:766:> **WARNING:**
content/docs/1.6.1/references/settings.md:767:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.6.1/references/settings.md:768:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.6.1/references/settings.md:769:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.6.1/references/settings.md:774:> Example: `nodetype=storage:NoSchedule`
content/docs/1.6.1/references/settings.md:793:> Default: `longhorn-critical`
content/docs/1.6.1/references/settings.md:804:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.6.1/references/settings.md:810:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.6.1/references/settings.md:817:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.1/references/settings.md:823:> Default: `false`
content/docs/1.6.1/references/settings.md:829:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.6.1/references/settings.md:833:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.6.1/references/settings.md:837:> **Warning:** This setting should change after all Longhorn volumes are detached because some pods that run Longhorn system components are recreated to apply the setting. When all volumes are detached, Longhorn attempts to restart all Instance Manager and Backing Image Manager pods immediately. When volumes are in use, Longhorn components are not restarted, and you need to reconfigure the settings after detaching the remaining volumes; otherwise, you can wait for the setting change to be reconciled in an hour.
content/docs/1.6.1/references/settings.md:843:> Example: `false`
content/docs/1.6.1/references/settings.md:855:> Default: `12`
content/docs/1.6.1/references/settings.md:867:> **Warning:**
content/docs/1.6.1/references/settings.md:868:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.6.1/references/settings.md:869:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.6.1/references/settings.md:870:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.6.1/references/settings.md:871:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.6.1/references/settings.md:872:>  - After the setting is changed, the V1 Instance Manager pods that use this setting are automatically restarted when no instances are running.
content/docs/1.6.1/references/settings.md:876:> Default: `false`
content/docs/1.6.1/references/storage-class-parameters.md:52:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.6.1/references/storage-class-parameters.md:55:> Default: `true`  
content/docs/1.6.1/references/storage-class-parameters.md:56:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.6.1/references/storage-class-parameters.md:59:> Default: `Delete`  
content/docs/1.6.1/references/storage-class-parameters.md:60:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.6.1/references/storage-class-parameters.md:63:> Default `[]`  
content/docs/1.6.1/references/storage-class-parameters.md:64:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.6.1/references/storage-class-parameters.md:67:> Default `Immediate`  
content/docs/1.6.1/references/storage-class-parameters.md:68:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.6.1/references/storage-class-parameters.md:75:> Default: `3`  
content/docs/1.6.1/references/storage-class-parameters.md:81:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.6.1/references/storage-class-parameters.md:84:> Default: `30`
content/docs/1.6.1/references/storage-class-parameters.md:89:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:90:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.6.1/references/storage-class-parameters.md:95:> Default: `ext4`  
content/docs/1.6.1/references/storage-class-parameters.md:96:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../nodes-and-volumes/volumes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.6.1/references/storage-class-parameters.md:99:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:100:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../nodes-and-volumes/volumes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.6.1/references/storage-class-parameters.md:103:> Default: `false`  
content/docs/1.6.1/references/storage-class-parameters.md:108:> Default: `false`  
content/docs/1.6.1/references/storage-class-parameters.md:109:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.6.1/references/storage-class-parameters.md:112:> Default: `disabled`  
content/docs/1.6.1/references/storage-class-parameters.md:119:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.6.1/references/storage-class-parameters.md:120:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.6.1/references/storage-class-parameters.md:123:> Default: `ignored`  
content/docs/1.6.1/references/storage-class-parameters.md:129:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.6.1/references/storage-class-parameters.md:130:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.6.1/references/storage-class-parameters.md:133:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:134:> Example: `"ssd,fast"`  
content/docs/1.6.1/references/storage-class-parameters.md:137:> More details in [Storage Tags](../../nodes-and-volumes/nodes/storage-tags)
content/docs/1.6.1/references/storage-class-parameters.md:140:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:141:> Example: `"storage,fast"`  
content/docs/1.6.1/references/storage-class-parameters.md:144:> More details in [Storage Tags](../../nodes-and-volumes/nodes/storage-tags)
content/docs/1.6.1/references/storage-class-parameters.md:147:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:148:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.6.1/references/storage-class-parameters.md:151:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.6.1/references/storage-class-parameters.md:154:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:155:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.1/references/storage-class-parameters.md:158:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:159:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.1/references/storage-class-parameters.md:162:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:163:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.1/references/storage-class-parameters.md:166:> Default: `""`  
content/docs/1.6.1/references/storage-class-parameters.md:167:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.1/references/storage-class-parameters.md:170:> Default: `ignored`  
content/docs/1.6.1/references/storage-class-parameters.md:175:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.6.1/references/storage-class-parameters.md:176:> More details in [Trim Filesystem](../../nodes-and-volumes/volumes/trim-filesystem).
content/docs/1.6.1/references/storage-class-parameters.md:179:> Default: `false`  
content/docs/1.6.1/references/storage-class-parameters.md:181:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.6.1/references/storage-class-parameters.md:182:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.6.1/references/storage-class-parameters.md:185:> Default: `ignored`  
content/docs/1.6.1/references/storage-class-parameters.md:190:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.6.1/references/storage-class-parameters.md:191:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.6.1/references/storage-class-parameters.md:194:> Default: `ignored`  
content/docs/1.6.1/references/storage-class-parameters.md:199:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.6.1/references/storage-class-parameters.md:200:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling).
content/docs/1.6.1/references/storage-class-parameters.md:203:> Default: `ignored`  
content/docs/1.6.1/references/storage-class-parameters.md:208:> Global setting: [Replica Disk Level Soft Anti-Affinity](../settings#replica-disk-level-soft-anti-affinity).  
content/docs/1.6.1/references/storage-class-parameters.md:209:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling).
content/docs/1.6.1/references/storage-class-parameters.md:212:> Default: `""`
content/docs/1.6.1/references/storage-class-parameters.md:213:> Example: `"hard,sync"`  
content/docs/1.6.1/references/storage-class-parameters.md:218:> More details in [RWX Workloads](../../nodes-and-volumes/volumes/rwx-volumes#configuring-volume-mount-options)
content/docs/1.6.1/references/storage-class-parameters.md:221:> Default: `"v1"`  
content/docs/1.6.1/references/storage-class-parameters.md:225:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.6.1/references/storage-class-parameters.md:226:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:50:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:61:       kubectl create secret generic <aws-secret> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:62:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:111:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:       kubectl create secret generic <aws-secret> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:123:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:139:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:kubectl create secret generic <aws-secret> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:159:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:160:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:161:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:264:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:334:  cat <<EOF >>cifs_secret.yml
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:349:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:402:    cat <<EOF >>azblob_secret.yml
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:423:    kubectl create secret generic <azblob-secret> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:424:      --from-literal=AZBLOB_ACCOUNT_NAME=<your-azure-storage-account-name> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:425:      --from-literal=AZBLOB_ACCOUNT_KEY=<your-azure-storage-account-key> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:426:      --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:427:      --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:428:      --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:432:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:446:   - If you set `<endpoint-suffix>` in the URL, the default endpoint suffix will be `core.windows.net`.
content/docs/1.6.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:447:   - If you set `AZBLOB_ENDPOINT` in the secret, Longhorn will use `AZBLOB_ENDPOINT` as your storage URL, and `<endpoint-suffix>` will not be used even if it has been set. 
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:52:To see the BackingImage, click **Setting > Backing Image** in the top navigation bar and click the BackingImage mentioned in the `VolumeSnapshotContent.snapshotHandle`.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:108:The parameters can be retrieved from the **Setting > Backing Image** page in the Longhorn UI.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.6.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.6.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.6.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.6.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:82:    > - `snapshot` with retain value set to 5
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:83:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:84:    >
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:85:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:121:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:129:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:137:kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:149:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/source=enabled
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:157:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:165:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:173:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.6.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.6.1/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.6.1/troubleshoot/troubleshooting.md:28:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.6.1/v2-data-engine/prerequisites.md:13:  > **NOTICE**
content/docs/1.6.1/v2-data-engine/prerequisites.md:14:  >
content/docs/1.6.1/v2-data-engine/prerequisites.md:15:  >  AMD64 CPUs require SSE4.2 instruction support.
content/docs/1.6.1/v2-data-engine/prerequisites.md:20:  > **NOTICE**
content/docs/1.6.1/v2-data-engine/prerequisites.md:21:  >
content/docs/1.6.1/v2-data-engine/prerequisites.md:22:  > Host machines with Linux kernel 5.15 may unexpectedly reboot when volume-related IO errors occur. Update the Linux kernel on Longhorn nodes to version 5.19 or later to prevent such issues.
content/docs/1.6.1/v2-data-engine/quick-start.md:75:  echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
content/docs/1.6.1/v2-data-engine/quick-start.md:80:  echo "vm.nr_hugepages=1024" >> /etc/sysctl.conf
content/docs/1.6.1/v2-data-engine/quick-start.md:125:Or, you can enable it in `Setting > General > V2 Data Engine`.
content/docs/1.6.1/v2-data-engine/quick-start.md:156:You can observe the utilization of allocated huge pages on each node by running the command `kubectl get node <node name> -o yaml`.
content/docs/1.6.1/v2-data-engine/quick-start.md:203:kubectl -n longhorn-system edit node.longhorn.io <NODE NAME>
content/docs/1.6.1/v2-data-engine/quick-start.md:208:<DISK NAME>:
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:46:> **IMPORTANT:**
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:47:> - `name`: Use the same name for the backing image and its backup. If the names are not identical, Longhorn will not be able to find the backing image.
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:48:> - `userCreated`: Set the value to `true` to indicate that you created the backup custom resource, which enabled the creation of the backup in the backupstore. The value `false` indicates that the backup custom resource was synced from the backupstore.
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:49:> - `labels`: You can add labels to the backing image backup.
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:71:> **IMPORTANT:**
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:72:> - `sourceType`: Set the value to `restore`.
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:73:> - `sourceParameters`: Configure the following parameters:
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:74:>   - `backup-url`: URL of the backing image resource in the backupstore. You can find this information in the status of the backup custom resource `.Status.URL`.
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:75:>   - `concurrent-limit`: Maximum number of worker threads that can concurrently run for each restore operation. When unspecified, Longhorn uses the default value.
content/docs/1.7.0/advanced-resources/backing-image/backing-image-backup.md:76:> - `checksum`: You can specify the expected SHA-512 checksum of the backing image file, which Longhorn uses to validate the restored file. When unspecified, Longhorn uses the checksum of the restored file as the truth.
content/docs/1.7.0/advanced-resources/backing-image/backing-image.md:23:  What's the difference? When Longhorn calculates the checksum of a qcow2 file, it will read the file as a raw file instead of using the qcow library to read the correct content. In other words, users always get the correct checksum by executing `shasum -a 512 <the file path>` regardless of the file format.
content/docs/1.7.0/advanced-resources/backing-image/backing-image.md:30:On **Setting > Backing Image** page, users can create backing images with any kinds of data source.
content/docs/1.7.0/advanced-resources/backing-image/backing-image.md:115:1. Click **Setting > Backing Image** in the Longhorn UI.
content/docs/1.7.0/advanced-resources/backing-image/backing-image.md:136:- The unused backing images can be also cleaned up manually via the Longhorn UI: Click **Setting > Backing Image > Operation list of one backing image > Clean Up**. Then choose disks.
content/docs/1.7.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:48:4. Get the detailed information of one of the orphaned replica directories in `spec.parameters` by `kubcel -n longhorn-system get orphan <name>`.
content/docs/1.7.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:91:5. One can delete the `orphan` resource by `kubectl -n longhorn-system delete orphan <name>` and then the corresponding orphaned replica directory will be deleted.
content/docs/1.7.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:134:    # kubectl -n longhorn-system delete orphan -l "longhornnode=<node name>”
content/docs/1.7.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:139:In the top navigation bar of the Longhorn UI, click `Setting > Orphaned Data`. Orphaned replica directories on each node and in each disk are listed. One can delete the directories by `Operation > Delete`.
content/docs/1.7.0/advanced-resources/data-cleanup/orphaned-data-cleanup.md:141:By default, Longhorn will not automatically delete the orphaned replica directory. One can enable the automatic deletion in `Setting > General > Orphan`.
content/docs/1.7.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:16:- **snapshot-data-integrity** <br/>
content/docs/1.7.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:24:- **snapshot-data-integrity-immediate-check-after-snapshot-creation** <br/>
content/docs/1.7.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:28:- **snapshot-data-integrity-cronjob** <br/>
content/docs/1.7.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:32:    > **Warning**
content/docs/1.7.0/advanced-resources/data-integrity/snapshot-data-integrity-check.md:33:    > Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.7.0/advanced-resources/data-recovery/data-error.md:32:5. Mount the volume from `/dev/longhorn/<volume_name>` and check the volume content.
content/docs/1.7.0/advanced-resources/data-recovery/export-from-replica.md:40:    <volume_name>-<8 bytes UUID>
content/docs/1.7.0/advanced-resources/data-recovery/export-from-replica.md:63:   docker run -v /dev:/host/dev -v /proc:/host/proc -v <data_path>:/volume --privileged longhornio/longhorn-engine:v1.7.0 launch-simple-longhorn <volume_name> <volume_size>
content/docs/1.7.0/advanced-resources/data-recovery/export-from-replica.md:71:**Result:** Now you should have a block device created on `/dev/longhorn/<volume_name>` for this device, such as `/dev/longhorn/pvc-06b4a8a8-b51d-42c6-a8cc-d8c8d6bc65bc` for the example above. Now you can mount the block device to get the access to the data.
content/docs/1.7.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:10:2. Set the node which the output file should be placed on by replacing `<NODE_NAME>`, e.g. `node1`.
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:14:4. Set the first argument (backup url) by replacing `<BACKUP_URL>`, e.g. `s3://<your-bucket-name>@<your-aws-region>/backupstore?backup=<backup-name>&volume=<volume-name>`.
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:16:    - `<backup-name>` and `<volume-name>` can be retrieved from backup.cfg stored in the backup destination folder, e.g. `backup_backup-72bcbdad913546cf.cfg`. The content will be like below: 
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:22:5. Set argument `output-file` by replacing `<OUTPUT_FILE>`, e.g. `volume.raw` or `volume.qcow2`.
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:24:6. Set argument `output-format` by replacing `<OUTPUT_FORMAT>`. The supported options are `raw` or `qcow2`.
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:26:7. Set argument `longhorn-version` by replacing `<LONGHORN_VERSION>`, e.g. `v1.7.0`
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:28:8. Set the S3 Credential Secret by replacing `<S3_SECRET_NAME>`, e.g. `minio-secret`.  
content/docs/1.7.0/advanced-resources/data-recovery/recover-without-system.md:40:After the pod status changed to `Completed`, you should able to find `<OUTPUT_FILE>` at e.g. `/tmp/restore` on the `<NODE_NAME>`.
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:31:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Install > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before installing the app.
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:99:> **NOTE:**
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:100:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:170:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app to the current Longhorn version.
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:176:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:181:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then update the settings using
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:190:From the project view in Rancher, go to **Apps && Marketplace > Longhorn > Upgrade > Next > Edit Options > Longhorn Default Settings > Customize Default Settings** and edit the settings before upgrading the app.
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:193:Modify the config map named `longhorn-default-setting` in the yaml file `longhorn/deploy/longhorn.yaml` as described in [Fresh Installation > Using the Longhorn Deployment YAML File](#using-the-longhorn-deployment-yaml-file) and then upgrade the Longhorn system using `kubectl`.
content/docs/1.7.0/advanced-resources/deploy/customizing-default-settings.md:197:Modify the default settings in the YAML file as described in [Fresh Installation > Using Helm](#using-helm) and then upgrade the Longhorn system using `helm upgrade`.
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:38:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `systemManagedComponentsNodeSelector` to `defaultSettings`.
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:39:   >
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:40:   > For example:
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:41:   > ```yaml
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:42:   > defaultSettings:
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:43:   >   systemManagedComponentsNodeSelector: "label-key1:label-value1"
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:44:   >  ```
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:48:> **Warning**:
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:49:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:50:> * When all Longhorn volumes are detached, the customized settings are immediately applied to the system-managed components (for example, Instance manager, CSI driver and Engine images).
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:51:> * When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.7.0/advanced-resources/deploy/node-selector.md:77:   The node selector setting can be found at Longhorn UI under **Setting > General > System Managed Components Node Selector.**
content/docs/1.7.0/advanced-resources/deploy/priority-class.md:21:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.7.0/advanced-resources/deploy/priority-class.md:22:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.7.0/advanced-resources/deploy/priority-class.md:23:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.7.0/advanced-resources/deploy/priority-class.md:27:1. Set taint Priority Class for system managed components: The Priority Class setting can be found in the Longhorn UI by clicking **Setting > General > Priority Class.**
content/docs/1.7.0/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.7.0/advanced-resources/deploy/rancher_windows_cluster.md:23:`Edit Options` > `Other Settings` > `Rancher Windows Cluster`
content/docs/1.7.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.7.0/advanced-resources/deploy/revision_counter.md:20:To disable or enable the revision counter from the Longhorn UI, click **Setting > General > Disable Revision Counter.**
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:8:The Storage Network setting takes Multus NetworkAttachmentDefinition in `<NAMESPACE>/<NAME>` format.
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:15:> **Important**: To ensure that your preferred settings are immediately applied, stop all workloads and detach all Longhorn volumes before configuring the settings.
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:17:> When all volumes are detached, Longhorn attempts to restart all Instance Manager and Backing Image Manager pods to apply the setting.
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:18:> When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:31:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:32:>
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:33:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:34:>
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:35:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:41:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:42:>
content/docs/1.7.0/advanced-resources/deploy/storage-network.md:43:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:47:   > Note: Because of the limitation of Rancher 2.5.x, if you are using Rancher UI to install Longhorn, you need to click `Edit As Yaml` and add setting `taintToleration` to `defaultSettings`.
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:48:   >
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:49:   > For example:
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:50:   > ```yaml
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:51:   > defaultSettings:
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:52:   >   taintToleration: "key=value:NoSchedule"
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:53:   >  ```
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:59:> To ensure that your preferred toleration settings are immediately applied, stop all workloads and detach all Longhorn volumes before configuring the settings.
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:62:>
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:63:> When all Longhorn volumes are detached, the customized setting is immediately applied to the system-managed components.
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:64:> When one or more Longhorn volumes are still attached, the customized setting is applied to the Instance Manager only when no engines and replica instances are running. You are required to reconfigure the setting after detaching the remaining volumes. Alternatively, you can wait for the next setting synchronization, which will occur in an hour.
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:65:>
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:66:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.7.0/advanced-resources/deploy/taint-toleration.md:101:   The taint toleration setting can be found at Longhorn UI under **Setting > General > Kubernetes Taint Toleration.**
content/docs/1.7.0/advanced-resources/driver-migration/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.7.0/advanced-resources/os-distro-specific/csi-on-gke.md:13:    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<name@example.com>
content/docs/1.7.0/advanced-resources/os-distro-specific/csi-on-k3s.md:17:  If you want to deploy these new Longhorn versions on versions before k3s v0.10.0, you need to set `--kubelet-root-dir` to `<data-dir>/agent/kubelet` for the Deployment `longhorn-driver-deployer` in `longhorn/deploy/longhorn.yaml`. 
content/docs/1.7.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:12:**For k8s v1.12+**, the kubelet will detect the `csi.sock` according to argument `<--kubelet-registration-path>` passed in by Kubernetes CSI driver-registrar, and `<drivername>-reg.sock` (for Longhorn, it's `io.rancher.longhorn-reg.sock`) on kubelet path `<root-dir>/plugins`.
content/docs/1.7.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:16:By default, Longhorn CSI driver creates and expose these two sock files on the host path `/var/lib/kubelet/plugins`. Then the kubelet cannot find `<drivername>-reg.sock`, so CSI driver doesn't work.
content/docs/1.7.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:18:Furthermore, the kubelet will instruct the CSI plugin to mount the Longhorn volume on `<root-dir>/pods/<pod-name>/volumes/kubernetes.io~csi/<volume-name>/mount`. But this path inside the CSI plugin container won't be bind mounted on the host path. And the mount operation for the Longhorn volume is meaningless.
content/docs/1.7.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.7.0/advanced-resources/os-distro-specific/okd-support.md:62:cat <<EOF >>auto-mount-machineconfig.yaml
content/docs/1.7.0/advanced-resources/rebuilding/fast-replica-rebuild.md:20:- fast-replica-rebuild-enabled <br/>
content/docs/1.7.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.7.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.7.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:20:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:21:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:22:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:23:    AKS_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:24:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:25:    AKS_K8S_VERSION=<kubernetes-version>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks.md:41:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:17:2. Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab `Configuration/Compute/<node-group-name>` and click the launch template.
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:20:    GKE_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:21:    GKE_REGION=<gke-region>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:22:    GKE_CLUSTER_NAME=<gke-cluster-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:24:    GKE_MACHINE_TYPE=<gcp-machine-type>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:25:    GKE_DISK_SIZE_NEW=<new-disk-size-in-gb>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:26:    GKE_NODE_NUM=<number-of-nodes>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke.md:45:    GKE_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:14:    AKS_RESOURCE_GROUP=<aks-resource-group>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:15:    AKS_CLUSTER_NAME=<aks-cluster-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:16:    AKS_K8S_VERSION_UPGRADE=<aks-k8s-version>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:28:    AKS_NODEPOOL_NAME_NEW=<new-nodepool-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:29:    AKS_DISK_SIZE=<disk-size-in-gb>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:30:    AKS_NODE_NUM=<number-of-nodes>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks.md:46:    AKS_NODEPOOL_NAME_OLD=<old-nodepool-name>
content/docs/1.7.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.7.0/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.7.0/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.7.0/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.7.0/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:56:   > **Note:** Unsetting the backup target clears the existing `SystemBackup` custom resource. Longhorn syncs to the remote backup store after setting the backup target. Another cluster can also sync to the same list of system backups when the backup target is the same.
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:60:  > **Note:** Longhorn system restores volume with the latest backup. We recommend updating the last backup for all volumes. By taking volume backups, you ensure that the data is up-to-date with the system backup. For more information, please refer to the [Configuration - Volume Backup Policy](#volume-backup-policy) section.
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:92:   > kubectl -n longhorn-system get systembackup
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:106:   > **Note:** Deleting the system backup will also make a deletion in the backup store.
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:112:   > kubectl -n longhorn-system get systembackup
content/docs/1.7.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:116:   > kubectl -n longhorn-system delete systembackup/demo
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:32:Longhorn does not support cross-major/minor version system restore except for upgrade failures, ex: 1.4.x -> 1.5.
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:61:   > kubectl -n longhorn-system get systembackup
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:79:   > kubectl -n longhorn-system get systemrestore
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:99:   > kubectl -n longhorn-system get systemrestore
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:103:   > kubectl -n longhorn-system delete systemrestore/restore-demo
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.7.0/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.7.0/best-practices.md:52:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.7.0/best-practices.md:176:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.7.0/best-practices.md:192:> Recommend: `false`
content/docs/1.7.0/best-practices.md:198:> Recommend: `false`
content/docs/1.7.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.7.0/deploy/accessing-the-ui/_index.md:28:    longhorn-backend    ClusterIP      10.20.248.250   <none>           9500/TCP       58m
content/docs/1.7.0/deploy/accessing-the-ui/_index.md:29:    longhorn-frontend   ClusterIP      10.20.245.110   <none>           80/TCP         58m
content/docs/1.7.0/deploy/accessing-the-ui/_index.md:35:    > For Longhorn v0.8.0+, UI service type changed from `LoadBalancer` to `ClusterIP.`
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:12:    $ USER=<USERNAME_HERE>; PASSWORD=<PASSWORD_HERE>; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:19:    > Since v1.2.0, Longhorn supports uploading backing image from the UI, so please specify `nginx.ingress.kubernetes.io/proxy-body-size: 10000m` as below to ensure uploading images work as expected.
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:57:$ USER=foo; PASSWORD=bar; echo "${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})" >> auth
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:119:< HTTP/1.1 401 Unauthorized
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:120:< Server: openresty/1.15.8.1
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:121:< Date: Fri, 29 May 2020 11:47:33 GMT
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:122:< Content-Type: text/html
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:123:< Content-Length: 185
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:124:< Connection: keep-alive
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:125:< WWW-Authenticate: Basic realm="Authentication Required"
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:126:<
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:127:<html>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:128:<head><title>401 Authorization Required</title></head>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:129:<body>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:130:<center><h1>401 Authorization Required</h1></center>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:131:<hr><center>openresty/1.15.8.1</center>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:132:</body>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:133:</html>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:148:< HTTP/1.1 200 OK
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:149:< Date: Fri, 29 May 2020 11:51:27 GMT
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:150:< Content-Type: text/html
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:151:< Content-Length: 1118
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:152:< Last-Modified: Thu, 28 May 2020 00:39:41 GMT
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:153:< ETag: "5ecf084d-3fd"
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:154:< Cache-Control: max-age=0
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:155:<
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:156:<!DOCTYPE html>
content/docs/1.7.0/deploy/accessing-the-ui/longhorn-ingress.md:157:<html lang="en">
content/docs/1.7.0/deploy/important-notes/index.md:81:For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, please do:
content/docs/1.7.0/deploy/important-notes/index.md:84:  - Rancher UI installation method: set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.7.0/deploy/important-notes/index.md:120:> **Warning:**
content/docs/1.7.0/deploy/important-notes/index.md:121:> Whenever engine upgrade enforcement causes upgrade failure, Longhorn allows you to revert to the previous version because Longhorn Manager will block the entire upgrade. However, Longhorn prohibits downgrading when an upgrade is successful. For more information, see [Upgrade Path Enforcement](../../deploy/upgrade/#upgrade-path-enforcement-and-downgrade-prevention).
content/docs/1.7.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.7.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.7.0/deploy/install/_index.md:30:-  Kubernetes >= v1.21
content/docs/1.7.0/deploy/install/_index.md:157:  echo "InitiatorName=$(/sbin/iscsi-iname)" > /etc/iscsi/initiatorname.iscsi
content/docs/1.7.0/deploy/install/_index.md:266:The `Server Version` should be >= v1.21.
content/docs/1.7.0/deploy/install/airgap.md:29:  - We provide another script, [load-images.sh](https://raw.githubusercontent.com/longhorn/longhorn/v1.7.0/scripts/load-images.sh), to push Longhorn images to your private registry. If you specify a `tar.gz` file name for flag `--images`, the script loads images from the `tar` file and pushes them. Otherwise, it will find images in your local Docker and push them. In the example below, the script loads images from the file `longhorn-images.tar.gz` and pushes them to `<YOUR-PRIVATE-REGISTRY>`
content/docs/1.7.0/deploy/install/airgap.md:33:    ./load-images.sh --image-list longhorn-images.txt --images longhorn-images.tar.gz --registry <YOUR-PRIVATE-REGISTRY>
content/docs/1.7.0/deploy/install/airgap.md:54:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.7.0/deploy/install/airgap.md:89:            - name: <SECRET_NAME>                          ## Add SECRET_NAME here
content/docs/1.7.0/deploy/install/airgap.md:104:        value: <REGISTRY_URL>/csi-attacher:<CSI_ATTACHER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:106:        value: <REGISTRY_URL>/csi-provisioner:<CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:108:        value: <REGISTRY_URL>/csi-node-driver-registrar:<CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:110:        value: <REGISTRY_URL>/csi-resizer:<CSI_RESIZER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:112:        value: <REGISTRY_URL>/csi-snapshotter:<CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:118:        `image: <REGISTRY_URL>/longhorn-manager:<LONGHORN_MANAGER_IMAGE_TAG>`
content/docs/1.7.0/deploy/install/airgap.md:122:        `image: <REGISTRY_URL>/longhorn-engine:<LONGHORN_ENGINE_IMAGE_TAG>`
content/docs/1.7.0/deploy/install/airgap.md:126:        `image: <REGISTRY_URL>/longhorn-instance-manager:<LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>`
content/docs/1.7.0/deploy/install/airgap.md:130:        `image: <REGISTRY_URL>/longhorn-share-manager:<LONGHORN_SHARE_MANAGER_IMAGE_TAG>`
content/docs/1.7.0/deploy/install/airgap.md:134:        `image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>`
content/docs/1.7.0/deploy/install/airgap.md:157:              image: <REGISTRY_URL>/longhorn-ui:<LONGHORN_UI_IMAGE_TAG>   ## Add image name and tag here
content/docs/1.7.0/deploy/install/airgap.md:164:            - name: <SECRET_NAME>
content/docs/1.7.0/deploy/install/airgap.md:173:In v1.7.0, Longhorn automatically adds <REGISTRY_URL> prefix to images. You simply need to set the registryUrl parameters to pull images from your private registry.
content/docs/1.7.0/deploy/install/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.7.0/deploy/install/airgap.md:192:        registrySecret: <SECRET_NAME>
content/docs/1.7.0/deploy/install/airgap.md:195:          registryUrl: <REGISTRY_URL>
content/docs/1.7.0/deploy/install/airgap.md:196:          registryUser: <REGISTRY_USER>
content/docs/1.7.0/deploy/install/airgap.md:197:          registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.7.0/deploy/install/airgap.md:198:          registrySecret: <REGISTRY_SECRET_NAME>
content/docs/1.7.0/deploy/install/airgap.md:211:    > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.7.0/deploy/install/airgap.md:220:              tag: <LONGHORN_ENGINE_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:223:              tag: <LONGHORN_MANAGER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:226:              tag: <LONGHORN_UI_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:229:              tag: <LONGHORN_INSTANCE_MANAGER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:232:              tag: <LONGHORN_SHARE_MANAGER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:241:              tag: <CSI_ATTACHER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:244:              tag: <CSI_PROVISIONER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:247:              tag: <CSI_NODE_DRIVER_REGISTRAR_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:250:              tag: <CSI_RESIZER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:253:              tag: <CSI_SNAPSHOTTER_IMAGE_TAG>
content/docs/1.7.0/deploy/install/airgap.md:261:          registrySecret: <SECRET_NAME>
content/docs/1.7.0/deploy/install/airgap.md:264:            registryUrl: <REGISTRY_URL>
content/docs/1.7.0/deploy/install/airgap.md:265:            registryUser: <REGISTRY_USER>
content/docs/1.7.0/deploy/install/airgap.md:266:            registryPasswd: <REGISTRY_PASSWORD>
content/docs/1.7.0/deploy/install/airgap.md:295:  > **Note:** Do not include the private registry prefix, it will be added automatically. e.g: if your image is `example.com/username/longhorn-manager`, use `username/longhorn-manager` in the following charts.
content/docs/1.7.0/deploy/install/airgap.md:311:    `kubectl -n longhorn-system create secret docker-registry <SECRET_NAME> --docker-server=<REGISTRY_URL> --docker-username=<REGISTRY_USER> --docker-password=<REGISTRY_PASSWORD>`
content/docs/1.7.0/deploy/install/airgap.md:322:    value: <SECRET_NAME>
content/docs/1.7.0/deploy/install/install-with-argocd.md:18:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.7.0/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.7.0/deploy/install/install-with-argocd.md:37:    cat > longhorn-application.yaml <<EOF
content/docs/1.7.0/deploy/install/install-with-fleet.md:19:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.7.0/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.7.0/deploy/install/install-with-fleet.md:94:    cat > longhorn-gitrepo.yaml << "EOF"
content/docs/1.7.0/deploy/install/install-with-flux.md:15:    export GITHUB_TOKEN=<gh-token>
content/docs/1.7.0/deploy/install/install-with-flux.md:18:      --owner=<github_username> \
content/docs/1.7.0/deploy/install/install-with-flux.md:19:      --repository=<github_repo_name> \
content/docs/1.7.0/deploy/install/install-with-flux.md:20:      --branch=<branch_name> \
content/docs/1.7.0/deploy/install/install-with-flux.md:21:      --path=<folder_path_within_repo> \
content/docs/1.7.0/deploy/install/install-with-flux.md:25:> Use [this script](https://github.com/longhorn/longhorn/blob/v1.7.0/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.7.0/deploy/install/install-with-flux.md:36:      --export > helmrepo.yaml
content/docs/1.7.0/deploy/install/install-with-flux.md:47:      --export > helmrelease.yaml
content/docs/1.7.0/deploy/install/install-with-flux.md:97:    git push origin <branch_name>
content/docs/1.7.0/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v1.7.0/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.7.0/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.7.0/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.7.0/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.7.0/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.7.0/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.7.0/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.7.0/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.7.0/deploy/uninstall/_index.md:69:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v1.7.0/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.7.0/deploy/uninstall/_index.md:88:      > helm list -n longhorn-system -a
content/docs/1.7.0/deploy/uninstall/_index.md:95:      > helm rollback longhorn 2 -n longhorn-system
content/docs/1.7.0/deploy/upgrade/_index.md:18:> **Warning**:
content/docs/1.7.0/deploy/upgrade/_index.md:19:> - Once you successfully upgrade to v1.6.0, you will not be allowed to revert to the previously installed version.
content/docs/1.7.0/deploy/upgrade/_index.md:20:> - The Downgrade Prevention feature was introduced in v1.5.0 so Longhorn is unable to prevent downgrade attempts in older versions.
content/docs/1.7.0/deploy/upgrade/longhorn-manager.md:23:          numberOfReplicas: <user specified replica count, 3 by default>
content/docs/1.7.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.7.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.7.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.7.0/maintenance/maintenance.md:34:   > **Note:** Volumes that are not attached through the CSI flow on the node (for example, manually attached using
content/docs/1.7.0/maintenance/maintenance.md:35:   > UI) will not be automatically attached to new nodes by Kubernetes during the draining. Therefore, Longhorn will
content/docs/1.7.0/maintenance/maintenance.md:36:   > prevent the node from completing the drain operation. The user will need to detach these volumes manually to
content/docs/1.7.0/maintenance/maintenance.md:37:   > unblock the draining.
content/docs/1.7.0/maintenance/maintenance.md:42:   > **Note:** By default, if there is one last healthy replica for a volume on the node, Longhorn will prevent the node
content/docs/1.7.0/maintenance/maintenance.md:43:   > from completing the drain operation, to protect the last replica and prevent the disruption of the workload. You
content/docs/1.7.0/maintenance/maintenance.md:44:   > can control this behavior with the setting [Node Drain Policy](../../references/settings#node-drain-policy), or
content/docs/1.7.0/maintenance/maintenance.md:45:   > [evict the replica to other nodes before draining](../../nodes-and-volumes/nodes/disks-or-nodes-eviction). See [Node Drain Policy
content/docs/1.7.0/maintenance/maintenance.md:46:   > Recommendations](#node-drain-policy-recommendations) for considerations when selecting a policy.
content/docs/1.7.0/maintenance/maintenance.md:54:   > **Note:** Normally you don't need to evict the replicas before the drain operation, as long as you have healthy
content/docs/1.7.0/maintenance/maintenance.md:55:   > replicas on other nodes. The replicas can be reused later, once the node back online and uncordoned. See [Node
content/docs/1.7.0/maintenance/maintenance.md:56:   > Drain Policy](#node-drain-policy-recommendations) for further guidance.
content/docs/1.7.0/maintenance/maintenance.md:94:      kubectl delete node <node-name>
content/docs/1.7.0/monitoring/alert-rules-example.md:27:      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
content/docs/1.7.0/monitoring/alert-rules-example.md:57:      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
content/docs/1.7.0/monitoring/alert-rules-example.md:67:      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
content/docs/1.7.0/monitoring/alert-rules-example.md:76:      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
content/docs/1.7.0/monitoring/alert-rules-example.md:86:      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
content/docs/1.7.0/monitoring/alert-rules-example.md:96:      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:34:This document uses the `default` namespace for the monitoring system. To install on a different namespace, change the field `namespace: <OTHER_NAMESPACE>` in manifests.
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:113:      - to: <the email address to send notifications to>
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:114:        from: <the sender address>
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:115:        smarthost: <the SMTP host through which emails are sent>
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:117:        auth_username: <the username>
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:118:        auth_identity: <the identity>
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:119:        auth_password: <the password>
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:131:      - api_url: <the Slack webhook URL>
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:132:        channel: <the channel or user to send notifications to>
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:145:    Alertmanager instances require the secret resource naming to follow the format `alertmanager-<ALERTMANAGER_NAME>`. In the previous step, the name of the Alertmanager is `longhorn`, so the secret name must be `alertmanager-longhorn`
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:173:    > Use the above `NodePort` service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress-controller to expose the web UI of Alertmanager over a TLS connection.
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:197:          expr: 100 * (longhorn_volume_usage_bytes / longhorn_volume_capacity_bytes) > 90
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:299:    > At this point, you should be able to see all Longhorn manager targets as well as Longhorn rules in the targets and rules section of the Prometheus server UI.
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:301:    > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to `ClusterIP` and set up an Ingress controller to expose the web UI of the Prometheus server over a TLS connection.
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:331:    > **NOTE:** change field `url` if you are installing the monitoring stack in a different namespace.
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:332:    > `http://prometheus-longhorn.<NAMESPACE>.svc:9090"`
content/docs/1.7.0/monitoring/prometheus-and-grafana-setup.md:403:   > Use the above NodePort service for quick verification only because it doesn't communicate over the TLS connection. You may want to change the service type to ClusterIP and set up an Ingress controller to expose Grafana over a TLS connection.
content/docs/1.7.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:22:    node.longhorn.io/default-node-tags: <node tag list with JSON string format>
content/docs/1.7.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.7.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.7.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:62:    node.longhorn.io/default-disks-config: <disks configuration with JSON string format>
content/docs/1.7.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:87:    > **Note:** If the same name is specified for different disks, the configuration will be treated as invalid.
content/docs/1.7.0/nodes-and-volumes/nodes/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.7.0/nodes-and-volumes/nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.7.0/nodes-and-volumes/nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.7.0/nodes-and-volumes/nodes/multidisk.md:24:> **Note**:
content/docs/1.7.0/nodes-and-volumes/nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.7.0/nodes-and-volumes/nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.7.0/nodes-and-volumes/nodes/scheduling.md:53:Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` or `topology.kubernetes.io/region=<Region name of the node>` in the Kubernetes node object to identify the zone/region.
content/docs/1.7.0/nodes-and-volumes/nodes/storage-tags.md:16:1. *Node -> Select one node -> Edit Node and Disks*
content/docs/1.7.0/nodes-and-volumes/volumes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.7.0/nodes-and-volumes/volumes/create-volumes.md:155:In order to help users understand the error causes, Longhorn summarizes them in the PV annotation, `longhorn.io/volume-scheduling-error`. Failures are combined in this annotation and separated by a semicolon, for example, `longhorn.io/volume-scheduling-error: insufficient storage;disks are unavailable`. The annotation can be checked by using `kubectl describe pv <pvc name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/create-volumes.md:159:Labels:          <none>
content/docs/1.7.0/nodes-and-volumes/volumes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.7.0/nodes-and-volumes/volumes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.7.0/nodes-and-volumes/volumes/detaching-volumes.md:11:Edit the deployment with `kubectl edit deploy/<name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/detaching-volumes.md:16:Edit the statefulset with `kubectl edit statefulset/<name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/detaching-volumes.md:23:Delete the daemonset with `kubectl delete ds/<name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/detaching-volumes.md:26:Delete the pod with `kubectl delete pod/<name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/detaching-volumes.md:31:Edit the cronjob with `kubectl edit cronjob/<name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/detaching-volumes.md:40:Otherwise, delete the job with `kubectl delete job/<name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/detaching-volumes.md:43:Edit the replicaset with `kubectl edit replicaset/<name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/detaching-volumes.md:48:Edit the replicationcontroller with `kubectl edit rc/<name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:102:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:103:    umount /dev/longhorn/<volume name>
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:104:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:105:    resize2fs /dev/longhorn/<volume name>
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:106:    umount /dev/longhorn/<volume name>
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:112:    mount /dev/longhorn/<volume name> <arbitrary mount directory>
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:113:    xfs_growfs <the mount directory>
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:114:    umount /dev/longhorn/<volume name>
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:123:2. Execute `cryptsetup resize <volume name>`. The passphrase this command requires is the field `CRYPTO_KEY_VALUE` of the corresponding secret.
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:131:2. Figure out the share manager pod of the RWX volume then execute the filesystem expansion command. The share manager pod is typically named as `share-manager-<volume name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/expansion.md:133:    kubectl -n longhorn-system exec -it <the share manager pod> -- resize2fs /dev/longhorn/<volume name>
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:11:Longhorn creates a dedicated `share-manager-<volume-name>` Pod within the `longhorn-system` namespace for each RWX volume that is currently in active use. The Pod facilitate the export of Longhorn volume via an internally hosted NFSv4 server. Additionally, a corresponding Service is created for each RWX volume, serving as the designated endpoint for actual NFSv4 client connections.
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:23:      > **Troubleshooting:** If the NFSv4 client is not available on the node, when trying to mount the volume the below message will be part of the error:
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:24:      > ```
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:25:      > for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:26:      > ```
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:32:    > **Tip:** The [environment check script](https://raw.githubusercontent.com/longhorn/longhorn/v1.7.0/scripts/environment_check.sh) helps users to check all nodes have unique hostnames.
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:36:> **Notice**  
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:37:> An RWX volume must have the access mode set to `ReadWriteMany` and the "migratable" flag disabled (*parameters.migratable: `false`*).
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:72:> **Important:**
content/docs/1.7.0/nodes-and-volumes/volumes/rwx-volumes.md:73:> To create PVCs for RWX volumes using the sample StorageClass, replace the `nfsOptions` string with a customized comma-separated list of legal options.
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:8:> **Note:**
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:10:>
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:27:Then Longhorn will **try its best** to figure out the mount point and execute `fstrim <the mount point>`.  If something is wrong or the filesystem does not exist, the UI will return an error.
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:31:When using `fstrim`, you must identify the mount point of the volume and then run the command `fstrim <the mount point>`.
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:34:- RWX volume: The mount point is the share manager pod of the volume. The share manager pod contains the NFS server and is typically named `share-manager-<volume name>`.
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:39:    kubectl -n longhorn-system exec -it <the share manager pod> -- bash
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:41:1. Identify the work directory of the NFS server (for example, `/export/<volume name>`).
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:43:    mount | grep <volume name>
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:44:    /dev/longhorn/<volume name> on /export/<volume name> type ext4 (rw,relatime)
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:48:    fstrim /export/<volume name>
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:80:    cryptsetup --allow-discards --persistent refresh <Longhorn volume name>
content/docs/1.7.0/nodes-and-volumes/volumes/trim-filesystem.md:82:    3. Directly use Longhorn UI to trim the volume or execute `fstrim` for **the mount point** of `/dev/mapper/<volume name>` manually.
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:44:    > Users can see by default the volume `actual size` is not shrunk after deleting the 4 Gi data. Longhorn is a block-level storage system. Therefore, the deletion in the filesystem only marks the blocks that belong to the deleted file as unused. Currently, Longhorn will not apply TRIM/UNMAP operations automatically/periodically. if you want to do filesystem trim, please check [this doc](../trim-filesystem) for details.
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:50:     > After deletion, filesystem may or maynot reuse the recently freed blocks from recently deleted files according to the filesystem design and please refer to [Block allocation strategies of various filesystems](https://www.ogris.de/blkalloc). If the volume nominal `size` is 12 Gi, the `actual size` in the end would range from 4 Gi to 8 Gi since the filesystem may or maynot reuse the freed blocks. On the other hand, if the volume nominal `size` is 6 Gi, the `actual size` at the end would range from 4 Gi to 6 Gi, because the filesystem has to reuse the freed blocks in the 2nd round of writing. See Figure 3(b) of the illustration.
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:51:     >
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:52:     > Thus, allocating an appropriate nominal `size` for a volume that holds heavy writing tasks according to the IO pattern would make disk space usage more efficient.
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:71:    > The volume head holds the latest data of the volume only, while each snapshot may store historical data as well as active data, which consumes at most size space. Therefore, the volume `actual size`, which is the size sum of the volume head and all snapshots, is possibly bigger than the size specified by users.
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:72:    >
content/docs/1.7.0/nodes-and-volumes/volumes/volume-size.md:73:    > Even if users will not take snapshots for volumes, there are operations like rebuilding, expansion, or backing up that would lead to system (hidden) snapshot creation. As a result, volume `actual size` being larger than size is unavoidable under some use cases.
content/docs/1.7.0/references/examples.md:244:      nodeName: <NODE_NAME>
content/docs/1.7.0/references/examples.md:252:          '<BACKUP_URL>'
content/docs/1.7.0/references/examples.md:253:          --output-file '/tmp/restore/<OUTPUT_FILE>'
content/docs/1.7.0/references/examples.md:254:          --output-format <OUTPUT_FORMAT>
content/docs/1.7.0/references/examples.md:262:          mountPath: /tmp/restore  # the argument <output-file> should be in this directory
content/docs/1.7.0/references/examples.md:268:              name: <S3_SECRET_NAME>
content/docs/1.7.0/references/examples.md:273:              name: <S3_SECRET_NAME>
content/docs/1.7.0/references/examples.md:278:              name: <S3_SECRET_NAME>
content/docs/1.7.0/references/helm-values.md:243:| defaultSettings.replicaZoneSoftAntiAffinity | Setting that allows Longhorn to schedule new replicas of a volume to nodes in the same zone as existing healthy replicas. Nodes that do not belong to any zone are treated as existing in the zone that contains healthy replicas. When identifying zones, Longhorn relies on the label "topology.kubernetes.io/zone=<Zone name of the node>" in the Kubernetes node object. |
content/docs/1.7.0/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.7.0/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.7.0/references/networking.md:76:`Longhorn CSI plugin` pods communitate with `CSI sidecar` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.7.0/references/networking.md:88:`CSI sidecar` pods communitate with `Longhorn CSI plugin` pods over the Unix Domain Socket at `<Kuberlet-Directory>/plugins/driver.longhorn.io/csi.sock`
content/docs/1.7.0/references/settings.md:94:> Default: `block-if-contains-last-replica`
content/docs/1.7.0/references/settings.md:118:> Default: `false`
content/docs/1.7.0/references/settings.md:125:> Default: `true`
content/docs/1.7.0/references/settings.md:131:> Default: `true`
content/docs/1.7.0/references/settings.md:139:> Default: `true`
content/docs/1.7.0/references/settings.md:146:> **Note:** This setting doesn't apply to below cases.
content/docs/1.7.0/references/settings.md:147:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.7.0/references/settings.md:148:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../nodes-and-volumes/volumes/rwx-volumes).
content/docs/1.7.0/references/settings.md:152:> Default: `true`
content/docs/1.7.0/references/settings.md:158:> Default: `0`
content/docs/1.7.0/references/settings.md:166:> Default: `5`
content/docs/1.7.0/references/settings.md:176:> Default: `false`
content/docs/1.7.0/references/settings.md:186:> Default: `longhorn.io/v1beta2`
content/docs/1.7.0/references/settings.md:192:> Default: `disabled`
content/docs/1.7.0/references/settings.md:210:> Default: `/var/lib/longhorn/`
content/docs/1.7.0/references/settings.md:224:> Default: `longhorn-static`
content/docs/1.7.0/references/settings.md:230:> Default: `3`
content/docs/1.7.0/references/settings.md:241:> Default: `false`
content/docs/1.7.0/references/settings.md:245:> Default: `false`
content/docs/1.7.0/references/settings.md:251:> Default: `true`
content/docs/1.7.0/references/settings.md:259:> Only available if `Upgrade Checker` is enabled.
content/docs/1.7.0/references/settings.md:263:> Default: `true`
content/docs/1.7.0/references/settings.md:271:  > This value may not be accurate for virtual machines.
content/docs/1.7.0/references/settings.md:348:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.7.0/references/settings.md:352:> Default: `do-nothing`
content/docs/1.7.0/references/settings.md:367:> Default: `600`
content/docs/1.7.0/references/settings.md:375:> Default: `if-not-present`
content/docs/1.7.0/references/settings.md:391:> Default: `60`
content/docs/1.7.0/references/settings.md:396:> Default: `300`
content/docs/1.7.0/references/settings.md:400:> **Note:**
content/docs/1.7.0/references/settings.md:401:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.7.0/references/settings.md:402:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.7.0/references/settings.md:405:> Default: `8`
content/docs/1.7.0/references/settings.md:416:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.7.0/references/settings.md:420:> Default: `1`
content/docs/1.7.0/references/settings.md:430:> Default: `false`
content/docs/1.7.0/references/settings.md:436:> Default: `30`
content/docs/1.7.0/references/settings.md:442:> Default: `true`
content/docs/1.7.0/references/settings.md:449:> Default: `false`
content/docs/1.7.0/references/settings.md:453:> **Warning**
content/docs/1.7.0/references/settings.md:454:>
content/docs/1.7.0/references/settings.md:455:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.7.0/references/settings.md:456:>
content/docs/1.7.0/references/settings.md:457:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.7.0/references/settings.md:461:> Default: `2048`
content/docs/1.7.0/references/settings.md:467:> Default: `1250`
content/docs/1.7.0/references/settings.md:471:> **Warning:**
content/docs/1.7.0/references/settings.md:472:>  - Specifying a value of 0 disables CPU requests for instance manager pods. You must specify an integer between 1000 and 8000. 
content/docs/1.7.0/references/settings.md:473:>  - This is a global setting. Modifying the value triggers an automatic restart of the Instance Manager pods. However, V2 Instance Manager pods that use this setting are restarted only when no instances are running.
content/docs/1.7.0/references/settings.md:477:> Default: `enabled`
content/docs/1.7.0/references/settings.md:489:> Default: `fast-check`
content/docs/1.7.0/references/settings.md:498:> Default: `false`
content/docs/1.7.0/references/settings.md:504:> Default: `0 0 */7 * *`
content/docs/1.7.0/references/settings.md:507:> **Warning**
content/docs/1.7.0/references/settings.md:508:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.7.0/references/settings.md:514:> Default: `false`
content/docs/1.7.0/references/settings.md:522:> Default: `false`
content/docs/1.7.0/references/settings.md:526:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.7.0/references/settings.md:530:> Examples:  
content/docs/1.7.0/references/settings.md:531:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.7.0/references/settings.md:532:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.7.0/references/settings.md:533:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.7.0/references/settings.md:539:> Example: `s3-secret`
content/docs/1.7.0/references/settings.md:545:> Default: `300`
content/docs/1.7.0/references/settings.md:553:> Default: `1440`
content/docs/1.7.0/references/settings.md:561:> Default: `1`
content/docs/1.7.0/references/settings.md:570:> Default: `1`
content/docs/1.7.0/references/settings.md:578:> Default: `false`
content/docs/1.7.0/references/settings.md:583:> Default: `ignored`
content/docs/1.7.0/references/settings.md:593:> Default: `lz4`
content/docs/1.7.0/references/settings.md:605:> Default: `2`
content/docs/1.7.0/references/settings.md:611:> Default: `2`
content/docs/1.7.0/references/settings.md:619:> Default: `true`
content/docs/1.7.0/references/settings.md:623:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.7.0/references/settings.md:627:> Default: `true`
content/docs/1.7.0/references/settings.md:635:> Default: `false`
content/docs/1.7.0/references/settings.md:641:> **Note:**
content/docs/1.7.0/references/settings.md:642:>   - This setting is superseded if replicas are forbidden to share a zone by the Replica Zone Level Anti-Affinity setting.
content/docs/1.7.0/references/settings.md:646:> Default: `true`
content/docs/1.7.0/references/settings.md:652:> **Note:**
content/docs/1.7.0/references/settings.md:653:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.7.0/references/settings.md:654:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.7.0/references/settings.md:658:> Default: `true`
content/docs/1.7.0/references/settings.md:664:> **Note:**
content/docs/1.7.0/references/settings.md:665:>   - Even if the setting is "true" and disk sharing is allowed, Longhorn will seek to use a different disk if possible, even if on the same node.
content/docs/1.7.0/references/settings.md:666:>   - This setting is superseded if replicas are forbidden to share a zone or a node by either of the other Soft Anti-Affinity settings.
content/docs/1.7.0/references/settings.md:670:> Default: `disabled`
content/docs/1.7.0/references/settings.md:686:> Default: `ignored`
content/docs/1.7.0/references/settings.md:700:> Default: `25`
content/docs/1.7.0/references/settings.md:708:> Default: `100`
content/docs/1.7.0/references/settings.md:718:> Default: `30`
content/docs/1.7.0/references/settings.md:726:> Default: `true`
content/docs/1.7.0/references/settings.md:732:> Default: `true`
content/docs/1.7.0/references/settings.md:760:> Default: `5`
content/docs/1.7.0/references/settings.md:766:> **WARNING:**
content/docs/1.7.0/references/settings.md:767:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.7.0/references/settings.md:768:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.7.0/references/settings.md:769:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.7.0/references/settings.md:774:> Example: `nodetype=storage:NoSchedule`
content/docs/1.7.0/references/settings.md:793:> Default: `longhorn-critical`
content/docs/1.7.0/references/settings.md:804:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.7.0/references/settings.md:810:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.7.0/references/settings.md:817:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.7.0/references/settings.md:823:> Default: `false`
content/docs/1.7.0/references/settings.md:829:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.7.0/references/settings.md:833:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.7.0/references/settings.md:837:> **Warning:** This setting should change after all Longhorn volumes are detached because some pods that run Longhorn system components are recreated to apply the setting. When all volumes are detached, Longhorn attempts to restart all Instance Manager and Backing Image Manager pods immediately. When volumes are in use, Longhorn components are not restarted, and you need to reconfigure the settings after detaching the remaining volumes; otherwise, you can wait for the setting change to be reconciled in an hour.
content/docs/1.7.0/references/settings.md:843:> Example: `false`
content/docs/1.7.0/references/settings.md:855:> Default: `12`
content/docs/1.7.0/references/settings.md:867:> **Warning:**
content/docs/1.7.0/references/settings.md:868:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.7.0/references/settings.md:869:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.7.0/references/settings.md:870:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.7.0/references/settings.md:871:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.7.0/references/settings.md:872:>  - After the setting is changed, the V1 Instance Manager pods that use this setting are automatically restarted when no instances are running.
content/docs/1.7.0/references/settings.md:876:> Default: `false`
content/docs/1.7.0/references/storage-class-parameters.md:52:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.7.0/references/storage-class-parameters.md:55:> Default: `true`  
content/docs/1.7.0/references/storage-class-parameters.md:56:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.7.0/references/storage-class-parameters.md:59:> Default: `Delete`  
content/docs/1.7.0/references/storage-class-parameters.md:60:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.7.0/references/storage-class-parameters.md:63:> Default `[]`  
content/docs/1.7.0/references/storage-class-parameters.md:64:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.7.0/references/storage-class-parameters.md:67:> Default `Immediate`  
content/docs/1.7.0/references/storage-class-parameters.md:68:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.7.0/references/storage-class-parameters.md:75:> Default: `3`  
content/docs/1.7.0/references/storage-class-parameters.md:81:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.7.0/references/storage-class-parameters.md:84:> Default: `30`
content/docs/1.7.0/references/storage-class-parameters.md:89:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:90:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.7.0/references/storage-class-parameters.md:95:> Default: `ext4`  
content/docs/1.7.0/references/storage-class-parameters.md:96:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../nodes-and-volumes/volumes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.7.0/references/storage-class-parameters.md:99:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:100:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../nodes-and-volumes/volumes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.7.0/references/storage-class-parameters.md:103:> Default: `false`  
content/docs/1.7.0/references/storage-class-parameters.md:108:> Default: `false`  
content/docs/1.7.0/references/storage-class-parameters.md:109:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.7.0/references/storage-class-parameters.md:112:> Default: `disabled`  
content/docs/1.7.0/references/storage-class-parameters.md:119:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.7.0/references/storage-class-parameters.md:120:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.7.0/references/storage-class-parameters.md:123:> Default: `ignored`  
content/docs/1.7.0/references/storage-class-parameters.md:129:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.7.0/references/storage-class-parameters.md:130:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.7.0/references/storage-class-parameters.md:133:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:134:> Example: `"ssd,fast"`  
content/docs/1.7.0/references/storage-class-parameters.md:137:> More details in [Storage Tags](../../nodes-and-volumes/nodes/storage-tags)
content/docs/1.7.0/references/storage-class-parameters.md:140:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:141:> Example: `"storage,fast"`  
content/docs/1.7.0/references/storage-class-parameters.md:144:> More details in [Storage Tags](../../nodes-and-volumes/nodes/storage-tags)
content/docs/1.7.0/references/storage-class-parameters.md:147:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:148:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.7.0/references/storage-class-parameters.md:151:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.7.0/references/storage-class-parameters.md:154:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:155:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.7.0/references/storage-class-parameters.md:158:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:159:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.7.0/references/storage-class-parameters.md:162:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:163:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.7.0/references/storage-class-parameters.md:166:> Default: `""`  
content/docs/1.7.0/references/storage-class-parameters.md:167:> See [Backing Image](../../advanced-resources/backing-image/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.7.0/references/storage-class-parameters.md:170:> Default: `ignored`  
content/docs/1.7.0/references/storage-class-parameters.md:175:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.7.0/references/storage-class-parameters.md:176:> More details in [Trim Filesystem](../../nodes-and-volumes/volumes/trim-filesystem).
content/docs/1.7.0/references/storage-class-parameters.md:179:> Default: `false`  
content/docs/1.7.0/references/storage-class-parameters.md:181:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.7.0/references/storage-class-parameters.md:182:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.7.0/references/storage-class-parameters.md:185:> Default: `ignored`  
content/docs/1.7.0/references/storage-class-parameters.md:190:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.7.0/references/storage-class-parameters.md:191:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.7.0/references/storage-class-parameters.md:194:> Default: `ignored`  
content/docs/1.7.0/references/storage-class-parameters.md:199:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.7.0/references/storage-class-parameters.md:200:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling).
content/docs/1.7.0/references/storage-class-parameters.md:203:> Default: `ignored`  
content/docs/1.7.0/references/storage-class-parameters.md:208:> Global setting: [Replica Disk Level Soft Anti-Affinity](../settings#replica-disk-level-soft-anti-affinity).  
content/docs/1.7.0/references/storage-class-parameters.md:209:> More details in [Scheduling](../../nodes-and-volumes/nodes/scheduling).
content/docs/1.7.0/references/storage-class-parameters.md:212:> Default: `""`
content/docs/1.7.0/references/storage-class-parameters.md:213:> Example: `"hard,sync"`  
content/docs/1.7.0/references/storage-class-parameters.md:218:> More details in [RWX Workloads](../../nodes-and-volumes/volumes/rwx-volumes#configuring-volume-mount-options)
content/docs/1.7.0/references/storage-class-parameters.md:221:> Default: `"v1"`  
content/docs/1.7.0/references/storage-class-parameters.md:225:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.7.0/references/storage-class-parameters.md:226:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:27:        storage: <size> # must match size of Longhorn volume
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:36:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:47:        storage: <size>  # must match size of Longhorn volume
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:56:          numberOfReplicas: <replicas> # must match Longhorn volume value
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/restore-statefulset.md:64:    <name of Volume Claim Template>-<name of StatefulSet>-<index>
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:50:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:51:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:61:       kubectl create secret generic <aws-secret> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:62:           --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:63:           --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:87:               "arn:aws:s3:::<your-bucket-name>",
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:88:               "arn:aws:s3:::<your-bucket-name>/*"
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:111:               "AWS": "arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_EC2_NODE_INSTANCE_ROLE>"
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:122:       kubectl create secret generic <aws-secret> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:123:           --from-literal=AWS_IAM_ROLE_ARN=<your-aws-iam-role-arn> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:130:    s3://<your-bucket-name>@<your-aws-region>/
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:136:    s3://<your-bucket-name>@<your-aws-region>/mypath/
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:139:   Also make sure you've set **`<your-aws-region>` in the URL**.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:156:kubectl create secret generic <aws-secret> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:157:    --from-literal=AWS_ACCESS_KEY_ID=<your-aws-access-key-id> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:158:    --from-literal=AWS_SECRET_ACCESS_KEY=<your-aws-secret-access-key> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:159:    --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:160:    --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:161:    --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:264:   > Note: Make sure to use `echo -n` when generating the base64 encoding, otherwise an new line will be added at the end of the string and it will cause error when accessing the S3.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:334:  cat <<EOF >>cifs_secret.yml
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:349:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:402:    cat <<EOF >>azblob_secret.yml
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:423:    kubectl create secret generic <azblob-secret> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:424:      --from-literal=AZBLOB_ACCOUNT_NAME=<your-azure-storage-account-name> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:425:      --from-literal=AZBLOB_ACCOUNT_KEY=<your-azure-storage-account-key> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:426:      --from-literal=HTTP_PROXY=<your-proxy-ip-and-port> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:427:      --from-literal=HTTPS_PROXY=<your-proxy-ip-and-port> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:428:      --from-literal=NO_PROXY=<excluded-ip-list> \
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:432:Then, navigate to Longhorn UI > Setting > General > Backup
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:446:   - If you set `<endpoint-suffix>` in the URL, the default endpoint suffix will be `core.windows.net`.
content/docs/1.7.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:447:   - If you set `AZBLOB_ENDPOINT` in the secret, Longhorn will use `AZBLOB_ENDPOINT` as your storage URL, and `<endpoint-suffix>` will not be used even if it has been set. 
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:52:To see the BackingImage, click **Setting > Backing Image** in the top navigation bar and click the BackingImage mentioned in the `VolumeSnapshotContent.snapshotHandle`.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:108:The parameters can be retrieved from the **Setting > Backing Image** page in the Longhorn UI.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.7.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.7.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.7.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.7.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:76:    > **Note:** retain value has no effect for this task, Longhorn automatically mutates the `retain` value to 0.
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:79:    > **Note:** The `retain` value is independent of each recurring job.
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:80:    >
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:81:    > Using a volume with 2 recurring jobs as an example:
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:82:    > - `snapshot` with retain value set to 5
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:83:    > - `snapshot-delete`: with retain value set to 2
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:84:    >
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:85:    > Eventually, there will be 2 snapshots retained after a complete `snapshot-delete` task execution.
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:121:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:129:kubectl -n longhorn-system label volume/<VOLUME-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:137:kubectl -n longhorn-system label volume/<VOLUME-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:149:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/source=enabled
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:157:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job-group.longhorn.io/<RECURRING-JOB-GROUP-NAME>=enabled
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:165:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> recurring-job.longhorn.io/<RECURRING-JOB-NAME>=enabled
content/docs/1.7.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:173:kubectl -n <NAMESPACE> label pvc/<PVC-NAME> <RECURRING-JOB-LABEL>-
content/docs/1.7.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.7.0/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.7.0/troubleshoot/troubleshooting.md:28:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.7.0/v2-data-engine/prerequisites.md:13:  > **NOTICE**
content/docs/1.7.0/v2-data-engine/prerequisites.md:14:  >
content/docs/1.7.0/v2-data-engine/prerequisites.md:15:  >  AMD64 CPUs require SSE4.2 instruction support.
content/docs/1.7.0/v2-data-engine/prerequisites.md:20:  > **NOTICE**
content/docs/1.7.0/v2-data-engine/prerequisites.md:21:  >
content/docs/1.7.0/v2-data-engine/prerequisites.md:22:  > Host machines with Linux kernel 5.15 may unexpectedly reboot when volume-related IO errors occur. Update the Linux kernel on Longhorn nodes to version 5.19 or later to prevent such issues.
content/docs/1.7.0/v2-data-engine/quick-start.md:75:  echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
content/docs/1.7.0/v2-data-engine/quick-start.md:80:  echo "vm.nr_hugepages=1024" >> /etc/sysctl.conf
content/docs/1.7.0/v2-data-engine/quick-start.md:125:Or, you can enable it in `Setting > General > V2 Data Engine`.
content/docs/1.7.0/v2-data-engine/quick-start.md:156:You can observe the utilization of allocated huge pages on each node by running the command `kubectl get node <node name> -o yaml`.
content/docs/1.7.0/v2-data-engine/quick-start.md:203:kubectl -n longhorn-system edit node.longhorn.io <NODE NAME>
content/docs/1.7.0/v2-data-engine/quick-start.md:208:<DISK NAME>:
content/docs/_index.md:9:<!-- {{< version-list >}} -->
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:13:* Longhorn v1.2.x that are < v1.2.6
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:14:* Longhorn v1.3.x that are < v1.3.2
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:107:(i.e., running `blockdev --getpbsz /dev/longhorn/<longhorn-volume-name>` returns `512`).
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:121:DEV=/dev/longhorn/<volume-name>
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:135:Deploy any Longhorn version < master
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:140:1. Format the block device `mkfs.ext4 /dev/longhorn/<volume-name> -b 1024`
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:148:1. Format the block device `mkfs.xfs /dev/longhorn/<volume-name>`
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:171:1. Find the major and minor version of the block device by `ls -l /dev/longhorn/<volume-name>`
content/kb/analysis-filesystem-corrupted-issues-due-to-error-on-cow-while-rebuilding-replicas.md:187:1. Run the above `mounting script` with the correct block device `/dev/.../<root-partition-name>`
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:23:Now, you have already upgraded Rancher to the version >= v2.6.3.
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:26:You can still find your old Longhorn installation by going to Downstream Cluster ->
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:27:Legacy -> Project -> select the correct project name from the dropdown on the top -> Apps. For example:
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:41:2. Rancher >= v2.6.3 versions
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:47:1. Go to Longhorn UI -> Settings -> General -> Change Concurrent Automatic Engine Upgrade Per Node Limit setting to 0 to disable auto engine upgrade.
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:158:      1. Go to Rancher UI -> Go to the downstream cluster -> App&Marketplace -> Charts
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:166:4. Go to Rancher UI -> Go to the downstream cluster -> App&Marketplace -> Charts -> Find and select Longhorn chart
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:167:   -> Select the chart version corresponding to the correct Longhorn version
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:168:   -> Install the chart with the correct helm values.
content/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui.md:170:5. Verify that the migrated chart (in Rancher UI -> Downstream Cluster -> App & Marketplace -> Installed Apps) is working ok. E.g., check all Longhorn resources inside the namespace `longhorn-system`,
content/kb/selinux-and-longhorn.md:69:-> ps -eZf | grep spc_t | grep longhorn
content/kb/selinux-and-longhorn.md:70:system_u:system_r:spc_t:s0      root      3146  2558  0 20:10 ?        00:00:00 /bin/bash -c diff /usr/local/bin/longhorn /data/longhorn > /dev/null 2>&1; if [ $? -ne 0 ]; then cp -p /usr/local/bin/longhorn /data/ && echo installed; fi && trap 'rm /data/longhorn* && echo cleaned up' EXIT && sleep infinity
content/kb/selinux-and-longhorn.md:86:-> ps -eZf | grep container_t | grep longhorn
content/kb/selinux-and-longhorn.md:118:-> sesearch -A -s container_t -t spc_t -p connectto
content/kb/selinux-and-longhorn.md:123:-> seinfo -x -a container_domain
content/kb/selinux-and-longhorn.md:157:-> ausearch -m AVC -ts recent
content/kb/selinux-and-longhorn.md:158:time->Tue Nov 21 19:51:01 2023
content/kb/selinux-and-longhorn.md:161:time->Tue Nov 21 19:51:01 2023
content/kb/selinux-and-longhorn.md:164:time->Tue Nov 21 19:51:01 2023
content/kb/selinux-and-longhorn.md:167:time->Tue Nov 21 19:51:01 2023
content/kb/troubleshooting-create-support-bundle-with-curl.md:26:    longhorn-backend   NodePort   10.43.136.157       <none>        9500:32595/TCP   156m
content/kb/troubleshooting-create-support-bundle-with-curl.md:32:    # Replace this block ====>
content/kb/troubleshooting-create-support-bundle-with-curl.md:37:    # <==== Replace this block
content/kb/troubleshooting-create-support-bundle-with-curl.md:42:    ID=$( jq -r '.id' <<< ${REQUEST_SUPPORT_BUNDLE} )
content/kb/troubleshooting-create-support-bundle-with-curl.md:43:    SUPPORT_BUNDLE_NAME=$( jq -r '.name' <<< ${REQUEST_SUPPORT_BUNDLE} )
content/kb/troubleshooting-default-settings-do-not-persist.md:12:Longhorn version < v1.3.0.
content/kb/troubleshooting-default-settings-do-not-persist.md:30:kubectl edit settings <SETTING-NAME> -n longhorn-system
content/kb/troubleshooting-dns-resolution-failed.md:38:   kubectl exec -it <longhorn-manager-pod-name> -- nslookup longhorn-backend
content/kb/troubleshooting-engine-upgrading-stuck-in-deadlock.md:12:This happens when users upgrade Longhorn from version <= v1.1.1 to a newer version.
content/kb/troubleshooting-engine-upgrading-stuck-in-deadlock.md:23:There is a bug Longhorn version <= v1.1.1 which leads to a deadlock in the instance manager pods.
content/kb/troubleshooting-engine-upgrading-stuck-in-deadlock.md:25:When you upgrade Longhorn from version <= v1.1.1 to a newer version, you may hit this bug in a cluster with a few hundred volumes.
content/kb/troubleshooting-engine-upgrading-stuck-in-deadlock.md:31:We fixed this bug in Longhorn version >= v1.1.2.
content/kb/troubleshooting-engine-upgrading-stuck-in-deadlock.md:32:If you are planning to upgrade Longhorn to a version >= v1.1.2, you can follow the following steps to avoid the bug:
content/kb/troubleshooting-engine-upgrading-stuck-in-deadlock.md:42:   For example, this drain command skips Longhorn components `kubectl drain --pod-selector='!longhorn.io/component,app!=csi-attacher,app!=csi-provisioner,app!=csi-snapshotter,app!=csi-resizer,app!=longhorn-driver-deployer,app!=longhorn-ui' <NODE-NAME> --ignore-daemonsets`
content/kb/troubleshooting-engine-upgrading-stuck-in-deadlock.md:67:        grpcurl -d '' -plaintext -import-path ./ -proto health.proto <INSTANCE-MANAGER-IP>:8500 grpc.health.v1.Health/Check
content/kb/troubleshooting-engine-upgrading-stuck-in-deadlock.md:69:        grpcurl -d '' -plaintext -import-path ./ -proto rpc.proto <INSTANCE-MANAGER-IP>:8500 ProcessManagerService/ProcessList
content/kb/troubleshooting-failed-rwx-mount-due-to-connection-timeout.md:23:- Any Kubernetes distribution running Flannel CNI < `v0.20.2`
content/kb/troubleshooting-failed-rwx-mount-due-to-connection-timeout.md:57:-> kubectl get -oyaml -n longhorn-system volume | grep shareEndpoint
content/kb/troubleshooting-failed-rwx-mount-due-to-connection-timeout.md:61:-> SSH <user>@<node_ip>
content/kb/troubleshooting-failed-rwx-mount-due-to-connection-timeout.md:64:-> time nsenter -t 1 -m -n -u nc -z 10.43.78.204 2049
content/kb/troubleshooting-failed-rwx-mount-due-to-connection-timeout.md:72:There is a double-NAT bug in Flannel CNI < `v0.20.2` that can slow down connections and even cause packet loss,
content/kb/troubleshooting-failed-rwx-mount-due-to-connection-timeout.md:91:The Flannel bug is fixed in `v0.20.2`. All K3s and RKE2 minor versions >= `v1.24` have a patch version with the fix. For
content/kb/troubleshooting-failed-rwx-mount-due-to-connection-timeout.md:95:Flannel >= `v0.20.2`. If you are using a different VXLAN-based CNI, consult the documentation for that CNI.
content/kb/troubleshooting-failure-to-delete-orphaned-pod-volume-directory.md:20:orphaned pod <pod-uid> found, but error not a directory occurred when trying to remove the volumes dir
content/kb/troubleshooting-failure-to-delete-orphaned-pod-volume-directory.md:31:However, in cases where the Pod and its associated volume are rescheduled to a different node, leaving behind a lingering `vol_data.json` file on the crashed node, manual intervention is required. You will need to manually delete the `vol_data.json` file located within the `/var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/pvc_<pod-uid>/` directory.
content/kb/troubleshooting-fstrim-error.md:12:* Longhorn version >= `v1.4.0`
content/kb/troubleshooting-fstrim-error.md:13:* Node kernel version <= `v4.12`
content/kb/troubleshooting-fstrim-error.md:35:1. Double-check the engine image of the volume is >= `v1.4.0` in the volume detail page in Longhorn UI. Only engine
content/kb/troubleshooting-fstrim-error.md:36:image >= `v1.4.0` supports filesystem trim feature.
content/kb/troubleshooting-fstrim-error.md:37:1. If the volume's engine's image was recently live upgraded from a version < `v1.4.0` to >= `v1.4.0`, make sure that the
content/kb/troubleshooting-fstrim-error.md:45:1. Find the major and minor of a Longhorn block device by `ls -l /dev/longhorn/<volume-name>`. For example:
content/kb/troubleshooting-fstrim-error.md:65:1. Check the kernel version by `uname -r`. If the kernel version is < `v4.12`, you have hit this issue
content/kb/troubleshooting-fstrim-error.md:70:At the time when writing this article, the recommended kernel version is >= `v5.8`
content/kb/troubleshooting-generate-pprof-runtime-profiling-data.md:12:Longhorn >= v1.1.2.
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:19:> For every CronJob, the CronJob Controller checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:20:>
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:21:>```
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:22:>Cannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:23:>```
content/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4.md:22:After restoring the PVC via Velero with its CSI plugin (< 0.4), it will only remove the `volume.beta.kubernetes.io/storage-provisioner` but keep the `volume.kubernetes.io/storage-provisioner` annotation intact from the PVC, because the plugin doesn't respect the general available `volume.kubernetes.io/storage-provisioner` annotation. Because Kubernetes will not add `volume.kubernetes.io/storage-provisioner` to the PVC which already has the beta annotation, it will cause the restoring PVC will be failed to be processed by the built-in Longhorn CSI provisioner and be stuck in the `Pending` state.
content/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4.md:28:It is recommended to use the Velero CSI plugin version >= 0.4 for PVC backup and restore because it is compatible with different storage-provisioner annotations supported by different versions of CSI Provisioner. 
content/kb/troubleshooting-some-old-instance-manager-pods-are-still-running-after-upgrade.md:12:Longhorn >= v0.8.0.
content/kb/troubleshooting-some-old-instance-manager-pods-are-still-running-after-upgrade.md:73:- The user can check which volumes are still using the old engine manager pods from `Volume > Name > Volume Details > Instance Manager` on Longhorn UI.
content/kb/troubleshooting-volume-attachment-fails-due-to-selinux-denials.md:26:-> dmesg
content/kb/troubleshooting-volume-attachment-fails-due-to-selinux-denials.md:32:-> ausearch -m AVC -ts recent
content/kb/troubleshooting-volume-attachment-fails-due-to-selinux-denials.md:35:time->Wed Jun  7 19:33:17 2023
content/kb/troubleshooting-volume-attachment-fails-due-to-selinux-denials.md:68:echo '(allow iscsid_t self (capability (dac_override)))' > local_longhorn.cil && semodule -vi local_longhorn.cil
content/kb/troubleshooting-volume-pvc-xxx-not-scheduled.md:18:When checking for error message using `kubectl describe <pod>`, the following message is shown:
content/kb/troubleshooting-volume-with-multipath.md:21:time="2020-04-16T08:49:27Z" level=info msg="NodeServer NodePublishVolume req: volume_id:\"pvc-d061512e-870a-4ece-bd45-2f04672d5256\" target_path:\"/var/lib/kubelet/pods/cf0a0b5b-106e-4793-a74a-28bfae21be1a/volumes/kubernetes.io~csi/pvc-d061512e-870a-4ece-bd45-2f04672d5256/mount\" volume_capability:<mount:<fs_type:\"ext4\" > access_mode:<mode:SINGLE_NODE_WRITER > > volume_context:<key:\"baseImage\" value:\"\" > volume_context:<key:\"fromBackup\" value:\"\" > volume_context:<key:\"numberOfReplicas\" value:\"3\" > volume_context:<key:\"staleReplicaTimeout\" value:\"30\" > volume_context:<key:\"storage.kubernetes.io/csiProvisionerIdentity\" value:\"1586958032802-8081-driver.longhorn.io\" > "
content/kb/troubleshooting-volume-with-multipath.md:101:> The default configurations for multipath blacklist section is preventing the following device names by default
content/kb/troubleshooting-volume-with-multipath.md:102:> ^(ram|raw|loop|fd|md|dm-|sr|scd|st|dcssblk)[0-9]
content/kb/troubleshooting-volume-with-multipath.md:103:> ^(td|hd|vd)[a-z]
content/kb/troubleshooting-websocket_handshake_error_with_unexpected_response_code_200.md:12:Existing Longhorn versions < v1.1.0 upgrade to Longhorn >= v1.1.0.
content/kb/troubleshooting-websocket_handshake_error_with_unexpected_response_code_200.md:16:After upgrading Longhorn to version >= v1.1.0, the following is encountered:
content/kb/troubleshooting-websocket_handshake_error_with_unexpected_response_code_200.md:42:As result, `<LONGHORN_URL>/<TAG>` changes to `<LONGHORN_URL>/#/<TAG>`.
content/kb/troubleshooting-websocket_handshake_error_with_unexpected_response_code_200.md:84:2. Access/Rebookmark Longhorn URL from `<LONGHORN_URL>/#`.
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:49:    Expansion Error: the expected size <small_size> of engine <engine> should not be smaller than the current size <large_size>. You can cancel the expansion to avoid volume crash.
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:53:    unable to cancel expansion for volume <volume>: volume expansion is not started
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:60:    <time> time="<time>" level=error msg="failed to prune <snapshot>.img based on <snapshot>.img: file sizes are not
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:67:    <time> [longhorn-instance-manager] time="<time>" level=debug msg="Adding replica <replica_address>" 
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:68:    currentSize=<size> restore=false serviceURL="<engine_address>" size=<size>
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:69:    <time> [longhorn-instance-manager] time="<time>" level=info msg="Prepare to expand new replica to size <size>"
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:70:    <time> [longhorn-instance-manager] time="<time>" level=info msg="Adding replica <replica_address> in WO mode"
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:74:    <time> [<replica>] time="<time>" level=info msg="Replica server starts to expand to size <large_size>"
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:80:    E<date> <time> 1 engine_controller.go:731] failed to update status for engine <engine>: BUG: The expected size
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:81:    <small_size> of engine <engine> should not be smaller than the current size <large_size>
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:87:    <time> time="<time>" level=error msg="Failed rebuilding of replica <replica_address>" controller=longhorn-engine
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:88:    engine=<engine> error="proxyServer=<instance_manager_address> destination=<engine_address>: failed to add replica
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:89:    <replica_address> for volume: rpc error: code = Unknown desc = failed to create replica <replica_address> for volume
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:90:    <engine_address>: rpc error: code = Unknown desc = replica must be closed, Can not add in state: dirty" node=<node>
content/kb/troubleshooting-unexpected-expansion-leads-to-degradation-or-attach-failure.md:91:    volume=<volume>
content/kb/troubleshooting-volume-filesystem-corruption.md:44:> **Warning**
content/kb/troubleshooting-volume-filesystem-corruption.md:45:> When a file system check tool fixes errors, it modifies the filesystem metadata and brings the filesystem to a
content/kb/troubleshooting-volume-filesystem-corruption.md:51:5. Find the block device corresponding to the Longhorn volume under `/dev/longhorn/<volume-name>`.
content/kb/troubleshooting-volume-filesystem-corruption.md:65:-> fsck.ext4 /dev/longhorn/pvc-c7152ef5-55c7-43ce-a35e-dac69d2be591 
content/kb/troubleshooting-volume-filesystem-corruption.md:83:> **Warning**
content/kb/troubleshooting-volume-filesystem-corruption.md:84:> When a file system check tool fixes errors, it modifies the filesystem metadata and brings the filesystem to a
content/kb/troubleshooting-volume-filesystem-corruption.md:91:   `kubectl exec -it -n longhorn-system instance-manager-<additional-characters> -- bash`
content/kb/troubleshooting-volume-filesystem-corruption.md:92:5. Find the block device corresponding to the Longhorn volume under `/dev/longhorn/<volume-name>`.
content/kb/troubleshooting-volume-filesystem-corruption.md:102:-> kl exec -it instance-manager-e-545c3360290f259fb0fe5638303b8f9a bash
content/kb/troubleshooting-volume-readonly-or-io-error.md:17:/data # echo test > test
content/kb/troubleshooting-volume-readonly-or-io-error.md:33:When checking the event using `kubectl -n longhorn-system get event | grep <volume name>`, an event like the following is shown:
content/kb/troubleshooting-volume-readonly-or-io-error.md:38:When checking for logs of the Longhorn manager pods on the node the workload is running on by running `kubectl -n longhorn-system logs <longhorn manager pod name> | grep <volume name>`, the following message is shown:
content/kb/troubleshooting-unable-to-mount-an-nfs-backup-target.md:93:  - <https://github.com/longhorn/longhorn/issues/3576>
content/kb/troubleshooting-unable-to-mount-an-nfs-backup-target.md:94:  - <https://github.com/longhorn/longhorn/discussions/3805>
