content/blog/longhorn-v1.2.md:186:> Longhorn will automatically add the volume to the default group when the volume has no recurring job
content/blog/longhorn-v1.2.md:187:>
content/blog/performance-scalability-report-aug-2020.md:31:>Disk: 200 GiB NVMe SSD as the instance store.
content/blog/performance-scalability-report-aug-2020.md:32:>
content/blog/performance-scalability-report-aug-2020.md:33:>CPU: 8 vCPUs (Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz)
content/blog/performance-scalability-report-aug-2020.md:34:>
content/blog/performance-scalability-report-aug-2020.md:35:>Memory: 16 GB
content/blog/performance-scalability-report-aug-2020.md:36:>
content/blog/performance-scalability-report-aug-2020.md:37:>Network: Up to 10Gbps
content/blog/performance-scalability-report-aug-2020.md:45:>Kubernetes: v1.17.5.
content/blog/performance-scalability-report-aug-2020.md:46:>
content/blog/performance-scalability-report-aug-2020.md:47:>Node OS: 5.3.0-1023-aws #25~18.04.1-Ubuntu SMP
content/blog/performance-scalability-report-aug-2020.md:48:>
content/blog/performance-scalability-report-aug-2020.md:49:>Longhorn: v1.0.1
content/blog/performance-scalability-report-aug-2020.md:86:>CPU: 8 vCPUs
content/blog/performance-scalability-report-aug-2020.md:87:>
content/blog/performance-scalability-report-aug-2020.md:88:>Memory: 32 GB Memory
content/blog/performance-scalability-report-aug-2020.md:89:>
content/blog/performance-scalability-report-aug-2020.md:97:>Kubernetes: v1.18.6, installed using Rancher
content/blog/performance-scalability-report-aug-2020.md:98:>
content/blog/performance-scalability-report-aug-2020.md:99:>Longhorn v1.0.1
content/docs/1.4.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.0/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.0/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.0/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.0/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.0/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.0/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.0/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.0/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.0/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.0/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.0/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:84:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:122:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.0/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.0/best-practices.md:123:> Recommend: `false`
content/docs/1.4.0/best-practices.md:129:> Recommend: `false`
content/docs/1.4.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.0/deploy/install/install-with-helm.md:15:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.0/deploy/install/install-with-helm.md:20:> **Note**:
content/docs/1.4.0/deploy/install/install-with-helm.md:21:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.0/deploy/install/install-with-helm.md:22:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.0/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.0/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.0/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.0/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.0/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.0/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.0/references/settings.md:83:> Default: `false`
content/docs/1.4.0/references/settings.md:91:> Default: `true`
content/docs/1.4.0/references/settings.md:97:> Default: `true`
content/docs/1.4.0/references/settings.md:104:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.0/references/settings.md:105:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.0/references/settings.md:106:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.0/references/settings.md:110:> Default: `true`
content/docs/1.4.0/references/settings.md:116:> Default: `0`
content/docs/1.4.0/references/settings.md:124:> Default: `5`
content/docs/1.4.0/references/settings.md:134:> Default: `false`
content/docs/1.4.0/references/settings.md:144:> Default: `longhorn.io/v1beta2`
content/docs/1.4.0/references/settings.md:150:> Default: `disabled`
content/docs/1.4.0/references/settings.md:168:> Default: `/var/lib/longhorn/`
content/docs/1.4.0/references/settings.md:186:> Default: `longhorn-static`
content/docs/1.4.0/references/settings.md:192:> Default: `3`
content/docs/1.4.0/references/settings.md:207:> Default: `false`
content/docs/1.4.0/references/settings.md:211:> Default: `false`
content/docs/1.4.0/references/settings.md:217:> Default: `true`
content/docs/1.4.0/references/settings.md:229:> Default: `do-nothing`
content/docs/1.4.0/references/settings.md:244:> Default: `600`
content/docs/1.4.0/references/settings.md:252:> Default: `if-not-present`
content/docs/1.4.0/references/settings.md:268:> Default: `60`
content/docs/1.4.0/references/settings.md:273:> Default: `300`
content/docs/1.4.0/references/settings.md:277:> **Note:**
content/docs/1.4.0/references/settings.md:278:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.0/references/settings.md:279:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.0/references/settings.md:282:> Default: `8`
content/docs/1.4.0/references/settings.md:293:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.0/references/settings.md:297:> Default: `1`
content/docs/1.4.0/references/settings.md:307:> Default: `false`
content/docs/1.4.0/references/settings.md:313:> Default: `30`
content/docs/1.4.0/references/settings.md:321:> Default: `fast-check`
content/docs/1.4.0/references/settings.md:330:> Default: `false`
content/docs/1.4.0/references/settings.md:336:> Default: `0 0 */7 * *`
content/docs/1.4.0/references/settings.md:339:> **Warning**
content/docs/1.4.0/references/settings.md:340:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.0/references/settings.md:346:> Default: `false`
content/docs/1.4.0/references/settings.md:354:> Default: `false`
content/docs/1.4.0/references/settings.md:358:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.0/references/settings.md:362:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.0/references/settings.md:368:> Example: `s3-secret`
content/docs/1.4.0/references/settings.md:374:> Default: `300`
content/docs/1.4.0/references/settings.md:382:> Default: `1440`
content/docs/1.4.0/references/settings.md:390:> Default: `1`
content/docs/1.4.0/references/settings.md:399:> Default: `1`
content/docs/1.4.0/references/settings.md:407:> Default: `false`
content/docs/1.4.0/references/settings.md:412:> Default: `ignored`
content/docs/1.4.0/references/settings.md:424:> Default: `true`
content/docs/1.4.0/references/settings.md:428:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.0/references/settings.md:432:> Default: `true`
content/docs/1.4.0/references/settings.md:440:> Default: `false`
content/docs/1.4.0/references/settings.md:448:> Default: `true`
content/docs/1.4.0/references/settings.md:454:> **Note:**
content/docs/1.4.0/references/settings.md:455:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.0/references/settings.md:456:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.0/references/settings.md:460:> Default: `disabled`
content/docs/1.4.0/references/settings.md:476:> Default: `ignored`
content/docs/1.4.0/references/settings.md:490:> Default: `25`
content/docs/1.4.0/references/settings.md:498:> Default: `200`
content/docs/1.4.0/references/settings.md:510:> Default: `5`
content/docs/1.4.0/references/settings.md:516:> **WARNING:**
content/docs/1.4.0/references/settings.md:517:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.0/references/settings.md:518:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.0/references/settings.md:519:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.0/references/settings.md:524:> Default: `12`
content/docs/1.4.0/references/settings.md:536:> **Warning:**
content/docs/1.4.0/references/settings.md:537:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.0/references/settings.md:538:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.0/references/settings.md:539:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.0/references/settings.md:540:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.0/references/settings.md:541:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.0/references/settings.md:545:> Default: `12`
content/docs/1.4.0/references/settings.md:557:> **Warning:**
content/docs/1.4.0/references/settings.md:558:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.0/references/settings.md:559:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.0/references/settings.md:560:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.0/references/settings.md:561:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.0/references/settings.md:562:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.0/references/settings.md:567:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.0/references/settings.md:586:> Example: `high-priority`
content/docs/1.4.0/references/settings.md:597:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.0/references/settings.md:603:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.0/references/settings.md:610:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.0/references/settings.md:616:> Default: `false`
content/docs/1.4.0/references/settings.md:622:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.0/references/settings.md:626:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.0/references/settings.md:630:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.0/references/settings.md:636:> Example: `false`
content/docs/1.4.0/references/settings.md:650:> Default: `false`
content/docs/1.4.0/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.0/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.0/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.0/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.0/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.0/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.0/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.0/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.0/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.0/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.0/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.0/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.0/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.0/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.0/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.0/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.0/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.0/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.0/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.0/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.0/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.0/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.0/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.0/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.0/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.0/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.0/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.0/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.0/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.0/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.0/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.0/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.0/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.0/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.0/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.0/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.0/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.0/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.0/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.0/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.0/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.0/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.0/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.0/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.0/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.0/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.0/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.0/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.0/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.0/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.0/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.0/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.0/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.0/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.0/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.1/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.1/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.1/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.1/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.1/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.1/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.1/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.1/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.1/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.1/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.1/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.1/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.1/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.1/best-practices.md:123:> Recommend: `false`
content/docs/1.4.1/best-practices.md:129:> Recommend: `false`
content/docs/1.4.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.1/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.1/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.1/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.1/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.1/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.1/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.1/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.1/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.1/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.1/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.1/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.1/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.1/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.1/references/settings.md:83:> Default: `false`
content/docs/1.4.1/references/settings.md:91:> Default: `true`
content/docs/1.4.1/references/settings.md:97:> Default: `true`
content/docs/1.4.1/references/settings.md:104:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.1/references/settings.md:105:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.1/references/settings.md:106:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.1/references/settings.md:110:> Default: `true`
content/docs/1.4.1/references/settings.md:116:> Default: `0`
content/docs/1.4.1/references/settings.md:124:> Default: `5`
content/docs/1.4.1/references/settings.md:134:> Default: `false`
content/docs/1.4.1/references/settings.md:144:> Default: `longhorn.io/v1beta2`
content/docs/1.4.1/references/settings.md:150:> Default: `disabled`
content/docs/1.4.1/references/settings.md:168:> Default: `/var/lib/longhorn/`
content/docs/1.4.1/references/settings.md:186:> Default: `longhorn-static`
content/docs/1.4.1/references/settings.md:192:> Default: `3`
content/docs/1.4.1/references/settings.md:207:> Default: `false`
content/docs/1.4.1/references/settings.md:211:> Default: `false`
content/docs/1.4.1/references/settings.md:217:> Default: `true`
content/docs/1.4.1/references/settings.md:229:> Default: `do-nothing`
content/docs/1.4.1/references/settings.md:244:> Default: `600`
content/docs/1.4.1/references/settings.md:252:> Default: `if-not-present`
content/docs/1.4.1/references/settings.md:268:> Default: `60`
content/docs/1.4.1/references/settings.md:273:> Default: `300`
content/docs/1.4.1/references/settings.md:277:> **Note:**
content/docs/1.4.1/references/settings.md:278:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.1/references/settings.md:279:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.1/references/settings.md:282:> Default: `8`
content/docs/1.4.1/references/settings.md:293:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.1/references/settings.md:297:> Default: `1`
content/docs/1.4.1/references/settings.md:307:> Default: `false`
content/docs/1.4.1/references/settings.md:313:> Default: `30`
content/docs/1.4.1/references/settings.md:321:> Default: `fast-check`
content/docs/1.4.1/references/settings.md:330:> Default: `false`
content/docs/1.4.1/references/settings.md:336:> Default: `0 0 */7 * *`
content/docs/1.4.1/references/settings.md:339:> **Warning**
content/docs/1.4.1/references/settings.md:340:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.1/references/settings.md:346:> Default: `false`
content/docs/1.4.1/references/settings.md:354:> Default: `false`
content/docs/1.4.1/references/settings.md:358:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.1/references/settings.md:362:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.1/references/settings.md:368:> Example: `s3-secret`
content/docs/1.4.1/references/settings.md:374:> Default: `300`
content/docs/1.4.1/references/settings.md:382:> Default: `1440`
content/docs/1.4.1/references/settings.md:390:> Default: `1`
content/docs/1.4.1/references/settings.md:399:> Default: `1`
content/docs/1.4.1/references/settings.md:407:> Default: `false`
content/docs/1.4.1/references/settings.md:412:> Default: `ignored`
content/docs/1.4.1/references/settings.md:424:> Default: `true`
content/docs/1.4.1/references/settings.md:428:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.1/references/settings.md:432:> Default: `true`
content/docs/1.4.1/references/settings.md:440:> Default: `false`
content/docs/1.4.1/references/settings.md:448:> Default: `true`
content/docs/1.4.1/references/settings.md:454:> **Note:**
content/docs/1.4.1/references/settings.md:455:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.1/references/settings.md:456:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.1/references/settings.md:460:> Default: `disabled`
content/docs/1.4.1/references/settings.md:476:> Default: `ignored`
content/docs/1.4.1/references/settings.md:490:> Default: `25`
content/docs/1.4.1/references/settings.md:498:> Default: `200`
content/docs/1.4.1/references/settings.md:510:> Default: `5`
content/docs/1.4.1/references/settings.md:516:> **WARNING:**
content/docs/1.4.1/references/settings.md:517:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.1/references/settings.md:518:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.1/references/settings.md:519:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.1/references/settings.md:524:> Default: `12`
content/docs/1.4.1/references/settings.md:536:> **Warning:**
content/docs/1.4.1/references/settings.md:537:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.1/references/settings.md:538:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.1/references/settings.md:539:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.1/references/settings.md:540:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.1/references/settings.md:541:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.1/references/settings.md:545:> Default: `12`
content/docs/1.4.1/references/settings.md:557:> **Warning:**
content/docs/1.4.1/references/settings.md:558:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.1/references/settings.md:559:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.1/references/settings.md:560:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.1/references/settings.md:561:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.1/references/settings.md:562:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.1/references/settings.md:567:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.1/references/settings.md:586:> Example: `high-priority`
content/docs/1.4.1/references/settings.md:597:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.1/references/settings.md:603:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.1/references/settings.md:610:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.1/references/settings.md:616:> Default: `false`
content/docs/1.4.1/references/settings.md:622:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.1/references/settings.md:626:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.1/references/settings.md:630:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.1/references/settings.md:636:> Example: `false`
content/docs/1.4.1/references/settings.md:650:> Default: `false`
content/docs/1.4.1/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.1/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.1/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.1/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.1/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.1/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.1/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.1/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.1/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.1/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.1/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.1/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.1/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.1/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.1/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.1/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.1/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.1/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.1/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.1/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.1/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.1/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.1/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.1/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.1/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.1/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.1/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.1/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.1/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.1/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.1/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.1/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.1/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.1/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.1/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.1/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.1/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.1/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.1/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.1/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.1/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.1/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.1/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.1/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.1/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.1/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.1/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.1/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.1/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.1/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.1/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.1/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.1/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.1/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.1/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.2/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.2/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.2/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.2/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.2/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.2/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.2/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.2/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.2/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.2/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.2/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.2/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.2/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.2/best-practices.md:123:> Recommend: `false`
content/docs/1.4.2/best-practices.md:129:> Recommend: `false`
content/docs/1.4.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.2/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.2/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.2/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.2/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.2/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.2/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.2/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.2/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.2/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.2/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.2/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.2/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.2/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.2/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.2/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.2/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.2/references/settings.md:84:> Default: `block-if-contains-last-replica`
content/docs/1.4.2/references/settings.md:95:> Default: `true`
content/docs/1.4.2/references/settings.md:101:> Default: `true`
content/docs/1.4.2/references/settings.md:108:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.2/references/settings.md:109:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.2/references/settings.md:110:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.2/references/settings.md:114:> Default: `true`
content/docs/1.4.2/references/settings.md:120:> Default: `0`
content/docs/1.4.2/references/settings.md:128:> Default: `5`
content/docs/1.4.2/references/settings.md:138:> Default: `false`
content/docs/1.4.2/references/settings.md:148:> Default: `longhorn.io/v1beta2`
content/docs/1.4.2/references/settings.md:154:> Default: `disabled`
content/docs/1.4.2/references/settings.md:172:> Default: `/var/lib/longhorn/`
content/docs/1.4.2/references/settings.md:190:> Default: `longhorn-static`
content/docs/1.4.2/references/settings.md:196:> Default: `3`
content/docs/1.4.2/references/settings.md:211:> Default: `false`
content/docs/1.4.2/references/settings.md:215:> Default: `false`
content/docs/1.4.2/references/settings.md:221:> Default: `true`
content/docs/1.4.2/references/settings.md:233:> Default: `do-nothing`
content/docs/1.4.2/references/settings.md:248:> Default: `600`
content/docs/1.4.2/references/settings.md:256:> Default: `if-not-present`
content/docs/1.4.2/references/settings.md:272:> Default: `60`
content/docs/1.4.2/references/settings.md:277:> Default: `300`
content/docs/1.4.2/references/settings.md:281:> **Note:**
content/docs/1.4.2/references/settings.md:282:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.2/references/settings.md:283:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.2/references/settings.md:286:> Default: `8`
content/docs/1.4.2/references/settings.md:297:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.2/references/settings.md:301:> Default: `1`
content/docs/1.4.2/references/settings.md:311:> Default: `false`
content/docs/1.4.2/references/settings.md:317:> Default: `30`
content/docs/1.4.2/references/settings.md:325:> Default: `fast-check`
content/docs/1.4.2/references/settings.md:334:> Default: `false`
content/docs/1.4.2/references/settings.md:340:> Default: `0 0 */7 * *`
content/docs/1.4.2/references/settings.md:343:> **Warning**
content/docs/1.4.2/references/settings.md:344:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.2/references/settings.md:350:> Default: `false`
content/docs/1.4.2/references/settings.md:358:> Default: `false`
content/docs/1.4.2/references/settings.md:362:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.2/references/settings.md:366:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.2/references/settings.md:372:> Example: `s3-secret`
content/docs/1.4.2/references/settings.md:378:> Default: `300`
content/docs/1.4.2/references/settings.md:386:> Default: `1440`
content/docs/1.4.2/references/settings.md:394:> Default: `1`
content/docs/1.4.2/references/settings.md:403:> Default: `1`
content/docs/1.4.2/references/settings.md:411:> Default: `false`
content/docs/1.4.2/references/settings.md:416:> Default: `ignored`
content/docs/1.4.2/references/settings.md:428:> Default: `true`
content/docs/1.4.2/references/settings.md:432:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.2/references/settings.md:436:> Default: `true`
content/docs/1.4.2/references/settings.md:444:> Default: `false`
content/docs/1.4.2/references/settings.md:452:> Default: `true`
content/docs/1.4.2/references/settings.md:458:> **Note:**
content/docs/1.4.2/references/settings.md:459:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.2/references/settings.md:460:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.2/references/settings.md:464:> Default: `disabled`
content/docs/1.4.2/references/settings.md:480:> Default: `ignored`
content/docs/1.4.2/references/settings.md:494:> Default: `25`
content/docs/1.4.2/references/settings.md:502:> Default: `200`
content/docs/1.4.2/references/settings.md:514:> Default: `5`
content/docs/1.4.2/references/settings.md:520:> **WARNING:**
content/docs/1.4.2/references/settings.md:521:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.2/references/settings.md:522:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.2/references/settings.md:523:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.2/references/settings.md:528:> Default: `12`
content/docs/1.4.2/references/settings.md:540:> **Warning:**
content/docs/1.4.2/references/settings.md:541:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.2/references/settings.md:542:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.2/references/settings.md:543:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.2/references/settings.md:544:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.2/references/settings.md:545:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.2/references/settings.md:549:> Default: `12`
content/docs/1.4.2/references/settings.md:561:> **Warning:**
content/docs/1.4.2/references/settings.md:562:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.2/references/settings.md:563:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.2/references/settings.md:564:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.2/references/settings.md:565:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.2/references/settings.md:566:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.2/references/settings.md:571:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.2/references/settings.md:590:> Example: `high-priority`
content/docs/1.4.2/references/settings.md:601:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.2/references/settings.md:607:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.2/references/settings.md:614:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.2/references/settings.md:620:> Default: `false`
content/docs/1.4.2/references/settings.md:626:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.2/references/settings.md:630:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.2/references/settings.md:634:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.2/references/settings.md:640:> Example: `false`
content/docs/1.4.2/references/settings.md:654:> Default: `false`
content/docs/1.4.2/references/settings.md:666:> Default: `false`
content/docs/1.4.2/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.2/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.2/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.2/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.2/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.2/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.2/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.2/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.2/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.2/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.2/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.2/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.2/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.2/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.2/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.2/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.2/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.2/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.2/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.2/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.2/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.2/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.2/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.2/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.2/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.2/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.2/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.2/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.2/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.2/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.2/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.2/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.2/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.2/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.2/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.2/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.2/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.2/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.2/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.2/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.2/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.2/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.2/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.2/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.2/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.2/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.2/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.2/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.2/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.2/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.2/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.2/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.2/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.2/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.2/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.2/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.2/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.2/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.3/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.3/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.3/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.3/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.3/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.3/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.3/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.3/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.3/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.3/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.3/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.3/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.3/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.3/best-practices.md:123:> Recommend: `false`
content/docs/1.4.3/best-practices.md:129:> Recommend: `false`
content/docs/1.4.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.3/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.3/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.3/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.3/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.3/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.3/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.3/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.3/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.3/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.3/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.3/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.3/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.3/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.3/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.3/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.3/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.3/references/settings.md:84:> Default: `block-if-contains-last-replica`
content/docs/1.4.3/references/settings.md:95:> Default: `true`
content/docs/1.4.3/references/settings.md:101:> Default: `true`
content/docs/1.4.3/references/settings.md:108:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.3/references/settings.md:109:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.3/references/settings.md:110:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.3/references/settings.md:114:> Default: `true`
content/docs/1.4.3/references/settings.md:120:> Default: `0`
content/docs/1.4.3/references/settings.md:128:> Default: `5`
content/docs/1.4.3/references/settings.md:138:> Default: `false`
content/docs/1.4.3/references/settings.md:148:> Default: `longhorn.io/v1beta2`
content/docs/1.4.3/references/settings.md:154:> Default: `disabled`
content/docs/1.4.3/references/settings.md:172:> Default: `/var/lib/longhorn/`
content/docs/1.4.3/references/settings.md:190:> Default: `longhorn-static`
content/docs/1.4.3/references/settings.md:196:> Default: `3`
content/docs/1.4.3/references/settings.md:211:> Default: `false`
content/docs/1.4.3/references/settings.md:215:> Default: `false`
content/docs/1.4.3/references/settings.md:221:> Default: `true`
content/docs/1.4.3/references/settings.md:233:> Default: `do-nothing`
content/docs/1.4.3/references/settings.md:248:> Default: `600`
content/docs/1.4.3/references/settings.md:256:> Default: `if-not-present`
content/docs/1.4.3/references/settings.md:272:> Default: `60`
content/docs/1.4.3/references/settings.md:277:> Default: `300`
content/docs/1.4.3/references/settings.md:281:> **Note:**
content/docs/1.4.3/references/settings.md:282:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.3/references/settings.md:283:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.3/references/settings.md:286:> Default: `8`
content/docs/1.4.3/references/settings.md:297:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.3/references/settings.md:301:> Default: `1`
content/docs/1.4.3/references/settings.md:311:> Default: `false`
content/docs/1.4.3/references/settings.md:317:> Default: `30`
content/docs/1.4.3/references/settings.md:325:> Default: `fast-check`
content/docs/1.4.3/references/settings.md:334:> Default: `false`
content/docs/1.4.3/references/settings.md:340:> Default: `0 0 */7 * *`
content/docs/1.4.3/references/settings.md:343:> **Warning**
content/docs/1.4.3/references/settings.md:344:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.3/references/settings.md:350:> Default: `false`
content/docs/1.4.3/references/settings.md:358:> Default: `false`
content/docs/1.4.3/references/settings.md:362:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.3/references/settings.md:366:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.3/references/settings.md:372:> Example: `s3-secret`
content/docs/1.4.3/references/settings.md:378:> Default: `300`
content/docs/1.4.3/references/settings.md:386:> Default: `1440`
content/docs/1.4.3/references/settings.md:394:> Default: `1`
content/docs/1.4.3/references/settings.md:403:> Default: `1`
content/docs/1.4.3/references/settings.md:411:> Default: `false`
content/docs/1.4.3/references/settings.md:416:> Default: `ignored`
content/docs/1.4.3/references/settings.md:428:> Default: `true`
content/docs/1.4.3/references/settings.md:432:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.3/references/settings.md:436:> Default: `true`
content/docs/1.4.3/references/settings.md:444:> Default: `false`
content/docs/1.4.3/references/settings.md:452:> Default: `true`
content/docs/1.4.3/references/settings.md:458:> **Note:**
content/docs/1.4.3/references/settings.md:459:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.3/references/settings.md:460:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.3/references/settings.md:464:> Default: `disabled`
content/docs/1.4.3/references/settings.md:480:> Default: `ignored`
content/docs/1.4.3/references/settings.md:494:> Default: `25`
content/docs/1.4.3/references/settings.md:502:> Default: `200`
content/docs/1.4.3/references/settings.md:514:> Default: `5`
content/docs/1.4.3/references/settings.md:520:> **WARNING:**
content/docs/1.4.3/references/settings.md:521:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.3/references/settings.md:522:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.3/references/settings.md:523:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.3/references/settings.md:528:> Default: `12`
content/docs/1.4.3/references/settings.md:540:> **Warning:**
content/docs/1.4.3/references/settings.md:541:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.3/references/settings.md:542:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.3/references/settings.md:543:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.3/references/settings.md:544:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.3/references/settings.md:545:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.3/references/settings.md:549:> Default: `12`
content/docs/1.4.3/references/settings.md:561:> **Warning:**
content/docs/1.4.3/references/settings.md:562:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.3/references/settings.md:563:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.3/references/settings.md:564:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.3/references/settings.md:565:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.3/references/settings.md:566:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.3/references/settings.md:571:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.3/references/settings.md:590:> Example: `high-priority`
content/docs/1.4.3/references/settings.md:601:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.3/references/settings.md:607:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.3/references/settings.md:614:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.3/references/settings.md:620:> Default: `false`
content/docs/1.4.3/references/settings.md:626:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.3/references/settings.md:630:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.3/references/settings.md:634:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.3/references/settings.md:640:> Example: `false`
content/docs/1.4.3/references/settings.md:654:> Default: `false`
content/docs/1.4.3/references/settings.md:666:> Default: `false`
content/docs/1.4.3/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.3/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.3/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.3/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.3/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.3/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.3/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.3/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.3/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.3/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.3/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.3/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.3/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.3/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.3/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.3/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.3/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.3/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.3/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.3/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.3/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.3/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.3/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.3/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.3/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.3/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.3/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.3/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.3/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.3/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.3/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.3/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.3/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.3/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.3/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.3/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.3/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.3/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.3/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.3/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.3/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.3/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.3/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.3/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.3/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.3/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.3/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.3/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.3/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.3/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.3/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.3/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.3/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.3/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.3/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.3/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.3/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.3/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.4/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.4/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.4/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.4/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.4/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.4/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.4/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.4/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.4/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.4/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.4/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.4/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.4/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.4/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.4/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.4/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.4/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.4/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.4/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.4/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.4/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.4/best-practices.md:123:> Recommend: `false`
content/docs/1.4.4/best-practices.md:129:> Recommend: `false`
content/docs/1.4.4/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.4/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.4/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.4/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.4/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.4/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.4/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.4/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.4/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.4/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.4/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.4/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.4/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.4/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.4/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.4/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.4/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.4/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.4/references/settings.md:84:> Default: `block-if-contains-last-replica`
content/docs/1.4.4/references/settings.md:95:> Default: `true`
content/docs/1.4.4/references/settings.md:101:> Default: `true`
content/docs/1.4.4/references/settings.md:108:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.4/references/settings.md:109:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.4/references/settings.md:110:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.4/references/settings.md:114:> Default: `true`
content/docs/1.4.4/references/settings.md:120:> Default: `0`
content/docs/1.4.4/references/settings.md:128:> Default: `5`
content/docs/1.4.4/references/settings.md:138:> Default: `false`
content/docs/1.4.4/references/settings.md:148:> Default: `longhorn.io/v1beta2`
content/docs/1.4.4/references/settings.md:154:> Default: `disabled`
content/docs/1.4.4/references/settings.md:172:> Default: `/var/lib/longhorn/`
content/docs/1.4.4/references/settings.md:190:> Default: `longhorn-static`
content/docs/1.4.4/references/settings.md:196:> Default: `3`
content/docs/1.4.4/references/settings.md:211:> Default: `false`
content/docs/1.4.4/references/settings.md:215:> Default: `false`
content/docs/1.4.4/references/settings.md:221:> Default: `true`
content/docs/1.4.4/references/settings.md:233:> Default: `do-nothing`
content/docs/1.4.4/references/settings.md:248:> Default: `600`
content/docs/1.4.4/references/settings.md:256:> Default: `if-not-present`
content/docs/1.4.4/references/settings.md:272:> Default: `60`
content/docs/1.4.4/references/settings.md:277:> Default: `300`
content/docs/1.4.4/references/settings.md:281:> **Note:**
content/docs/1.4.4/references/settings.md:282:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.4/references/settings.md:283:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.4/references/settings.md:286:> Default: `8`
content/docs/1.4.4/references/settings.md:297:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.4/references/settings.md:301:> Default: `1`
content/docs/1.4.4/references/settings.md:311:> Default: `false`
content/docs/1.4.4/references/settings.md:317:> Default: `30`
content/docs/1.4.4/references/settings.md:325:> Default: `fast-check`
content/docs/1.4.4/references/settings.md:334:> Default: `false`
content/docs/1.4.4/references/settings.md:340:> Default: `0 0 */7 * *`
content/docs/1.4.4/references/settings.md:343:> **Warning**
content/docs/1.4.4/references/settings.md:344:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.4/references/settings.md:350:> Default: `false`
content/docs/1.4.4/references/settings.md:358:> Default: `false`
content/docs/1.4.4/references/settings.md:362:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.4/references/settings.md:366:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.4/references/settings.md:372:> Example: `s3-secret`
content/docs/1.4.4/references/settings.md:378:> Default: `300`
content/docs/1.4.4/references/settings.md:386:> Default: `1440`
content/docs/1.4.4/references/settings.md:394:> Default: `1`
content/docs/1.4.4/references/settings.md:403:> Default: `1`
content/docs/1.4.4/references/settings.md:411:> Default: `false`
content/docs/1.4.4/references/settings.md:416:> Default: `ignored`
content/docs/1.4.4/references/settings.md:428:> Default: `true`
content/docs/1.4.4/references/settings.md:432:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.4/references/settings.md:436:> Default: `true`
content/docs/1.4.4/references/settings.md:444:> Default: `false`
content/docs/1.4.4/references/settings.md:452:> Default: `true`
content/docs/1.4.4/references/settings.md:458:> **Note:**
content/docs/1.4.4/references/settings.md:459:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.4/references/settings.md:460:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.4/references/settings.md:464:> Default: `disabled`
content/docs/1.4.4/references/settings.md:480:> Default: `ignored`
content/docs/1.4.4/references/settings.md:494:> Default: `25`
content/docs/1.4.4/references/settings.md:502:> Default: `200`
content/docs/1.4.4/references/settings.md:514:> Default: `5`
content/docs/1.4.4/references/settings.md:520:> **WARNING:**
content/docs/1.4.4/references/settings.md:521:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.4/references/settings.md:522:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.4/references/settings.md:523:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.4/references/settings.md:528:> Default: `12`
content/docs/1.4.4/references/settings.md:540:> **Warning:**
content/docs/1.4.4/references/settings.md:541:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.4/references/settings.md:542:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.4/references/settings.md:543:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.4/references/settings.md:544:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.4/references/settings.md:545:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.4/references/settings.md:549:> Default: `12`
content/docs/1.4.4/references/settings.md:561:> **Warning:**
content/docs/1.4.4/references/settings.md:562:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.4/references/settings.md:563:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.4/references/settings.md:564:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.4/references/settings.md:565:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.4/references/settings.md:566:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.4/references/settings.md:571:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.4/references/settings.md:590:> Example: `high-priority`
content/docs/1.4.4/references/settings.md:601:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.4/references/settings.md:607:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.4/references/settings.md:614:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.4/references/settings.md:620:> Default: `false`
content/docs/1.4.4/references/settings.md:626:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.4/references/settings.md:630:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.4/references/settings.md:634:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.4/references/settings.md:640:> Example: `false`
content/docs/1.4.4/references/settings.md:654:> Default: `false`
content/docs/1.4.4/references/settings.md:666:> Default: `false`
content/docs/1.4.4/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.4/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.4/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.4/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.4/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.4/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.4/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.4/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.4/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.4/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.4/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.4/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.4/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.4/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.4/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.4/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.4/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.4/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.4/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.4/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.4/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.4/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.4/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.4/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.4/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.4/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.4/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.4/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.4/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.4/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.4/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.4/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.4/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.4/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.4/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.4/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.4/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.4/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.4/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.4/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.4/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.4/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.4/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.4/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.4/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.4/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.4/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.4/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.4/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.4/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.4/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.4/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.4/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.4/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.4/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.4/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.4/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.4/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.4/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.4/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.4/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.4/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.4/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.4/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.4/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.4/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.4.5/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.4.5/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.4.5/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:101:> **NOTE:**
content/docs/1.4.5/advanced-resources/deploy/customizing-default-settings.md:102:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:58:> **Warning**:
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:59:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:60:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.5/advanced-resources/deploy/node-selector.md:61:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.4.5/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.4.5/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.4.5/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.4.5/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.4.5/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:75:> **Warning**:
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:76:>
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:77:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:78:>
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:79:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:80:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:81:>
content/docs/1.4.5/advanced-resources/deploy/taint-toleration.md:82:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.4.5/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.4.5/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.4.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.4.5/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.4.5/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.4.5/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.4.5/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.4.5/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.4.5/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.4.5/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.4.5/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:40:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:42:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:44:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.4.5/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.4.5/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.4.5/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.4.5/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.4.5/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.4.5/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.4.5/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.4.5/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.4.5/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.4.5/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.5/deploy/install/install-with-kubectl.md:63:> **Note**:
content/docs/1.4.5/deploy/install/install-with-kubectl.md:64:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.4.5/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.4.5/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.4.5/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.4.5/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.4.5/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.4.5/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.5/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.4.5/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.4.5/references/settings.md:85:> Default: `block-if-contains-last-replica`
content/docs/1.4.5/references/settings.md:96:> Default: `true`
content/docs/1.4.5/references/settings.md:102:> Default: `true`
content/docs/1.4.5/references/settings.md:109:> **Note:** This setting doesn't apply to below cases.
content/docs/1.4.5/references/settings.md:110:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.4.5/references/settings.md:111:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.4.5/references/settings.md:115:> Default: `true`
content/docs/1.4.5/references/settings.md:121:> Default: `0`
content/docs/1.4.5/references/settings.md:129:> Default: `5`
content/docs/1.4.5/references/settings.md:139:> Default: `false`
content/docs/1.4.5/references/settings.md:149:> Default: `longhorn.io/v1beta2`
content/docs/1.4.5/references/settings.md:155:> Default: `disabled`
content/docs/1.4.5/references/settings.md:173:> Default: `/var/lib/longhorn/`
content/docs/1.4.5/references/settings.md:191:> Default: `longhorn-static`
content/docs/1.4.5/references/settings.md:197:> Default: `3`
content/docs/1.4.5/references/settings.md:212:> Default: `false`
content/docs/1.4.5/references/settings.md:216:> Default: `false`
content/docs/1.4.5/references/settings.md:222:> Default: `true`
content/docs/1.4.5/references/settings.md:234:> Default: `do-nothing`
content/docs/1.4.5/references/settings.md:249:> Default: `600`
content/docs/1.4.5/references/settings.md:257:> Default: `if-not-present`
content/docs/1.4.5/references/settings.md:273:> Default: `60`
content/docs/1.4.5/references/settings.md:278:> Default: `300`
content/docs/1.4.5/references/settings.md:282:> **Note:**
content/docs/1.4.5/references/settings.md:283:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.4.5/references/settings.md:284:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.4.5/references/settings.md:287:> Default: `8`
content/docs/1.4.5/references/settings.md:298:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.4.5/references/settings.md:302:> Default: `1`
content/docs/1.4.5/references/settings.md:312:> Default: `false`
content/docs/1.4.5/references/settings.md:318:> Default: `30`
content/docs/1.4.5/references/settings.md:326:> Default: `fast-check`
content/docs/1.4.5/references/settings.md:335:> Default: `false`
content/docs/1.4.5/references/settings.md:341:> Default: `0 0 */7 * *`
content/docs/1.4.5/references/settings.md:344:> **Warning**
content/docs/1.4.5/references/settings.md:345:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.4.5/references/settings.md:351:> Default: `false`
content/docs/1.4.5/references/settings.md:359:> Default: `false`
content/docs/1.4.5/references/settings.md:363:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.4.5/references/settings.md:367:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/1.4.5/references/settings.md:373:> Example: `s3-secret`
content/docs/1.4.5/references/settings.md:379:> Default: `300`
content/docs/1.4.5/references/settings.md:387:> Default: `1440`
content/docs/1.4.5/references/settings.md:395:> Default: `1`
content/docs/1.4.5/references/settings.md:404:> Default: `1`
content/docs/1.4.5/references/settings.md:412:> Default: `false`
content/docs/1.4.5/references/settings.md:417:> Default: `ignored`
content/docs/1.4.5/references/settings.md:429:> Default: `true`
content/docs/1.4.5/references/settings.md:433:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.4.5/references/settings.md:437:> Default: `true`
content/docs/1.4.5/references/settings.md:445:> Default: `false`
content/docs/1.4.5/references/settings.md:453:> Default: `true`
content/docs/1.4.5/references/settings.md:459:> **Note:**
content/docs/1.4.5/references/settings.md:460:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.4.5/references/settings.md:461:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.4.5/references/settings.md:465:> Default: `disabled`
content/docs/1.4.5/references/settings.md:481:> Default: `ignored`
content/docs/1.4.5/references/settings.md:495:> Default: `25`
content/docs/1.4.5/references/settings.md:503:> Default: `200`
content/docs/1.4.5/references/settings.md:515:> Default: `5`
content/docs/1.4.5/references/settings.md:521:> **WARNING:**
content/docs/1.4.5/references/settings.md:522:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.4.5/references/settings.md:523:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.4.5/references/settings.md:524:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.4.5/references/settings.md:529:> Default: `12`
content/docs/1.4.5/references/settings.md:541:> **Warning:**
content/docs/1.4.5/references/settings.md:542:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/1.4.5/references/settings.md:543:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/1.4.5/references/settings.md:544:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.5/references/settings.md:545:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/1.4.5/references/settings.md:546:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.5/references/settings.md:550:> Default: `12`
content/docs/1.4.5/references/settings.md:562:> **Warning:**
content/docs/1.4.5/references/settings.md:563:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/1.4.5/references/settings.md:564:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/1.4.5/references/settings.md:565:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.4.5/references/settings.md:566:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/1.4.5/references/settings.md:567:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.4.5/references/settings.md:572:> Example: `nodetype=storage:NoSchedule`
content/docs/1.4.5/references/settings.md:591:> Example: `high-priority`
content/docs/1.4.5/references/settings.md:602:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.4.5/references/settings.md:608:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.4.5/references/settings.md:615:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.4.5/references/settings.md:621:> Default: `false`
content/docs/1.4.5/references/settings.md:627:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.4.5/references/settings.md:631:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.4.5/references/settings.md:635:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.4.5/references/settings.md:641:> Example: `false`
content/docs/1.4.5/references/settings.md:653:> Default: `false`
content/docs/1.4.5/references/settings.md:668:> Default: `false`
content/docs/1.4.5/references/settings.md:680:> Default: `false`
content/docs/1.4.5/references/storage-class-parameters.md:49:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.4.5/references/storage-class-parameters.md:52:> Default: `true`  
content/docs/1.4.5/references/storage-class-parameters.md:53:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.4.5/references/storage-class-parameters.md:56:> Default: `Delete`  
content/docs/1.4.5/references/storage-class-parameters.md:57:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.4.5/references/storage-class-parameters.md:60:> Default `[]`  
content/docs/1.4.5/references/storage-class-parameters.md:61:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.4.5/references/storage-class-parameters.md:64:> Default `Immediate`  
content/docs/1.4.5/references/storage-class-parameters.md:65:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.4.5/references/storage-class-parameters.md:72:> Default: `3`  
content/docs/1.4.5/references/storage-class-parameters.md:78:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.4.5/references/storage-class-parameters.md:81:> Default: `30`
content/docs/1.4.5/references/storage-class-parameters.md:86:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:87:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.4.5/references/storage-class-parameters.md:92:> Default: `ext4`  
content/docs/1.4.5/references/storage-class-parameters.md:93:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.5/references/storage-class-parameters.md:96:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:97:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.4.5/references/storage-class-parameters.md:100:> Default: `false`  
content/docs/1.4.5/references/storage-class-parameters.md:105:> Default: `false`  
content/docs/1.4.5/references/storage-class-parameters.md:106:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.4.5/references/storage-class-parameters.md:109:> Default: `disabled`  
content/docs/1.4.5/references/storage-class-parameters.md:116:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.4.5/references/storage-class-parameters.md:117:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.4.5/references/storage-class-parameters.md:120:> Default: `ignored`  
content/docs/1.4.5/references/storage-class-parameters.md:126:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.4.5/references/storage-class-parameters.md:127:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.4.5/references/storage-class-parameters.md:130:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:131:> Example: `"ssd,fast"`  
content/docs/1.4.5/references/storage-class-parameters.md:134:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.5/references/storage-class-parameters.md:137:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:138:> Example: `"storage,fast"`  
content/docs/1.4.5/references/storage-class-parameters.md:141:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.4.5/references/storage-class-parameters.md:144:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:145:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.4.5/references/storage-class-parameters.md:148:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.4.5/references/storage-class-parameters.md:151:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:152:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.5/references/storage-class-parameters.md:155:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:156:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.5/references/storage-class-parameters.md:159:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:160:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.5/references/storage-class-parameters.md:163:> Default: `""`  
content/docs/1.4.5/references/storage-class-parameters.md:164:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.4.5/references/storage-class-parameters.md:167:> Default: `ignored`  
content/docs/1.4.5/references/storage-class-parameters.md:172:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.4.5/references/storage-class-parameters.md:173:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.4.5/references/storage-class-parameters.md:176:> Default: `false`  
content/docs/1.4.5/references/storage-class-parameters.md:178:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.4.5/references/storage-class-parameters.md:179:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.4.5/references/storage-class-parameters.md:182:> Default: `ignored`  
content/docs/1.4.5/references/storage-class-parameters.md:187:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.4.5/references/storage-class-parameters.md:188:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.4.5/references/storage-class-parameters.md:191:> Default: `ignored`  
content/docs/1.4.5/references/storage-class-parameters.md:196:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.4.5/references/storage-class-parameters.md:197:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.4.5/references/storage-class-parameters.md:200:> Default: `""`
content/docs/1.4.5/references/storage-class-parameters.md:201:> Example: `"hard,sync"`  
content/docs/1.4.5/references/storage-class-parameters.md:206:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.4.5/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.4.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.4.5/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.4.5/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.4.5/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.4.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.4.5/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.4.5/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.4.5/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.4.5/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.4.5/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.4.5/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.4.5/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.4.5/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.4.5/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.4.5/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.4.5/volumes-and-nodes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.4.5/best-practices.md:123:> Recommend: `false`
content/docs/1.4.5/best-practices.md:129:> Recommend: `false`
content/docs/1.4.5/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.4.5/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.0/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.0/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.0/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.0/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.0/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.0/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.0/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.0/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.0/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.0/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.0/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.0/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.0/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.0/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.0/best-practices.md:124:> Recommend: `false`
content/docs/1.5.0/best-practices.md:130:> Recommend: `false`
content/docs/1.5.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.0/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.0/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.0/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.0/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.0/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.0/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.0/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.0/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.0/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.0/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.0/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.0/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.0/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.0/references/settings.md:86:> Default: `block-if-contains-last-replica`
content/docs/1.5.0/references/settings.md:97:> Default: `true`
content/docs/1.5.0/references/settings.md:103:> Default: `true`
content/docs/1.5.0/references/settings.md:110:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.0/references/settings.md:111:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.0/references/settings.md:112:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.0/references/settings.md:116:> Default: `true`
content/docs/1.5.0/references/settings.md:122:> Default: `0`
content/docs/1.5.0/references/settings.md:130:> Default: `5`
content/docs/1.5.0/references/settings.md:140:> Default: `false`
content/docs/1.5.0/references/settings.md:150:> Default: `longhorn.io/v1beta2`
content/docs/1.5.0/references/settings.md:156:> Default: `disabled`
content/docs/1.5.0/references/settings.md:174:> Default: `/var/lib/longhorn/`
content/docs/1.5.0/references/settings.md:188:> Default: `longhorn-static`
content/docs/1.5.0/references/settings.md:194:> Default: `3`
content/docs/1.5.0/references/settings.md:205:> Default: `false`
content/docs/1.5.0/references/settings.md:209:> Default: `false`
content/docs/1.5.0/references/settings.md:215:> Default: `true`
content/docs/1.5.0/references/settings.md:223:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.0/references/settings.md:227:> Default: `true`
content/docs/1.5.0/references/settings.md:308:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.0/references/settings.md:312:> Default: `do-nothing`
content/docs/1.5.0/references/settings.md:327:> Default: `600`
content/docs/1.5.0/references/settings.md:335:> Default: `if-not-present`
content/docs/1.5.0/references/settings.md:351:> Default: `60`
content/docs/1.5.0/references/settings.md:356:> Default: `300`
content/docs/1.5.0/references/settings.md:360:> **Note:**
content/docs/1.5.0/references/settings.md:361:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.0/references/settings.md:362:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.0/references/settings.md:365:> Default: `8`
content/docs/1.5.0/references/settings.md:376:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.0/references/settings.md:380:> Default: `1`
content/docs/1.5.0/references/settings.md:390:> Default: `false`
content/docs/1.5.0/references/settings.md:396:> Default: `30`
content/docs/1.5.0/references/settings.md:403:> Default: `false`
content/docs/1.5.0/references/settings.md:407:> **Warning**
content/docs/1.5.0/references/settings.md:408:>
content/docs/1.5.0/references/settings.md:409:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.0/references/settings.md:410:>
content/docs/1.5.0/references/settings.md:411:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.0/references/settings.md:412:>
content/docs/1.5.0/references/settings.md:413:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.0/references/settings.md:417:> Default: `enabled`
content/docs/1.5.0/references/settings.md:429:> Default: `fast-check`
content/docs/1.5.0/references/settings.md:438:> Default: `false`
content/docs/1.5.0/references/settings.md:444:> Default: `0 0 */7 * *`
content/docs/1.5.0/references/settings.md:447:> **Warning**
content/docs/1.5.0/references/settings.md:448:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.0/references/settings.md:454:> Default: `false`
content/docs/1.5.0/references/settings.md:462:> Default: `false`
content/docs/1.5.0/references/settings.md:466:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.0/references/settings.md:470:> Examples:  
content/docs/1.5.0/references/settings.md:471:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.0/references/settings.md:472:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.0/references/settings.md:473:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.0/references/settings.md:479:> Example: `s3-secret`
content/docs/1.5.0/references/settings.md:485:> Default: `300`
content/docs/1.5.0/references/settings.md:493:> Default: `1440`
content/docs/1.5.0/references/settings.md:501:> Default: `1`
content/docs/1.5.0/references/settings.md:510:> Default: `1`
content/docs/1.5.0/references/settings.md:518:> Default: `false`
content/docs/1.5.0/references/settings.md:523:> Default: `ignored`
content/docs/1.5.0/references/settings.md:533:> Default: `lz4`
content/docs/1.5.0/references/settings.md:545:> Default: `2`
content/docs/1.5.0/references/settings.md:551:> Default: `2`
content/docs/1.5.0/references/settings.md:559:> Default: `true`
content/docs/1.5.0/references/settings.md:563:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.0/references/settings.md:567:> Default: `true`
content/docs/1.5.0/references/settings.md:575:> Default: `false`
content/docs/1.5.0/references/settings.md:583:> Default: `true`
content/docs/1.5.0/references/settings.md:589:> **Note:**
content/docs/1.5.0/references/settings.md:590:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.0/references/settings.md:591:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.0/references/settings.md:595:> Default: `disabled`
content/docs/1.5.0/references/settings.md:611:> Default: `ignored`
content/docs/1.5.0/references/settings.md:625:> Default: `25`
content/docs/1.5.0/references/settings.md:633:> Default: `100`
content/docs/1.5.0/references/settings.md:643:> Default: `30`
content/docs/1.5.0/references/settings.md:653:> Default: `5`
content/docs/1.5.0/references/settings.md:659:> **WARNING:**
content/docs/1.5.0/references/settings.md:660:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.0/references/settings.md:661:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.0/references/settings.md:662:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.0/references/settings.md:667:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.0/references/settings.md:686:> Example: `high-priority`
content/docs/1.5.0/references/settings.md:697:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.0/references/settings.md:703:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.0/references/settings.md:710:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.0/references/settings.md:716:> Default: `false`
content/docs/1.5.0/references/settings.md:722:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.0/references/settings.md:726:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.0/references/settings.md:730:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.0/references/settings.md:736:> Example: `false`
content/docs/1.5.0/references/settings.md:748:> Default: `12`
content/docs/1.5.0/references/settings.md:760:> **Warning:**
content/docs/1.5.0/references/settings.md:761:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.0/references/settings.md:762:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.0/references/settings.md:763:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.0/references/settings.md:764:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.0/references/settings.md:765:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.0/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.0/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.0/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.0/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.0/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.0/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.0/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.0/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.0/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.0/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.0/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.0/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.0/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.0/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.0/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.0/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.0/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.0/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.0/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.0/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.0/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.0/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.0/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.0/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.0/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.0/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.0/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.0/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.0/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.0/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.0/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.0/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.0/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.0/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.0/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.0/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.0/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.0/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.0/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.0/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.0/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.0/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.0/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.0/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.0/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.0/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.0/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.0/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.0/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.0/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.0/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.0/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.0/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.0/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.0/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.0/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.0/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.0/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.0/v2-data-engine/quick-start.md:81:> **NOTICE:**
content/docs/1.5.0/v2-data-engine/quick-start.md:82:>
content/docs/1.5.0/v2-data-engine/quick-start.md:83:> Make sure that the version of `nvme-cli` is equal to or greater than `1.12`.
content/docs/1.5.0/v2-data-engine/quick-start.md:84:>
content/docs/1.5.0/v2-data-engine/quick-start.md:85:> If the version of `nvme-cli` installed by the below steps is not equal to or greater than `1.12`., you will need to compile the utility from the [source codes](https://github.com/linux-nvme/nvme-cli) and install it on each Longhorn node by manual.
content/docs/1.5.0/v2-data-engine/quick-start.md:86:>
content/docs/1.5.0/v2-data-engine/quick-start.md:87:> Also, install the **uuid development library** before compiling to support the `show-hostnqn` subcommand.
content/docs/1.5.0/v2-data-engine/quick-start.md:88:>
content/docs/1.5.0/v2-data-engine/quick-start.md:89:> For SUSE/OpenSUSE you can install it use this command:
content/docs/1.5.0/v2-data-engine/quick-start.md:90:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:91:> zypper install uuid-devel
content/docs/1.5.0/v2-data-engine/quick-start.md:92:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:93:>
content/docs/1.5.0/v2-data-engine/quick-start.md:94:> For Debian and Ubuntu, use this command:
content/docs/1.5.0/v2-data-engine/quick-start.md:95:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:96:> apt install uuid-dev
content/docs/1.5.0/v2-data-engine/quick-start.md:97:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:98:>
content/docs/1.5.0/v2-data-engine/quick-start.md:99:> For RHEL, CentOS, and EKS with `EKS Kubernetes Worker AMI with AmazonLinux2 image`, use this command:
content/docs/1.5.0/v2-data-engine/quick-start.md:100:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:101:> yum install uuid-devel
content/docs/1.5.0/v2-data-engine/quick-start.md:102:> ```
content/docs/1.5.0/v2-data-engine/quick-start.md:103:>
content/docs/1.5.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.1/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.1/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.1/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.1/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.1/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.1/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.1/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.1/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.1/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.1/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.1/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.1/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.1/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.1/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.1/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.1/best-practices.md:124:> Recommend: `false`
content/docs/1.5.1/best-practices.md:130:> Recommend: `false`
content/docs/1.5.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.1/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.1/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.1/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.1/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.1/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.1/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.1/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.1/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.1/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.1/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.1/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.1/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.1/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.1/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.1/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.1/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.1/references/settings.md:86:> Default: `block-if-contains-last-replica`
content/docs/1.5.1/references/settings.md:97:> Default: `true`
content/docs/1.5.1/references/settings.md:103:> Default: `true`
content/docs/1.5.1/references/settings.md:110:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.1/references/settings.md:111:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.1/references/settings.md:112:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.1/references/settings.md:116:> Default: `true`
content/docs/1.5.1/references/settings.md:122:> Default: `0`
content/docs/1.5.1/references/settings.md:130:> Default: `5`
content/docs/1.5.1/references/settings.md:140:> Default: `false`
content/docs/1.5.1/references/settings.md:150:> Default: `longhorn.io/v1beta2`
content/docs/1.5.1/references/settings.md:156:> Default: `disabled`
content/docs/1.5.1/references/settings.md:174:> Default: `/var/lib/longhorn/`
content/docs/1.5.1/references/settings.md:188:> Default: `longhorn-static`
content/docs/1.5.1/references/settings.md:194:> Default: `3`
content/docs/1.5.1/references/settings.md:205:> Default: `false`
content/docs/1.5.1/references/settings.md:209:> Default: `false`
content/docs/1.5.1/references/settings.md:215:> Default: `true`
content/docs/1.5.1/references/settings.md:223:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.1/references/settings.md:227:> Default: `true`
content/docs/1.5.1/references/settings.md:308:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.1/references/settings.md:312:> Default: `do-nothing`
content/docs/1.5.1/references/settings.md:327:> Default: `600`
content/docs/1.5.1/references/settings.md:335:> Default: `if-not-present`
content/docs/1.5.1/references/settings.md:351:> Default: `60`
content/docs/1.5.1/references/settings.md:356:> Default: `300`
content/docs/1.5.1/references/settings.md:360:> **Note:**
content/docs/1.5.1/references/settings.md:361:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.1/references/settings.md:362:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.1/references/settings.md:365:> Default: `8`
content/docs/1.5.1/references/settings.md:376:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.1/references/settings.md:380:> Default: `1`
content/docs/1.5.1/references/settings.md:390:> Default: `false`
content/docs/1.5.1/references/settings.md:396:> Default: `30`
content/docs/1.5.1/references/settings.md:403:> Default: `false`
content/docs/1.5.1/references/settings.md:407:> **Warning**
content/docs/1.5.1/references/settings.md:408:>
content/docs/1.5.1/references/settings.md:409:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.1/references/settings.md:410:>
content/docs/1.5.1/references/settings.md:411:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.1/references/settings.md:412:>
content/docs/1.5.1/references/settings.md:413:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.1/references/settings.md:417:> Default: `enabled`
content/docs/1.5.1/references/settings.md:429:> Default: `fast-check`
content/docs/1.5.1/references/settings.md:438:> Default: `false`
content/docs/1.5.1/references/settings.md:444:> Default: `0 0 */7 * *`
content/docs/1.5.1/references/settings.md:447:> **Warning**
content/docs/1.5.1/references/settings.md:448:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.1/references/settings.md:454:> Default: `false`
content/docs/1.5.1/references/settings.md:462:> Default: `false`
content/docs/1.5.1/references/settings.md:466:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.1/references/settings.md:470:> Examples:  
content/docs/1.5.1/references/settings.md:471:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.1/references/settings.md:472:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.1/references/settings.md:473:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.1/references/settings.md:479:> Example: `s3-secret`
content/docs/1.5.1/references/settings.md:485:> Default: `300`
content/docs/1.5.1/references/settings.md:493:> Default: `1440`
content/docs/1.5.1/references/settings.md:501:> Default: `1`
content/docs/1.5.1/references/settings.md:510:> Default: `1`
content/docs/1.5.1/references/settings.md:518:> Default: `false`
content/docs/1.5.1/references/settings.md:523:> Default: `ignored`
content/docs/1.5.1/references/settings.md:533:> Default: `lz4`
content/docs/1.5.1/references/settings.md:545:> Default: `2`
content/docs/1.5.1/references/settings.md:551:> Default: `2`
content/docs/1.5.1/references/settings.md:559:> Default: `true`
content/docs/1.5.1/references/settings.md:563:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.1/references/settings.md:567:> Default: `true`
content/docs/1.5.1/references/settings.md:575:> Default: `false`
content/docs/1.5.1/references/settings.md:583:> Default: `true`
content/docs/1.5.1/references/settings.md:589:> **Note:**
content/docs/1.5.1/references/settings.md:590:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.1/references/settings.md:591:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.1/references/settings.md:595:> Default: `disabled`
content/docs/1.5.1/references/settings.md:611:> Default: `ignored`
content/docs/1.5.1/references/settings.md:625:> Default: `25`
content/docs/1.5.1/references/settings.md:633:> Default: `100`
content/docs/1.5.1/references/settings.md:643:> Default: `30`
content/docs/1.5.1/references/settings.md:653:> Default: `5`
content/docs/1.5.1/references/settings.md:659:> **WARNING:**
content/docs/1.5.1/references/settings.md:660:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.1/references/settings.md:661:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.1/references/settings.md:662:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.1/references/settings.md:667:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.1/references/settings.md:686:> Example: `high-priority`
content/docs/1.5.1/references/settings.md:697:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.1/references/settings.md:703:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.1/references/settings.md:710:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.1/references/settings.md:716:> Default: `false`
content/docs/1.5.1/references/settings.md:722:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.1/references/settings.md:726:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.1/references/settings.md:730:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.1/references/settings.md:736:> Example: `false`
content/docs/1.5.1/references/settings.md:748:> Default: `12`
content/docs/1.5.1/references/settings.md:760:> **Warning:**
content/docs/1.5.1/references/settings.md:761:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.1/references/settings.md:762:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.1/references/settings.md:763:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.1/references/settings.md:764:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.1/references/settings.md:765:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.1/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.1/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.1/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.1/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.1/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.1/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.1/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.1/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.1/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.1/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.1/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.1/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.1/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.1/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.1/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.1/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.1/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.1/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.1/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.1/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.1/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.1/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.1/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.1/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.1/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.1/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.1/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.1/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.1/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.1/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.1/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.1/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.1/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.1/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.1/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.1/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.1/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.1/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.1/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.1/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.1/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.1/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.1/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.1/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.1/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.1/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.1/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.1/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.1/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.1/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.1/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.1/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.1/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.1/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.1/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.1/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.1/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.1/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.1/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.1/v2-data-engine/quick-start.md:81:> **NOTICE:**
content/docs/1.5.1/v2-data-engine/quick-start.md:82:>
content/docs/1.5.1/v2-data-engine/quick-start.md:83:> Make sure that the version of `nvme-cli` is equal to or greater than `1.12`.
content/docs/1.5.1/v2-data-engine/quick-start.md:84:>
content/docs/1.5.1/v2-data-engine/quick-start.md:85:> If the version of `nvme-cli` installed by the below steps is not equal to or greater than `1.12`., you will need to compile the utility from the [source codes](https://github.com/linux-nvme/nvme-cli) and install it on each Longhorn node by manual.
content/docs/1.5.1/v2-data-engine/quick-start.md:86:>
content/docs/1.5.1/v2-data-engine/quick-start.md:87:> Also, install the **uuid development library** before compiling to support the `show-hostnqn` subcommand.
content/docs/1.5.1/v2-data-engine/quick-start.md:88:>
content/docs/1.5.1/v2-data-engine/quick-start.md:89:> For SUSE/OpenSUSE you can install it use this command:
content/docs/1.5.1/v2-data-engine/quick-start.md:90:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:91:> zypper install uuid-devel
content/docs/1.5.1/v2-data-engine/quick-start.md:92:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:93:>
content/docs/1.5.1/v2-data-engine/quick-start.md:94:> For Debian and Ubuntu, use this command:
content/docs/1.5.1/v2-data-engine/quick-start.md:95:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:96:> apt install uuid-dev
content/docs/1.5.1/v2-data-engine/quick-start.md:97:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:98:>
content/docs/1.5.1/v2-data-engine/quick-start.md:99:> For RHEL, CentOS, and EKS with `EKS Kubernetes Worker AMI with AmazonLinux2 image`, use this command:
content/docs/1.5.1/v2-data-engine/quick-start.md:100:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:101:> yum install uuid-devel
content/docs/1.5.1/v2-data-engine/quick-start.md:102:> ```
content/docs/1.5.1/v2-data-engine/quick-start.md:103:>
content/docs/1.5.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.2/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.2/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.2/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.2/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.2/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.2/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.2/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.2/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.2/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.2/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.2/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.2/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.2/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.2/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.2/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.2/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.2/best-practices.md:124:> Recommend: `false`
content/docs/1.5.2/best-practices.md:130:> Recommend: `false`
content/docs/1.5.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.2/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.2/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.2/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.2/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.2/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.2/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.2/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.2/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.2/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.2/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.2/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.2/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.2/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.2/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.2/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.2/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.2/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.2/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.2/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.2/references/settings.md:86:> Default: `block-if-contains-last-replica`
content/docs/1.5.2/references/settings.md:97:> Default: `true`
content/docs/1.5.2/references/settings.md:103:> Default: `true`
content/docs/1.5.2/references/settings.md:110:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.2/references/settings.md:111:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.2/references/settings.md:112:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.2/references/settings.md:116:> Default: `true`
content/docs/1.5.2/references/settings.md:122:> Default: `0`
content/docs/1.5.2/references/settings.md:130:> Default: `5`
content/docs/1.5.2/references/settings.md:140:> Default: `false`
content/docs/1.5.2/references/settings.md:150:> Default: `longhorn.io/v1beta2`
content/docs/1.5.2/references/settings.md:156:> Default: `disabled`
content/docs/1.5.2/references/settings.md:174:> Default: `/var/lib/longhorn/`
content/docs/1.5.2/references/settings.md:188:> Default: `longhorn-static`
content/docs/1.5.2/references/settings.md:194:> Default: `3`
content/docs/1.5.2/references/settings.md:205:> Default: `false`
content/docs/1.5.2/references/settings.md:209:> Default: `false`
content/docs/1.5.2/references/settings.md:215:> Default: `true`
content/docs/1.5.2/references/settings.md:223:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.2/references/settings.md:227:> Default: `true`
content/docs/1.5.2/references/settings.md:308:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.2/references/settings.md:312:> Default: `do-nothing`
content/docs/1.5.2/references/settings.md:327:> Default: `600`
content/docs/1.5.2/references/settings.md:335:> Default: `if-not-present`
content/docs/1.5.2/references/settings.md:351:> Default: `60`
content/docs/1.5.2/references/settings.md:356:> Default: `300`
content/docs/1.5.2/references/settings.md:360:> **Note:**
content/docs/1.5.2/references/settings.md:361:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.2/references/settings.md:362:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.2/references/settings.md:365:> Default: `8`
content/docs/1.5.2/references/settings.md:376:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.2/references/settings.md:380:> Default: `1`
content/docs/1.5.2/references/settings.md:390:> Default: `false`
content/docs/1.5.2/references/settings.md:396:> Default: `30`
content/docs/1.5.2/references/settings.md:403:> Default: `false`
content/docs/1.5.2/references/settings.md:407:> **Warning**
content/docs/1.5.2/references/settings.md:408:>
content/docs/1.5.2/references/settings.md:409:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.2/references/settings.md:410:>
content/docs/1.5.2/references/settings.md:411:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.2/references/settings.md:412:>
content/docs/1.5.2/references/settings.md:413:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.2/references/settings.md:417:> Default: `enabled`
content/docs/1.5.2/references/settings.md:429:> Default: `fast-check`
content/docs/1.5.2/references/settings.md:438:> Default: `false`
content/docs/1.5.2/references/settings.md:444:> Default: `0 0 */7 * *`
content/docs/1.5.2/references/settings.md:447:> **Warning**
content/docs/1.5.2/references/settings.md:448:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.2/references/settings.md:454:> Default: `false`
content/docs/1.5.2/references/settings.md:462:> Default: `false`
content/docs/1.5.2/references/settings.md:466:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.2/references/settings.md:470:> Examples:  
content/docs/1.5.2/references/settings.md:471:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.2/references/settings.md:472:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.2/references/settings.md:473:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.2/references/settings.md:479:> Example: `s3-secret`
content/docs/1.5.2/references/settings.md:485:> Default: `300`
content/docs/1.5.2/references/settings.md:493:> Default: `1440`
content/docs/1.5.2/references/settings.md:501:> Default: `1`
content/docs/1.5.2/references/settings.md:510:> Default: `1`
content/docs/1.5.2/references/settings.md:518:> Default: `false`
content/docs/1.5.2/references/settings.md:523:> Default: `ignored`
content/docs/1.5.2/references/settings.md:533:> Default: `lz4`
content/docs/1.5.2/references/settings.md:545:> Default: `2`
content/docs/1.5.2/references/settings.md:551:> Default: `2`
content/docs/1.5.2/references/settings.md:559:> Default: `true`
content/docs/1.5.2/references/settings.md:563:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.2/references/settings.md:567:> Default: `true`
content/docs/1.5.2/references/settings.md:575:> Default: `false`
content/docs/1.5.2/references/settings.md:583:> Default: `true`
content/docs/1.5.2/references/settings.md:589:> **Note:**
content/docs/1.5.2/references/settings.md:590:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.2/references/settings.md:591:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.2/references/settings.md:595:> Default: `disabled`
content/docs/1.5.2/references/settings.md:611:> Default: `ignored`
content/docs/1.5.2/references/settings.md:625:> Default: `25`
content/docs/1.5.2/references/settings.md:633:> Default: `100`
content/docs/1.5.2/references/settings.md:643:> Default: `30`
content/docs/1.5.2/references/settings.md:653:> Default: `5`
content/docs/1.5.2/references/settings.md:659:> **WARNING:**
content/docs/1.5.2/references/settings.md:660:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.2/references/settings.md:661:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.2/references/settings.md:662:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.2/references/settings.md:667:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.2/references/settings.md:686:> Example: `high-priority`
content/docs/1.5.2/references/settings.md:697:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.2/references/settings.md:703:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.2/references/settings.md:710:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.2/references/settings.md:716:> Default: `false`
content/docs/1.5.2/references/settings.md:722:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.2/references/settings.md:726:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.2/references/settings.md:730:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.2/references/settings.md:736:> Example: `false`
content/docs/1.5.2/references/settings.md:748:> Default: `12`
content/docs/1.5.2/references/settings.md:760:> **Warning:**
content/docs/1.5.2/references/settings.md:761:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.2/references/settings.md:762:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.2/references/settings.md:763:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.2/references/settings.md:764:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.2/references/settings.md:765:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.2/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.2/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.2/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.2/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.2/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.2/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.2/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.2/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.2/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.2/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.2/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.2/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.2/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.2/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.2/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.2/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.2/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.2/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.2/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.2/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.2/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.2/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.2/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.2/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.2/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.2/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.2/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.2/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.2/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.2/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.2/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.2/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.2/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.2/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.2/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.2/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.2/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.2/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.2/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.2/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.2/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.2/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.2/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.2/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.2/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.2/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.2/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.2/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.2/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.2/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.2/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.2/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.2/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.2/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.2/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.2/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.2/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.2/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.2/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.2/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.2/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.2/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.3/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.3/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.3/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.3/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.3/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.3/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.3/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.3/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.3/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.3/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.3/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.3/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.3/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.3/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.3/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.3/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.3/best-practices.md:124:> Recommend: `false`
content/docs/1.5.3/best-practices.md:130:> Recommend: `false`
content/docs/1.5.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.3/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.3/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.3/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.3/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.3/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.3/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.3/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.3/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.3/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.3/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.3/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.3/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.3/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.3/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.3/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.3/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.3/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.3/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.3/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.3/references/settings.md:86:> Default: `block-if-contains-last-replica`
content/docs/1.5.3/references/settings.md:97:> Default: `true`
content/docs/1.5.3/references/settings.md:103:> Default: `true`
content/docs/1.5.3/references/settings.md:110:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.3/references/settings.md:111:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.3/references/settings.md:112:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.3/references/settings.md:116:> Default: `true`
content/docs/1.5.3/references/settings.md:122:> Default: `0`
content/docs/1.5.3/references/settings.md:130:> Default: `5`
content/docs/1.5.3/references/settings.md:140:> Default: `false`
content/docs/1.5.3/references/settings.md:150:> Default: `longhorn.io/v1beta2`
content/docs/1.5.3/references/settings.md:156:> Default: `disabled`
content/docs/1.5.3/references/settings.md:174:> Default: `/var/lib/longhorn/`
content/docs/1.5.3/references/settings.md:188:> Default: `longhorn-static`
content/docs/1.5.3/references/settings.md:194:> Default: `3`
content/docs/1.5.3/references/settings.md:205:> Default: `false`
content/docs/1.5.3/references/settings.md:209:> Default: `false`
content/docs/1.5.3/references/settings.md:215:> Default: `true`
content/docs/1.5.3/references/settings.md:223:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.3/references/settings.md:227:> Default: `true`
content/docs/1.5.3/references/settings.md:308:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.3/references/settings.md:312:> Default: `do-nothing`
content/docs/1.5.3/references/settings.md:327:> Default: `600`
content/docs/1.5.3/references/settings.md:335:> Default: `if-not-present`
content/docs/1.5.3/references/settings.md:351:> Default: `60`
content/docs/1.5.3/references/settings.md:356:> Default: `300`
content/docs/1.5.3/references/settings.md:360:> **Note:**
content/docs/1.5.3/references/settings.md:361:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.3/references/settings.md:362:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.3/references/settings.md:365:> Default: `8`
content/docs/1.5.3/references/settings.md:376:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.3/references/settings.md:380:> Default: `1`
content/docs/1.5.3/references/settings.md:390:> Default: `false`
content/docs/1.5.3/references/settings.md:396:> Default: `30`
content/docs/1.5.3/references/settings.md:403:> Default: `false`
content/docs/1.5.3/references/settings.md:407:> **Warning**
content/docs/1.5.3/references/settings.md:408:>
content/docs/1.5.3/references/settings.md:409:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.3/references/settings.md:410:>
content/docs/1.5.3/references/settings.md:411:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.3/references/settings.md:412:>
content/docs/1.5.3/references/settings.md:413:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.3/references/settings.md:417:> Default: `enabled`
content/docs/1.5.3/references/settings.md:429:> Default: `fast-check`
content/docs/1.5.3/references/settings.md:438:> Default: `false`
content/docs/1.5.3/references/settings.md:444:> Default: `0 0 */7 * *`
content/docs/1.5.3/references/settings.md:447:> **Warning**
content/docs/1.5.3/references/settings.md:448:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.3/references/settings.md:454:> Default: `false`
content/docs/1.5.3/references/settings.md:462:> Default: `false`
content/docs/1.5.3/references/settings.md:466:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.3/references/settings.md:470:> Examples:  
content/docs/1.5.3/references/settings.md:471:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.3/references/settings.md:472:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.3/references/settings.md:473:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.3/references/settings.md:479:> Example: `s3-secret`
content/docs/1.5.3/references/settings.md:485:> Default: `300`
content/docs/1.5.3/references/settings.md:493:> Default: `1440`
content/docs/1.5.3/references/settings.md:501:> Default: `1`
content/docs/1.5.3/references/settings.md:510:> Default: `1`
content/docs/1.5.3/references/settings.md:518:> Default: `false`
content/docs/1.5.3/references/settings.md:523:> Default: `ignored`
content/docs/1.5.3/references/settings.md:533:> Default: `lz4`
content/docs/1.5.3/references/settings.md:545:> Default: `2`
content/docs/1.5.3/references/settings.md:551:> Default: `2`
content/docs/1.5.3/references/settings.md:559:> Default: `true`
content/docs/1.5.3/references/settings.md:563:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.3/references/settings.md:567:> Default: `true`
content/docs/1.5.3/references/settings.md:575:> Default: `false`
content/docs/1.5.3/references/settings.md:583:> Default: `true`
content/docs/1.5.3/references/settings.md:589:> **Note:**
content/docs/1.5.3/references/settings.md:590:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.3/references/settings.md:591:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.3/references/settings.md:595:> Default: `disabled`
content/docs/1.5.3/references/settings.md:611:> Default: `ignored`
content/docs/1.5.3/references/settings.md:625:> Default: `25`
content/docs/1.5.3/references/settings.md:633:> Default: `100`
content/docs/1.5.3/references/settings.md:643:> Default: `30`
content/docs/1.5.3/references/settings.md:653:> Default: `5`
content/docs/1.5.3/references/settings.md:659:> **WARNING:**
content/docs/1.5.3/references/settings.md:660:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.3/references/settings.md:661:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.3/references/settings.md:662:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.3/references/settings.md:667:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.3/references/settings.md:686:> Example: `high-priority`
content/docs/1.5.3/references/settings.md:697:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.3/references/settings.md:703:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.3/references/settings.md:710:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.3/references/settings.md:716:> Default: `false`
content/docs/1.5.3/references/settings.md:722:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.3/references/settings.md:726:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.3/references/settings.md:730:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.3/references/settings.md:736:> Example: `false`
content/docs/1.5.3/references/settings.md:748:> Default: `12`
content/docs/1.5.3/references/settings.md:760:> **Warning:**
content/docs/1.5.3/references/settings.md:761:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.3/references/settings.md:762:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.3/references/settings.md:763:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.3/references/settings.md:764:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.3/references/settings.md:765:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.3/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.3/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.3/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.3/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.3/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.3/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.3/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.3/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.3/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.3/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.3/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.3/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.3/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.3/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.3/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.3/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.3/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.3/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.3/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.3/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.3/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.3/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.3/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.3/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.3/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.3/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.3/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.3/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.3/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.3/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.3/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.3/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.3/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.3/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.3/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.3/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.3/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.3/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.3/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.3/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.3/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.3/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.3/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.3/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.3/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.3/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.3/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.3/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.3/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.3/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.3/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.3/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.3/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.3/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.3/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.3/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.3/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.3/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.3/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.3/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.3/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.3/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.5.4/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.5.4/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.5.4/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:98:> **NOTE:**
content/docs/1.5.4/advanced-resources/deploy/customizing-default-settings.md:99:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.4/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.5.4/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.5.4/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.5.4/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.5.4/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.5.4/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.5.4/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.5.4/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.5.4/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.5.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.5.4/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.5.4/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.5.4/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.5.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.5.4/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.5.4/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.5.4/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.5.4/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.5.4/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.5.4/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.5.4/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.5.4/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.5.4/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.5.4/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.5.4/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.5.4/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.5.4/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.5.4/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.4/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.5.4/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.5.4/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.5.4/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.5.4/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.5.4/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.5.4/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.5.4/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.5.4/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.5.4/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.4/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.5.4/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.5.4/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.5.4/references/storage-class-parameters.md:50:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.5.4/references/storage-class-parameters.md:53:> Default: `true`  
content/docs/1.5.4/references/storage-class-parameters.md:54:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.5.4/references/storage-class-parameters.md:57:> Default: `Delete`  
content/docs/1.5.4/references/storage-class-parameters.md:58:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.5.4/references/storage-class-parameters.md:61:> Default `[]`  
content/docs/1.5.4/references/storage-class-parameters.md:62:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.5.4/references/storage-class-parameters.md:65:> Default `Immediate`  
content/docs/1.5.4/references/storage-class-parameters.md:66:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.5.4/references/storage-class-parameters.md:73:> Default: `3`  
content/docs/1.5.4/references/storage-class-parameters.md:79:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.5.4/references/storage-class-parameters.md:82:> Default: `30`
content/docs/1.5.4/references/storage-class-parameters.md:87:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:88:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.5.4/references/storage-class-parameters.md:93:> Default: `ext4`  
content/docs/1.5.4/references/storage-class-parameters.md:94:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.4/references/storage-class-parameters.md:97:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:98:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.5.4/references/storage-class-parameters.md:101:> Default: `false`  
content/docs/1.5.4/references/storage-class-parameters.md:106:> Default: `false`  
content/docs/1.5.4/references/storage-class-parameters.md:107:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.5.4/references/storage-class-parameters.md:110:> Default: `disabled`  
content/docs/1.5.4/references/storage-class-parameters.md:117:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.5.4/references/storage-class-parameters.md:118:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.5.4/references/storage-class-parameters.md:121:> Default: `ignored`  
content/docs/1.5.4/references/storage-class-parameters.md:127:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.5.4/references/storage-class-parameters.md:128:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.5.4/references/storage-class-parameters.md:131:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:132:> Example: `"ssd,fast"`  
content/docs/1.5.4/references/storage-class-parameters.md:135:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.4/references/storage-class-parameters.md:138:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:139:> Example: `"storage,fast"`  
content/docs/1.5.4/references/storage-class-parameters.md:142:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.5.4/references/storage-class-parameters.md:145:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:146:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.5.4/references/storage-class-parameters.md:149:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.5.4/references/storage-class-parameters.md:152:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:153:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.4/references/storage-class-parameters.md:156:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:157:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.4/references/storage-class-parameters.md:160:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:161:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.4/references/storage-class-parameters.md:164:> Default: `""`  
content/docs/1.5.4/references/storage-class-parameters.md:165:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.5.4/references/storage-class-parameters.md:168:> Default: `ignored`  
content/docs/1.5.4/references/storage-class-parameters.md:173:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.5.4/references/storage-class-parameters.md:174:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.5.4/references/storage-class-parameters.md:177:> Default: `false`  
content/docs/1.5.4/references/storage-class-parameters.md:179:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.5.4/references/storage-class-parameters.md:180:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.5.4/references/storage-class-parameters.md:183:> Default: `ignored`  
content/docs/1.5.4/references/storage-class-parameters.md:188:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.5.4/references/storage-class-parameters.md:189:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.5.4/references/storage-class-parameters.md:192:> Default: `ignored`  
content/docs/1.5.4/references/storage-class-parameters.md:197:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.5.4/references/storage-class-parameters.md:198:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.5.4/references/storage-class-parameters.md:201:> Default: `""`
content/docs/1.5.4/references/storage-class-parameters.md:202:> Example: `"hard,sync"`  
content/docs/1.5.4/references/storage-class-parameters.md:207:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.5.4/references/storage-class-parameters.md:210:> Default: `"v1"`  
content/docs/1.5.4/references/storage-class-parameters.md:214:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.5.4/references/storage-class-parameters.md:215:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.5.4/references/settings.md:87:> Default: `block-if-contains-last-replica`
content/docs/1.5.4/references/settings.md:111:> Default: `true`
content/docs/1.5.4/references/settings.md:117:> Default: `true`
content/docs/1.5.4/references/settings.md:124:> **Note:** This setting doesn't apply to below cases.
content/docs/1.5.4/references/settings.md:125:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.5.4/references/settings.md:126:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.5.4/references/settings.md:130:> Default: `true`
content/docs/1.5.4/references/settings.md:136:> Default: `0`
content/docs/1.5.4/references/settings.md:144:> Default: `5`
content/docs/1.5.4/references/settings.md:154:> Default: `false`
content/docs/1.5.4/references/settings.md:164:> Default: `longhorn.io/v1beta2`
content/docs/1.5.4/references/settings.md:170:> Default: `disabled`
content/docs/1.5.4/references/settings.md:188:> Default: `/var/lib/longhorn/`
content/docs/1.5.4/references/settings.md:202:> Default: `longhorn-static`
content/docs/1.5.4/references/settings.md:208:> Default: `3`
content/docs/1.5.4/references/settings.md:219:> Default: `false`
content/docs/1.5.4/references/settings.md:223:> Default: `false`
content/docs/1.5.4/references/settings.md:229:> Default: `true`
content/docs/1.5.4/references/settings.md:237:> Only available if `Upgrade Checker` is enabled.
content/docs/1.5.4/references/settings.md:241:> Default: `true`
content/docs/1.5.4/references/settings.md:322:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.5.4/references/settings.md:326:> Default: `do-nothing`
content/docs/1.5.4/references/settings.md:341:> Default: `600`
content/docs/1.5.4/references/settings.md:349:> Default: `if-not-present`
content/docs/1.5.4/references/settings.md:365:> Default: `60`
content/docs/1.5.4/references/settings.md:370:> Default: `300`
content/docs/1.5.4/references/settings.md:374:> **Note:**
content/docs/1.5.4/references/settings.md:375:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.5.4/references/settings.md:376:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.5.4/references/settings.md:379:> Default: `8`
content/docs/1.5.4/references/settings.md:390:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.5.4/references/settings.md:394:> Default: `1`
content/docs/1.5.4/references/settings.md:404:> Default: `false`
content/docs/1.5.4/references/settings.md:410:> Default: `30`
content/docs/1.5.4/references/settings.md:417:> Default: `false`
content/docs/1.5.4/references/settings.md:421:> **Warning**
content/docs/1.5.4/references/settings.md:422:>
content/docs/1.5.4/references/settings.md:423:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.5.4/references/settings.md:424:>
content/docs/1.5.4/references/settings.md:425:> - When applying the setting, Longhorn will restart all instance-manager pods.
content/docs/1.5.4/references/settings.md:426:>
content/docs/1.5.4/references/settings.md:427:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.5.4/references/settings.md:431:> Default: `enabled`
content/docs/1.5.4/references/settings.md:443:> Default: `fast-check`
content/docs/1.5.4/references/settings.md:452:> Default: `false`
content/docs/1.5.4/references/settings.md:458:> Default: `0 0 */7 * *`
content/docs/1.5.4/references/settings.md:461:> **Warning**
content/docs/1.5.4/references/settings.md:462:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.5.4/references/settings.md:468:> Default: `false`
content/docs/1.5.4/references/settings.md:476:> Default: `false`
content/docs/1.5.4/references/settings.md:480:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.5.4/references/settings.md:484:> Examples:  
content/docs/1.5.4/references/settings.md:485:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.5.4/references/settings.md:486:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.5.4/references/settings.md:487:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.5.4/references/settings.md:493:> Example: `s3-secret`
content/docs/1.5.4/references/settings.md:499:> Default: `300`
content/docs/1.5.4/references/settings.md:507:> Default: `1440`
content/docs/1.5.4/references/settings.md:515:> Default: `1`
content/docs/1.5.4/references/settings.md:524:> Default: `1`
content/docs/1.5.4/references/settings.md:532:> Default: `false`
content/docs/1.5.4/references/settings.md:537:> Default: `ignored`
content/docs/1.5.4/references/settings.md:547:> Default: `lz4`
content/docs/1.5.4/references/settings.md:559:> Default: `2`
content/docs/1.5.4/references/settings.md:565:> Default: `2`
content/docs/1.5.4/references/settings.md:573:> Default: `true`
content/docs/1.5.4/references/settings.md:577:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.5.4/references/settings.md:581:> Default: `true`
content/docs/1.5.4/references/settings.md:589:> Default: `false`
content/docs/1.5.4/references/settings.md:597:> Default: `true`
content/docs/1.5.4/references/settings.md:603:> **Note:**
content/docs/1.5.4/references/settings.md:604:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.5.4/references/settings.md:605:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.5.4/references/settings.md:609:> Default: `disabled`
content/docs/1.5.4/references/settings.md:625:> Default: `ignored`
content/docs/1.5.4/references/settings.md:639:> Default: `25`
content/docs/1.5.4/references/settings.md:647:> Default: `100`
content/docs/1.5.4/references/settings.md:657:> Default: `30`
content/docs/1.5.4/references/settings.md:667:> Default: `5`
content/docs/1.5.4/references/settings.md:673:> **WARNING:**
content/docs/1.5.4/references/settings.md:674:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.5.4/references/settings.md:675:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.5.4/references/settings.md:676:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.5.4/references/settings.md:681:> Example: `nodetype=storage:NoSchedule`
content/docs/1.5.4/references/settings.md:700:> Example: `high-priority`
content/docs/1.5.4/references/settings.md:711:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.5.4/references/settings.md:717:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.5.4/references/settings.md:724:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.5.4/references/settings.md:730:> Default: `false`
content/docs/1.5.4/references/settings.md:736:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.5.4/references/settings.md:740:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.5.4/references/settings.md:744:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.5.4/references/settings.md:750:> Example: `false`
content/docs/1.5.4/references/settings.md:762:> Default: `12`
content/docs/1.5.4/references/settings.md:774:> **Warning:**
content/docs/1.5.4/references/settings.md:775:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.5.4/references/settings.md:776:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.5.4/references/settings.md:777:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.5.4/references/settings.md:778:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.5.4/references/settings.md:779:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.5.4/references/settings.md:783:> Default: `false`
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.5.4/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.5.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.5.4/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.5.4/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.5.4/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.5.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.5.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.5.4/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.5.4/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.5.4/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.5.4/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.5.4/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.5.4/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.5.4/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.5.4/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.5.4/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.5.4/volumes-and-nodes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.5.4/best-practices.md:45:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.5.4/best-practices.md:112:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.5.4/best-practices.md:124:> Recommend: `false`
content/docs/1.5.4/best-practices.md:130:> Recommend: `false`
content/docs/1.5.4/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.5.4/troubleshoot/troubleshooting.md:17:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/1.6.0/advanced-resources/backing-image-backup.md:46:> **IMPORTANT:**
content/docs/1.6.0/advanced-resources/backing-image-backup.md:47:> - `name`: Use the same name for the backing image and its backup. If the names are not identical, Longhorn will not be able to find the backing image.
content/docs/1.6.0/advanced-resources/backing-image-backup.md:48:> - `userCreated`: Set the value to `true` to indicate that you created the backup custom resource, which enabled the creation of the backup in the backupstore. The value `false` indicates that the backup custom resource was synced from the backupstore.
content/docs/1.6.0/advanced-resources/backing-image-backup.md:49:> - `labels`: You can add labels to the backing image backup.
content/docs/1.6.0/advanced-resources/backing-image-backup.md:71:> **IMPORTANT:**
content/docs/1.6.0/advanced-resources/backing-image-backup.md:72:> - `sourceType`: Set the value to `restore`.
content/docs/1.6.0/advanced-resources/backing-image-backup.md:73:> - `sourceParameters`: Configure the following parameters:
content/docs/1.6.0/advanced-resources/backing-image-backup.md:74:>   - `backup-url`: URL of the backing image resource in the backupstore. You can find this information in the status of the backup custom resource `.Status.URL`.
content/docs/1.6.0/advanced-resources/backing-image-backup.md:75:>   - `concurrent-limit`: Maximum number of worker threads that can concurrently run for each restore operation. When unspecified, Longhorn uses the default value.
content/docs/1.6.0/advanced-resources/backing-image-backup.md:76:> - `checksum`: You can specify the expected SHA-512 checksum of the backing image file, which Longhorn uses to validate the restored file. When unspecified, Longhorn uses the checksum of the restored file as the truth.
content/docs/1.6.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/1.6.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/1.6.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/1.6.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/1.6.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:99:> **NOTE:**
content/docs/1.6.0/advanced-resources/deploy/customizing-default-settings.md:100:> Use Helm 3 when installing and upgrading Longhorn. Helm 2 is [no longer supported](https://helm.sh/blog/helm-2-becomes-unsupported/).
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.6.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/1.6.0/advanced-resources/deploy/priority-class.md:21:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/1.6.0/advanced-resources/deploy/priority-class.md:22:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/1.6.0/advanced-resources/deploy/priority-class.md:23:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/1.6.0/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/1.6.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/1.6.0/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:31:>
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:33:>
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:41:>
content/docs/1.6.0/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/1.6.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/1.6.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/1.6.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/1.6.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/1.6.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/1.6.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/1.6.0/advanced-resources/system-backup-restore/_index.md:6:> Before v1.4.0, you can restore Longhorn with third-party tools.
content/docs/1.6.0/advanced-resources/system-backup-restore/_index.md:11:> Since v1.4.0, Longhorn introduced out-of-the-box Longhorn system backup and restore.
content/docs/1.6.0/advanced-resources/system-backup-restore/_index.md:12:> - Longhorn's custom resources will be backed up and bundled into a single system backup file, then saved to the remote backup target.
content/docs/1.6.0/advanced-resources/system-backup-restore/_index.md:13:> - Later, you can choose a system backup to restore to a new cluster or restore to an existing cluster.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:41:> **Warning:** Longhorn does not backup `BackingImages`. We will improve this part in the future. See [Restore Longhorn System - Prerequisite](../restore-longhorn-system/#prerequisite) for restoring volumes created with the backing image.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:43:> **Note:** Longhorn does not backup `Nodes`. The Longhorn manager on the target cluster is responsible for creating its own Longhorn `Node` custom resources.
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:45:> **Note:**  Longhorn system backup bundle only includes resources operated by Longhorn.  
content/docs/1.6.0/advanced-resources/system-backup-restore/backup-longhorn-system.md:46:> Here is an example of a cluster workload with a bare `Pod` workload. The system backup will collect the `PersistentVolumeClaim`, `PersistentVolume`, and `Volume`. The system backup will exclude the `Pod` during system backup resource collection.
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:86:> **Warning:** Deleting the SystemRestore also deletes the associated job and will abort the remaining resource rollouts. You can [Restart the Longhorn System Restore](#restart-longhorn-system-restore) to roll out the remaining resources.
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-longhorn-system.md:126:> kubectl -n longhorn-system logs --selector=job-name=longhorn-system-rollout-<SYSTEM-RESTORE-NAME>
content/docs/1.6.0/advanced-resources/system-backup-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/1.6.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/1.6.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/1.6.0/deploy/important-notes/index.md:119:> **Warning:**
content/docs/1.6.0/deploy/important-notes/index.md:120:> Whenever engine upgrade enforcement causes upgrade failure, Longhorn allows you to revert to the previous version because Longhorn Manager will block the entire upgrade. However, Longhorn prohibits downgrading when an upgrade is successful. For more information, see [Upgrade Path Enforcement](../../deploy/upgrade/#upgrade-path-enforcement).
content/docs/1.6.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/1.6.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/1.6.0/deploy/install/install-with-argocd.md:18:> Use [this script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.0/deploy/install/install-with-fleet.md:19:> Use [this script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.0/deploy/install/install-with-flux.md:25:> Use [this script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) to check the Longhorn environment for potential issues.
content/docs/1.6.0/deploy/install/install-with-helm.md:13:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/1.6.0/deploy/install/install-with-helm.md:18:> **Note**:
content/docs/1.6.0/deploy/install/install-with-helm.md:19:> * The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/1.6.0/deploy/install/install-with-helm.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set the helm value `enablePSP` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.6.0/deploy/install/install-with-kubectl.md:58:> **Note**:
content/docs/1.6.0/deploy/install/install-with-kubectl.md:59:> For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, need to apply the [podsecuritypolicy.yaml](https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/podsecuritypolicy.yaml) manifest in addition to applying the `longhorn.yaml` manifests.
content/docs/1.6.0/deploy/install/install-with-rancher.md:19:> **Note**:
content/docs/1.6.0/deploy/install/install-with-rancher.md:20:> * For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
content/docs/1.6.0/deploy/uninstall/_index.md:66:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/1.6.0/deploy/upgrade/_index.md:30:> **Warning**:
content/docs/1.6.0/deploy/upgrade/_index.md:31:> * Upgrade path enforcement is introduced in Longhorn v1.5.0, which means that downgrading from v1.5.0 to any previous version is possible. **Please note that downgrading is not supported**.
content/docs/1.6.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/1.6.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/1.6.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.6.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/1.6.0/references/networking.md:16:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/1.6.0/references/networking.md:17:> Note: Depending on your CNI for cluster network, there might be some delay when Kubernetes applying netowk policies to the pod. This delay may fail Longhorn recurring job for taking Snapshot or Backup of the Volume since it cannot access longhorn-manager in the beginning. This is a known issue found in K3s with Traefik and is beyond Longhorn control.
content/docs/1.6.0/references/settings.md:94:> Default: `block-if-contains-last-replica`
content/docs/1.6.0/references/settings.md:118:> Default: `false`
content/docs/1.6.0/references/settings.md:125:> Default: `true`
content/docs/1.6.0/references/settings.md:131:> Default: `true`
content/docs/1.6.0/references/settings.md:139:> Default: `true`
content/docs/1.6.0/references/settings.md:146:> **Note:** This setting doesn't apply to below cases.
content/docs/1.6.0/references/settings.md:147:> - The workload pods don't have a controller; Longhorn never deletes them.
content/docs/1.6.0/references/settings.md:148:> - The volumes used by workloads are RWX, because the Longhorn share manager, which provides the RWX NFS service, has its own resilience mechanism to ensure availability until the volume gets reattached without relying on the pod lifecycle to trigger volume reattachment. For details, see [here](../../advanced-resources/rwx-workloads).
content/docs/1.6.0/references/settings.md:152:> Default: `true`
content/docs/1.6.0/references/settings.md:158:> Default: `0`
content/docs/1.6.0/references/settings.md:166:> Default: `5`
content/docs/1.6.0/references/settings.md:176:> Default: `false`
content/docs/1.6.0/references/settings.md:186:> Default: `longhorn.io/v1beta2`
content/docs/1.6.0/references/settings.md:192:> Default: `disabled`
content/docs/1.6.0/references/settings.md:210:> Default: `/var/lib/longhorn/`
content/docs/1.6.0/references/settings.md:224:> Default: `longhorn-static`
content/docs/1.6.0/references/settings.md:230:> Default: `3`
content/docs/1.6.0/references/settings.md:241:> Default: `false`
content/docs/1.6.0/references/settings.md:245:> Default: `false`
content/docs/1.6.0/references/settings.md:251:> Default: `true`
content/docs/1.6.0/references/settings.md:259:> Only available if `Upgrade Checker` is enabled.
content/docs/1.6.0/references/settings.md:263:> Default: `true`
content/docs/1.6.0/references/settings.md:348:> The `Upgrade Checker` needs to be enabled to periodically send the collected data.
content/docs/1.6.0/references/settings.md:352:> Default: `do-nothing`
content/docs/1.6.0/references/settings.md:367:> Default: `600`
content/docs/1.6.0/references/settings.md:375:> Default: `if-not-present`
content/docs/1.6.0/references/settings.md:391:> Default: `60`
content/docs/1.6.0/references/settings.md:396:> Default: `300`
content/docs/1.6.0/references/settings.md:400:> **Note:**
content/docs/1.6.0/references/settings.md:401:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/1.6.0/references/settings.md:402:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/1.6.0/references/settings.md:405:> Default: `8`
content/docs/1.6.0/references/settings.md:416:> Default: `longhornio/support-bundle-kit:v0.0.14`
content/docs/1.6.0/references/settings.md:420:> Default: `1`
content/docs/1.6.0/references/settings.md:430:> Default: `false`
content/docs/1.6.0/references/settings.md:436:> Default: `30`
content/docs/1.6.0/references/settings.md:442:> Default: `true`
content/docs/1.6.0/references/settings.md:449:> Default: `false`
content/docs/1.6.0/references/settings.md:453:> **Warning**
content/docs/1.6.0/references/settings.md:454:>
content/docs/1.6.0/references/settings.md:455:> - DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES. Longhorn will block this setting update when there are attached volumes.
content/docs/1.6.0/references/settings.md:456:>
content/docs/1.6.0/references/settings.md:457:> - When the V2 Data Engine is enabled, each instance-manager pod utilizes 1 CPU core. This high CPU usage is attributed to the spdk_tgt process running within each instance-manager pod. The spdk_tgt process is responsible for handling input/output (IO) operations and requires intensive polling. As a result, it consumes 100% of a dedicated CPU core to efficiently manage and process the IO requests, ensuring optimal performance and responsiveness for storage operations.
content/docs/1.6.0/references/settings.md:461:> Default: `2048`
content/docs/1.6.0/references/settings.md:467:> Default: `1250`
content/docs/1.6.0/references/settings.md:471:> **Warning:**
content/docs/1.6.0/references/settings.md:472:>  - Specifying a value of 0 disables CPU requests for instance manager pods. You must specify an integer between 1000 and 8000. 
content/docs/1.6.0/references/settings.md:473:>  - This is a global setting. Modifying the value triggers an automatic restart of the instance manager pods. Do not modify the value while volumes are still attached.
content/docs/1.6.0/references/settings.md:477:> Default: `enabled`
content/docs/1.6.0/references/settings.md:489:> Default: `fast-check`
content/docs/1.6.0/references/settings.md:498:> Default: `false`
content/docs/1.6.0/references/settings.md:504:> Default: `0 0 */7 * *`
content/docs/1.6.0/references/settings.md:507:> **Warning**
content/docs/1.6.0/references/settings.md:508:> Hashing snapshot disk files impacts the performance of the system. It is recommended to run data integrity checks during off-peak times and to reduce the frequency of checks.
content/docs/1.6.0/references/settings.md:514:> Default: `false`
content/docs/1.6.0/references/settings.md:522:> Default: `false`
content/docs/1.6.0/references/settings.md:526:> **Note:** During the time the volume was attached automatically, the volume is not ready for the workload. The workload will have to wait until the recurring job finishes.
content/docs/1.6.0/references/settings.md:530:> Examples:  
content/docs/1.6.0/references/settings.md:531:> `s3://backupbucket@us-east-1/backupstore`  
content/docs/1.6.0/references/settings.md:532:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore`  
content/docs/1.6.0/references/settings.md:533:> `nfs://longhorn-test-nfs-svc.default:/opt/backupstore?nfsOptions=soft,timeo=330,retrans=3`  
content/docs/1.6.0/references/settings.md:539:> Example: `s3-secret`
content/docs/1.6.0/references/settings.md:545:> Default: `300`
content/docs/1.6.0/references/settings.md:553:> Default: `1440`
content/docs/1.6.0/references/settings.md:561:> Default: `1`
content/docs/1.6.0/references/settings.md:570:> Default: `1`
content/docs/1.6.0/references/settings.md:578:> Default: `false`
content/docs/1.6.0/references/settings.md:583:> Default: `ignored`
content/docs/1.6.0/references/settings.md:593:> Default: `lz4`
content/docs/1.6.0/references/settings.md:605:> Default: `2`
content/docs/1.6.0/references/settings.md:611:> Default: `2`
content/docs/1.6.0/references/settings.md:619:> Default: `true`
content/docs/1.6.0/references/settings.md:623:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/1.6.0/references/settings.md:627:> Default: `true`
content/docs/1.6.0/references/settings.md:635:> Default: `false`
content/docs/1.6.0/references/settings.md:641:> **Note:**
content/docs/1.6.0/references/settings.md:642:>   - This setting is superseded if replicas are forbidden to share a zone by the Replica Zone Level Anti-Affinity setting.
content/docs/1.6.0/references/settings.md:646:> Default: `true`
content/docs/1.6.0/references/settings.md:652:> **Note:**
content/docs/1.6.0/references/settings.md:653:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/1.6.0/references/settings.md:654:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/1.6.0/references/settings.md:658:> Default: `true`
content/docs/1.6.0/references/settings.md:664:> **Note:**
content/docs/1.6.0/references/settings.md:665:>   - Even if the setting is "true" and disk sharing is allowed, Longhorn will seek to use a different disk if possible, even if on the same node.
content/docs/1.6.0/references/settings.md:666:>   - This setting is superseded if replicas are forbidden to share a zone or a node by either of the other Soft Anti-Affinity settings.
content/docs/1.6.0/references/settings.md:670:> Default: `disabled`
content/docs/1.6.0/references/settings.md:686:> Default: `ignored`
content/docs/1.6.0/references/settings.md:700:> Default: `25`
content/docs/1.6.0/references/settings.md:708:> Default: `100`
content/docs/1.6.0/references/settings.md:718:> Default: `30`
content/docs/1.6.0/references/settings.md:726:> Default: `true`
content/docs/1.6.0/references/settings.md:732:> Default: `true`
content/docs/1.6.0/references/settings.md:740:> Default: `5`
content/docs/1.6.0/references/settings.md:746:> **WARNING:**
content/docs/1.6.0/references/settings.md:747:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/1.6.0/references/settings.md:748:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/1.6.0/references/settings.md:749:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/1.6.0/references/settings.md:754:> Example: `nodetype=storage:NoSchedule`
content/docs/1.6.0/references/settings.md:773:> Default: `longhorn-critical`
content/docs/1.6.0/references/settings.md:784:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/1.6.0/references/settings.md:790:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/1.6.0/references/settings.md:797:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/1.6.0/references/settings.md:803:> Default: `false`
content/docs/1.6.0/references/settings.md:809:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/1.6.0/references/settings.md:813:> Example: `kube-system/demo-192-168-0-0`
content/docs/1.6.0/references/settings.md:817:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/1.6.0/references/settings.md:823:> Example: `false`
content/docs/1.6.0/references/settings.md:835:> Default: `12`
content/docs/1.6.0/references/settings.md:847:> **Warning:**
content/docs/1.6.0/references/settings.md:848:>  - Value 0 means removing the CPU requests from spec of instance manager pods.
content/docs/1.6.0/references/settings.md:849:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40.
content/docs/1.6.0/references/settings.md:850:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/1.6.0/references/settings.md:851:>  - This global setting will be ignored for a node if the field "InstanceManagerCPURequest" on the node is set.
content/docs/1.6.0/references/settings.md:852:>  - After this setting is changed, all instance manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/1.6.0/references/settings.md:856:> Default: `false`
content/docs/1.6.0/references/storage-class-parameters.md:51:> See [Kubernetes Storage Class: Provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner).  
content/docs/1.6.0/references/storage-class-parameters.md:54:> Default: `true`  
content/docs/1.6.0/references/storage-class-parameters.md:55:> See [Kubernetes Storage Class: Allow Volume Expansion](https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion).  
content/docs/1.6.0/references/storage-class-parameters.md:58:> Default: `Delete`  
content/docs/1.6.0/references/storage-class-parameters.md:59:> See [Kubernetes Storage Class: Reclaim Policy](https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy).  
content/docs/1.6.0/references/storage-class-parameters.md:62:> Default `[]`  
content/docs/1.6.0/references/storage-class-parameters.md:63:> See [Kubernetes Storage Class: Mount Options](https://kubernetes.io/docs/concepts/storage/storage-classes/#mount-options).  
content/docs/1.6.0/references/storage-class-parameters.md:66:> Default `Immediate`  
content/docs/1.6.0/references/storage-class-parameters.md:67:> See [Kubernetes Storage Class: Volume Binding Mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode).  
content/docs/1.6.0/references/storage-class-parameters.md:74:> Default: `3`  
content/docs/1.6.0/references/storage-class-parameters.md:80:> Global setting: [Default Replica Count](../settings#default-replica-count).
content/docs/1.6.0/references/storage-class-parameters.md:83:> Default: `30`
content/docs/1.6.0/references/storage-class-parameters.md:88:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:89:> Example: `"s3://backupbucket@us-east-1?volume=minio-vol01&backup=backup-eeb2782d5b2f42bb"`
content/docs/1.6.0/references/storage-class-parameters.md:94:> Default: `ext4`  
content/docs/1.6.0/references/storage-class-parameters.md:95:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.6.0/references/storage-class-parameters.md:98:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:99:> For more details, see [Creating Longhorn Volumes with Kubernetes](../../volumes-and-nodes/create-volumes#creating-longhorn-volumes-with-kubectl)
content/docs/1.6.0/references/storage-class-parameters.md:102:> Default: `false`  
content/docs/1.6.0/references/storage-class-parameters.md:107:> Default: `false`  
content/docs/1.6.0/references/storage-class-parameters.md:108:> More details in [Encrypted Volumes](../../advanced-resources/security/volume-encryption)
content/docs/1.6.0/references/storage-class-parameters.md:111:> Default: `disabled`  
content/docs/1.6.0/references/storage-class-parameters.md:118:>  Global setting: [Default Data Locality](../settings#default-data-locality)  
content/docs/1.6.0/references/storage-class-parameters.md:119:>  More details in [Data Locality](../../high-availability/data-locality).
content/docs/1.6.0/references/storage-class-parameters.md:122:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:128:> Global setting: [Replica Auto Balance](../settings#replica-auto-balance)  
content/docs/1.6.0/references/storage-class-parameters.md:129:> More details in [Auto Balance Replicas](../../high-availability/auto-balance-replicas).
content/docs/1.6.0/references/storage-class-parameters.md:132:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:133:> Example: `"ssd,fast"`  
content/docs/1.6.0/references/storage-class-parameters.md:136:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.6.0/references/storage-class-parameters.md:139:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:140:> Example: `"storage,fast"`  
content/docs/1.6.0/references/storage-class-parameters.md:143:> More details in [Storage Tags](../../volumes-and-nodes/storage-tags)
content/docs/1.6.0/references/storage-class-parameters.md:146:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:147:> Example:  `[{"name":"backup", "isGroup":true}]`  
content/docs/1.6.0/references/storage-class-parameters.md:150:>  More details in [Recurring Snapshots and Backups](../../snapshots-and-backups/scheduling-backups-and-snapshots) 
content/docs/1.6.0/references/storage-class-parameters.md:153:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:154:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.0/references/storage-class-parameters.md:157:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:158:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.0/references/storage-class-parameters.md:161:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:162:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.0/references/storage-class-parameters.md:165:> Default: `""`  
content/docs/1.6.0/references/storage-class-parameters.md:166:> See [Backing Image](../../advanced-resources/backing-image#create-and-use-a-backing-image-via-storageclass-and-pvc)
content/docs/1.6.0/references/storage-class-parameters.md:169:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:174:> Global setting: [Remove Snapshots During Filesystem Trim](../settings#remove-snapshots-during-filesystem-trim).  
content/docs/1.6.0/references/storage-class-parameters.md:175:> More details in [Trim Filesystem](../../volumes-and-nodes/trim-filesystem).
content/docs/1.6.0/references/storage-class-parameters.md:178:> Default: `false`  
content/docs/1.6.0/references/storage-class-parameters.md:180:> Global setting: [Disable Revision Counter](../settings#disable-revision-counter).  
content/docs/1.6.0/references/storage-class-parameters.md:181:> More details in [Revision Counter](../../advanced-resources/deploy/revision_counter).  
content/docs/1.6.0/references/storage-class-parameters.md:184:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:189:> Global setting: [Replica Node Level Soft Anti-Affinity](../settings#replica-node-level-soft-anti-affinity).  
content/docs/1.6.0/references/storage-class-parameters.md:190:> More details in [Scheduling](../../volumes-and-nodes/scheduling) and [Best Practices](../../best-practices#replica-node-level-soft-anti-affinity).
content/docs/1.6.0/references/storage-class-parameters.md:193:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:198:> Global setting: [Replica Zone Level Soft Anti-Affinity](../settings#replica-zone-level-soft-anti-affinity).  
content/docs/1.6.0/references/storage-class-parameters.md:199:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.6.0/references/storage-class-parameters.md:202:> Default: `ignored`  
content/docs/1.6.0/references/storage-class-parameters.md:207:> Global setting: [Replica Disk Level Soft Anti-Affinity](../settings#replica-disk-level-soft-anti-affinity).  
content/docs/1.6.0/references/storage-class-parameters.md:208:> More details in [Scheduling](../../volumes-and-nodes/scheduling).
content/docs/1.6.0/references/storage-class-parameters.md:211:> Default: `""`
content/docs/1.6.0/references/storage-class-parameters.md:212:> Example: `"hard,sync"`  
content/docs/1.6.0/references/storage-class-parameters.md:217:> More details in [RWX Workloads](../../advanced-resources/rwx-workloads/#notice)
content/docs/1.6.0/references/storage-class-parameters.md:220:> Default: `"v1"`  
content/docs/1.6.0/references/storage-class-parameters.md:224:> Global setting: [V2 Data Engine](../settings#v2-data-engine).  
content/docs/1.6.0/references/storage-class-parameters.md:225:> More details in [V2 Data Engine Quick Start](../../v2-data-engine/quick-start#create-a-storageclass).
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:182:> Note: Consider creating an IAM condition to reduce how many buckets this serviceaccount has object admin access to.
content/docs/1.6.0/snapshots-and-backups/backup-and-restore/set-backup-target.md:222:> Note: The secret can be named whatever you like as long as they match what's in longhorn's settings.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:18:> **Prerequisite**
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:>
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> **Note:** previously, the snapshot controller YAML files were deployed into the `default` namespace by default.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:> The updated YAML files are being deployed into `kube-system` namespace by default.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> Therefore, we suggest deleting the previous snapshot controller in the `default` namespace to avoid having multiple snapshot controllers.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/1.6.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backing-image.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/1.6.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/1.6.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/1.6.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:30:> Note: When the PVC has recurring job labels, they will override all recurring job labels of the associated Volume.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/1.6.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:36:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/1.6.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/1.6.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/1.6.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/1.6.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/1.6.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/1.6.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/1.6.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/1.6.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/1.6.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/1.6.0/volumes-and-nodes/trim-filesystem.md:8:> **Note:**
content/docs/1.6.0/volumes-and-nodes/trim-filesystem.md:9:> - Trying to trim a removed files from a valid snapshot will do nothing but the filesystem will discard this kind of in-memory trimmable file info. Later on if you mark the snapshot as removed and want to retry the trim, you may need to unmount and remount the filesystem so that the filesystem can recollect the trimmable file info.
content/docs/1.6.0/volumes-and-nodes/trim-filesystem.md:10:>
content/docs/1.6.0/volumes-and-nodes/trim-filesystem.md:11:> - If you allow automatically removing snapshots during filesystem trim, please be careful of using mount option `discard`, which will trigger the snapshot removal frequently then interrupt some operations like backup creation.
content/docs/1.6.0/volumes-and-nodes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/1.6.0/best-practices.md:52:> **Note:** CentOS Linux has been removed from the verified OS list below, as it has been discontinued in favor of CentOS Stream [[ref](https://www.redhat.com/en/blog/faq-centos-stream-updates#Q5)], a rolling-release Linux distribution. Our focus for verifying RHEL-based downstream open source distributions will be enterprise-grade, such as Rocky and Oracle Linux.
content/docs/1.6.0/best-practices.md:176:> **Note:** This field will overwrite the above setting for the specified node.
content/docs/1.6.0/best-practices.md:192:> Recommend: `false`
content/docs/1.6.0/best-practices.md:198:> Recommend: `false`
content/docs/1.6.0/troubleshoot/support-bundle.md:12:> **Note:** The support bundle manager will use a dedicated `longhorn-support-bundle` service account and `longhorn-support-bundle` cluster role binding with `cluster-admin` access for bundle collection.
content/docs/1.6.0/troubleshoot/troubleshooting.md:28:> You can generate a support bundle file for offline troubleshooting. See [Support Bundle](../support-bundle) for detail.
content/docs/archives/0.8.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/0.8.0/install/viewing-ui.md:6:> For Longhorn v0.8.0+, UI service type has been changed from `LoadBalancer` to `ClusterIP`
content/docs/archives/0.8.0/users-guide/longhorn-ingress.md:7:> It's important the file generated is named auth (actually - that the secret has a key data.auth), otherwise the ingress-controller returns a 503
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/0.8.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:299:> "Dropping Longhorn node longhorn-system/**NODE_NAME** out of the queue: fail to sync node for longhorn-system/**NODE_NAME**:
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:300:> InstanceManager.longhorn.io \"instance-manager-e-605e9473\" is invalid: metadata.labels: Invalid value:
content/docs/archives/0.8.1/advanced-resources/deploy/airgap.md:301:> \"**PRIVATE_REGISTRY_URL**-**PREFIX**-longhorn-instance-manager-v1_20200301\": **must be no more than 63 characters**"
content/docs/archives/0.8.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/0.8.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/0.8.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/0.8.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there, 
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:12:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:206:> These steps should not be executed if you want to maintain the ability to [roll back](#rollbacks) from a v0.7.0 installation.
content/docs/archives/0.8.1/deploy/upgrade/longhorn-manager.md:225:> **Prerequisite:** To rollback from v0.7.0 installation, you must not have [cleaned up the v0.6.2 CRDs.](#clean-up-the-v062-crds)
content/docs/archives/0.8.1/high-availability/recover-volume.md:8:> This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/0.8.1/references/settings.md:32:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/0.8.1/references/settings.md:37:> Example: `s3-secret`
content/docs/archives/0.8.1/references/settings.md:42:> Example: `300`
content/docs/archives/0.8.1/references/settings.md:49:> Example: `false`
content/docs/archives/0.8.1/references/settings.md:58:> Example: `/var/lib/longhorn`
content/docs/archives/0.8.1/references/settings.md:65:> Example: `longhornio/longhorn-engine:v0.6.0`
content/docs/archives/0.8.1/references/settings.md:72:> Example: `true`
content/docs/archives/0.8.1/references/settings.md:77:> Example: `v0.6.0`
content/docs/archives/0.8.1/references/settings.md:84:> Example: `3`
content/docs/archives/0.8.1/references/settings.md:91:> Example: `0.2`
content/docs/archives/0.8.1/references/settings.md:95:> **Warning:** The system may fail to start or become stuck while using this feature due to the resource constraint. Disabled (\"0\") by default.
content/docs/archives/0.8.1/references/settings.md:100:>Example: `longhorn-static`
content/docs/archives/0.8.1/references/settings.md:105:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/0.8.1/references/settings.md:117:> Example: `true`
content/docs/archives/0.8.1/references/settings.md:124:> Example: `500`
content/docs/archives/0.8.1/references/settings.md:131:> Example: `10`
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/0.8.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/0.8.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/0.8.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.0.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.0.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:299:> "Dropping Longhorn node longhorn-system/**NODE_NAME** out of the queue: fail to sync node for longhorn-system/**NODE_NAME**:
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:300:> InstanceManager.longhorn.io \"instance-manager-e-605e9473\" is invalid: metadata.labels: Invalid value:
content/docs/archives/1.0.0/advanced-resources/deploy/airgap.md:301:> \"**PRIVATE_REGISTRY_URL**-**PREFIX**-longhorn-instance-manager-v1_20200301\": **must be no more than 63 characters**"
content/docs/archives/1.0.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.0.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.0.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:104:> GET / HTTP/1.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:105:> Host: 97.107.142.125
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:106:> User-Agent: curl/7.64.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:107:> Accept: */*
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:108:>
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:132:> GET / HTTP/1.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:133:> Host: 97.107.142.125
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:134:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:135:> User-Agent: curl/7.64.1
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:136:> Accept: */*
content/docs/archives/1.0.0/deploy/accessing-the-ui/longhorn-ingress.md:137:>
content/docs/archives/1.0.0/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there, 
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:52:> **Note**: Please make sure there is no PV running with driver `io.rancher.longhorn`.
content/docs/archives/1.0.0/deploy/upgrade/longhorn-manager.md:66:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.0.0/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.0/references/settings.md:46:> Default: `false`
content/docs/archives/1.0.0/references/settings.md:55:> Default: `/var/lib/longhorn/`
content/docs/archives/1.0.0/references/settings.md:62:> Default: `longhornio/longhorn-engine:v{{< current-version >}}` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.0/references/settings.md:69:> Default: `longhornio/longhorn-instance-manager:v1_20200514` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.0/references/settings.md:74:> Default: `true`
content/docs/archives/1.0.0/references/settings.md:79:> Default: `v{{< current-version >}}` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.0/references/settings.md:86:> Default: `3`
content/docs/archives/1.0.0/references/settings.md:93:> Default: `longhorn-static`
content/docs/archives/1.0.0/references/settings.md:98:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.0.0/references/settings.md:103:> Default: `true`
content/docs/archives/1.0.0/references/settings.md:112:> Default: `wait`
content/docs/archives/1.0.0/references/settings.md:127:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.0.0/references/settings.md:132:> Example: `s3-secret`
content/docs/archives/1.0.0/references/settings.md:137:> Default: `300`
content/docs/archives/1.0.0/references/settings.md:147:> Default: `false`
content/docs/archives/1.0.0/references/settings.md:154:> Default: `200`
content/docs/archives/1.0.0/references/settings.md:163:> Default: `25`
content/docs/archives/1.0.0/references/settings.md:170:> Default: `true`
content/docs/archives/1.0.0/references/settings.md:177:> Default: `true`
content/docs/archives/1.0.0/references/settings.md:183:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.0.0/references/settings.md:188:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.0.0/references/settings.md:200:> Default: `0.25`
content/docs/archives/1.0.0/references/settings.md:206:> **Warning:** This setting should be changed only when all the volumes on the nodes are detached. Changing the setting will result in all the Instance Manager Pods restarting, which will automatically detach all the attached volumes, and could cause a workload outage.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.0.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.0.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.0.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.0.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.0.1/advanced-resources/deploy/priority-class.md:15:> **Warning:** Longhorn will not start if Priority Class setting is invalid (such as the Priority Class not existing). You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`. You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.0.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.0.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.0.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:104:> GET / HTTP/1.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:105:> Host: 97.107.142.125
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:106:> User-Agent: curl/7.64.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:107:> Accept: */*
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:108:>
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:132:> GET / HTTP/1.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:133:> Host: 97.107.142.125
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:134:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:135:> User-Agent: curl/7.64.1
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:136:> Accept: */*
content/docs/archives/1.0.1/deploy/accessing-the-ui/longhorn-ingress.md:137:>
content/docs/archives/1.0.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there, 
content/docs/archives/1.0.1/deploy/upgrade/longhorn-manager.md:26:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.0.1/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.1/references/settings.md:47:> Default: `false`
content/docs/archives/1.0.1/references/settings.md:56:> Default: `/var/lib/longhorn/`
content/docs/archives/1.0.1/references/settings.md:63:> Default: `longhornio/longhorn-engine:v{{< current-version >}}` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.1/references/settings.md:70:> Default: `longhornio/longhorn-instance-manager:v1_20200514` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.1/references/settings.md:75:> Default: `true`
content/docs/archives/1.0.1/references/settings.md:80:> Default: `v{{< current-version >}}` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.1/references/settings.md:87:> Default: `3`
content/docs/archives/1.0.1/references/settings.md:94:> Default: `longhorn-static`
content/docs/archives/1.0.1/references/settings.md:99:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.0.1/references/settings.md:104:> Default: `true`
content/docs/archives/1.0.1/references/settings.md:113:> Default: `wait`
content/docs/archives/1.0.1/references/settings.md:128:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.0.1/references/settings.md:133:> Example: `s3-secret`
content/docs/archives/1.0.1/references/settings.md:138:> Default: `300`
content/docs/archives/1.0.1/references/settings.md:148:> Default: `false`
content/docs/archives/1.0.1/references/settings.md:155:> Default: `200`
content/docs/archives/1.0.1/references/settings.md:164:> Default: `25`
content/docs/archives/1.0.1/references/settings.md:171:> Default: `true`
content/docs/archives/1.0.1/references/settings.md:178:> Default: `true`
content/docs/archives/1.0.1/references/settings.md:184:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.0.1/references/settings.md:189:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.0.1/references/settings.md:201:> Default: `0.25`
content/docs/archives/1.0.1/references/settings.md:207:> **Warning:** This setting should be changed only when all the volumes on the nodes are detached. Changing the setting will result in all the Instance Manager Pods restarting, which will automatically detach all the attached volumes, and could cause a workload outage.
content/docs/archives/1.0.1/references/settings.md:222:> Example: `high-priority`
content/docs/archives/1.0.1/references/settings.md:228:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.0.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.0.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.0.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.0.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.0.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.0.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:195:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:295:> The `Private registry secret` must be specified in the default settings so that Longhorn can create a secret under the name. The process for setting up the private registry will be fixed to be less confusing, but not in the v1.0.2 release. More detail is at [this comment.](https://github.com/longhorn/longhorn/issues/1670#issuecomment-670723484)
content/docs/archives/1.0.2/advanced-resources/deploy/airgap.md:309:> The `Private registry secret` must be specified in the default settings so that Longhorn can create a secret under the name. The process for setting up the private registry will be fixed to be less confusing, but not in the v1.0.2 release. More detail is at [this comment.](https://github.com/longhorn/longhorn/issues/1670#issuecomment-670723484)
content/docs/archives/1.0.2/advanced-resources/deploy/priority-class.md:15:> **Warning:** Longhorn will not start if Priority Class setting is invalid (such as the Priority Class not existing). You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`. You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.0.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.0.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.0.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:104:> GET / HTTP/1.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:105:> Host: 97.107.142.125
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:106:> User-Agent: curl/7.64.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:107:> Accept: */*
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:108:>
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:132:> GET / HTTP/1.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:133:> Host: 97.107.142.125
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:134:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:135:> User-Agent: curl/7.64.1
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:136:> Accept: */*
content/docs/archives/1.0.2/deploy/accessing-the-ui/longhorn-ingress.md:137:>
content/docs/archives/1.0.2/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.0.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.0.2/high-availability/recover-volume.md:8:> **Note:** This section assumes familiarity with Linux storage concepts such as attaching and mounting volumes, and [Kubernetes configuration of persistent volume storage.](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-pod)
content/docs/archives/1.0.2/references/settings.md:47:> Default: `false`
content/docs/archives/1.0.2/references/settings.md:56:> Default: `/var/lib/longhorn/`
content/docs/archives/1.0.2/references/settings.md:63:> Default: `longhornio/longhorn-engine:v{{< current-version >}}` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.2/references/settings.md:70:> Default: `longhornio/longhorn-instance-manager:v1_20200514` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.2/references/settings.md:75:> Default: `true`
content/docs/archives/1.0.2/references/settings.md:80:> Default: `v{{< current-version >}}` for Longhorn v{{< current-version >}}
content/docs/archives/1.0.2/references/settings.md:87:> Default: `3`
content/docs/archives/1.0.2/references/settings.md:94:> Default: `longhorn-static`
content/docs/archives/1.0.2/references/settings.md:99:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.0.2/references/settings.md:104:> Default: `true`
content/docs/archives/1.0.2/references/settings.md:113:> Default: `wait`
content/docs/archives/1.0.2/references/settings.md:128:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.0.2/references/settings.md:133:> Example: `s3-secret`
content/docs/archives/1.0.2/references/settings.md:138:> Default: `300`
content/docs/archives/1.0.2/references/settings.md:148:> Default: `false`
content/docs/archives/1.0.2/references/settings.md:155:> Default: `200`
content/docs/archives/1.0.2/references/settings.md:164:> Default: `25`
content/docs/archives/1.0.2/references/settings.md:171:> Default: `true`
content/docs/archives/1.0.2/references/settings.md:178:> Default: `true`
content/docs/archives/1.0.2/references/settings.md:184:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.0.2/references/settings.md:189:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.0.2/references/settings.md:201:> Default: `0.25`
content/docs/archives/1.0.2/references/settings.md:207:> **Warning:** This setting should be changed only when all the volumes on the nodes are detached. Changing the setting will result in all the Instance Manager Pods restarting, which will automatically detach all the attached volumes, and could cause a workload outage.
content/docs/archives/1.0.2/references/settings.md:222:> Example: `high-priority`
content/docs/archives/1.0.2/references/settings.md:228:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.0.2/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.0.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.0.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.0.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.0.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.1.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.1.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.1.0/advanced-resources/deploy/priority-class.md:15:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing). You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`. You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.1.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.1.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.1.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.1.0/best-practices.md:103:> Recommend: `false`
content/docs/archives/1.1.0/best-practices.md:108:> Recommend: `false`
content/docs/archives/1.1.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:104:> GET / HTTP/1.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:105:> Host: 97.107.142.125
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:106:> User-Agent: curl/7.64.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:107:> Accept: */*
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:108:>
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:132:> GET / HTTP/1.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:133:> Host: 97.107.142.125
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:134:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:135:> User-Agent: curl/7.64.1
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:136:> Accept: */*
content/docs/archives/1.1.0/deploy/accessing-the-ui/longhorn-ingress.md:137:>
content/docs/archives/1.1.0/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.1.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.1.0/references/settings.md:59:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:68:> Default: `/var/lib/longhorn/`
content/docs/archives/1.1.0/references/settings.md:75:> Default: `longhornio/longhorn-engine:v{{< current-version >}}` for Longhorn v{{< current-version >}}
content/docs/archives/1.1.0/references/settings.md:82:> Default: `longhornio/longhorn-instance-manager:v1_20200514` for Longhorn v{{< current-version >}}
content/docs/archives/1.1.0/references/settings.md:87:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:92:> Default: `v{{< current-version >}}` for Longhorn v{{< current-version >}}
content/docs/archives/1.1.0/references/settings.md:99:> Default: `3`
content/docs/archives/1.1.0/references/settings.md:106:> Default: `disabled`
content/docs/archives/1.1.0/references/settings.md:120:> Default: `longhorn-static`
content/docs/archives/1.1.0/references/settings.md:125:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.1.0/references/settings.md:130:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:135:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:149:> Default: `wait`
content/docs/archives/1.1.0/references/settings.md:158:> Default: `do-nothing`
content/docs/archives/1.1.0/references/settings.md:172:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:177:> Default: `if-not-present`
content/docs/archives/1.1.0/references/settings.md:188:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:195:> Default: `600`
content/docs/archives/1.1.0/references/settings.md:202:> Default: `if-not-present`
content/docs/archives/1.1.0/references/settings.md:219:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.1.0/references/settings.md:224:> Example: `s3-secret`
content/docs/archives/1.1.0/references/settings.md:229:> Default: `300`
content/docs/archives/1.1.0/references/settings.md:236:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:245:> Default: `false`
content/docs/archives/1.1.0/references/settings.md:252:> Default: `200`
content/docs/archives/1.1.0/references/settings.md:261:> Default: `25`
content/docs/archives/1.1.0/references/settings.md:268:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:275:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:281:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.1.0/references/settings.md:284:> Default: `true`
content/docs/archives/1.1.0/references/settings.md:288:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.1.0/references/settings.md:293:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.1.0/references/settings.md:305:> Default: `0.25`
content/docs/archives/1.1.0/references/settings.md:319:> **Warning:** This setting should be changed only when all the volumes on the nodes are detached. Changing the setting will result in all the Instance Manager Pods restarting, which will automatically detach all the attached volumes, and could cause a workload outage.
content/docs/archives/1.1.0/references/settings.md:334:> Example: `high-priority`
content/docs/archives/1.1.0/references/settings.md:340:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.1.0/references/settings.md:345:> Default: `false`
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.1.0/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> CSI snapshot support is available for Kubernetes versions >= **1.17**.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:12:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:13:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:36:> **Prerequisite**
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:37:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:41:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:42:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.1.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:22:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't new data for a long time, Longhorn does the following:
content/docs/archives/1.1.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:23:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.1.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:24:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.1.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.1.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.1.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.1.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.1.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.1.0/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.1.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.1.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.1.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.1.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.1.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.1.1/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.1.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.1.1/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.1.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.1.1/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.1.1/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.1.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:110:> GET / HTTP/1.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:111:> Host: 97.107.142.125
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:112:> User-Agent: curl/7.64.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:113:> Accept: */*
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:114:>
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:138:> GET / HTTP/1.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:139:> Host: 97.107.142.125
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:140:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:141:> User-Agent: curl/7.64.1
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> Accept: */*
content/docs/archives/1.1.1/deploy/accessing-the-ui/longhorn-ingress.md:143:>
content/docs/archives/1.1.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.1.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.1.1/references/settings.md:62:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:70:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:76:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:87:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:93:> Default: `0`
content/docs/archives/1.1.1/references/settings.md:101:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:115:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.1.1/references/settings.md:121:> Default: `disabled`
content/docs/archives/1.1.1/references/settings.md:136:> Default: `/var/lib/longhorn/`
content/docs/archives/1.1.1/references/settings.md:154:> Default: `longhorn-static`
content/docs/archives/1.1.1/references/settings.md:160:> Default: `3`
content/docs/archives/1.1.1/references/settings.md:172:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:178:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:190:> Default: `do-nothing`
content/docs/archives/1.1.1/references/settings.md:205:> Default: `600`
content/docs/archives/1.1.1/references/settings.md:213:> Default: `if-not-present`
content/docs/archives/1.1.1/references/settings.md:229:> Default: `wait`
content/docs/archives/1.1.1/references/settings.md:238:> Default: `60`
content/docs/archives/1.1.1/references/settings.md:246:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:254:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.1.1/references/settings.md:260:> Example: `s3-secret`
content/docs/archives/1.1.1/references/settings.md:266:> Default: `300`
content/docs/archives/1.1.1/references/settings.md:276:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:280:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.1.1/references/settings.md:284:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:292:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:300:> Default: `true`
content/docs/archives/1.1.1/references/settings.md:306:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.1.1/references/settings.md:310:> Default: `25`
content/docs/archives/1.1.1/references/settings.md:318:> Default: `200`
content/docs/archives/1.1.1/references/settings.md:330:> Default: `false`
content/docs/archives/1.1.1/references/settings.md:336:> Default: `12`
content/docs/archives/1.1.1/references/settings.md:348:> **Warning:**
content/docs/archives/1.1.1/references/settings.md:349:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.1.1/references/settings.md:350:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.1.1/references/settings.md:351:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.1/references/settings.md:352:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.1.1/references/settings.md:353:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.1/references/settings.md:357:> Default: `12`
content/docs/archives/1.1.1/references/settings.md:369:> **Warning:**
content/docs/archives/1.1.1/references/settings.md:370:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.1.1/references/settings.md:371:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.1.1/references/settings.md:372:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.1/references/settings.md:373:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.1.1/references/settings.md:374:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.1/references/settings.md:379:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.1.1/references/settings.md:398:> Example: `high-priority`
content/docs/archives/1.1.1/references/settings.md:407:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.1.1/references/settings.md:413:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.1.1/references/settings.md:420:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.1/references/settings.md:428:> Default:
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:12:> **Warning:** Restore backup on XFS filesystem cluster
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:13:>
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:14:> The restored data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:15:>
content/docs/archives/1.1.1/snapshots-and-backups/backup-and-restore/restore-from-a-backup.md:16:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> CSI snapshot support is available for Kubernetes versions >= **1.17**.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:12:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:13:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:36:> **Prerequisite**
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:37:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:41:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:42:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.1.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:22:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't new data for a long time, Longhorn does the following:
content/docs/archives/1.1.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:23:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.1.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:24:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:15:> **Warning:** Disaster recovery volume on XFS filesystem cluster
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:16:>
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> The DR volume data can differ from the backup on the XFS file system. This is due to the finding in https://github.com/longhorn/longhorn/issues/2503#issuecomment-828158607.
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:18:>
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:19:> If your cluster is already on the XFS file system and needs to restore from backup before the upgrade. We suggest restoring to an ext4 filesystem cluster to avoid data loss.
content/docs/archives/1.1.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:23:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.1.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.1.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.1.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.1.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.1.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.1.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.1.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.1.1/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.1.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.1.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.1.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.2/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.1.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.1.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.1.2/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.1.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:58:> **Warning**:
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:59:>
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:60:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:61:>
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:62:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:63:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:64:>
content/docs/archives/1.1.2/advanced-resources/deploy/taint-toleration.md:65:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.1.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.1.2/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.1.2/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.1.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:110:> GET / HTTP/1.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:111:> Host: 97.107.142.125
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:112:> User-Agent: curl/7.64.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:113:> Accept: */*
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:114:>
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:138:> GET / HTTP/1.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:139:> Host: 97.107.142.125
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:140:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:141:> User-Agent: curl/7.64.1
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> Accept: */*
content/docs/archives/1.1.2/deploy/accessing-the-ui/longhorn-ingress.md:143:>
content/docs/archives/1.1.2/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.1.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:14:> **Note:**
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:15:> There is a bug in the instance manager image `v1_20201216` shipped in Longhorn v1.1.0 and v1.1.1
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:16:> which can lead to a deadlock in a big cluster with hundreds of volumes.
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:17:> See more details [here](https://github.com/longhorn/longhorn/issues/2697).
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:18:> Longhorn v1.1.2 is shipped with a new instance manager image `v1_20210621` which fixed the deadlock
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:19:> but engine/replica processes of volumes don't migrate from old instance manager to new instance manager
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:20:> until the next time the volumes are detached/attached because Longhorn doesn't want to interrupt the
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:21:> data plane of the volumes.
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:22:>
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:23:> To reduce the chance of hitting the deadlock while the engine/replica processes are still in the old instance manager,
content/docs/archives/1.1.2/deploy/upgrade/upgrade-engine.md:24:> you should upgrade engine for volumes in small batches, e.g., upgrading 2 or 3 volumes at a time.
content/docs/archives/1.1.2/deploy/upgrade/_index.md:23:> **Note:**
content/docs/archives/1.1.2/deploy/upgrade/_index.md:24:> There is a bug in the instance manager image `v1_20201216` shipped in Longhorn v1.1.0 and v1.1.1
content/docs/archives/1.1.2/deploy/upgrade/_index.md:25:> which can lead to a deadlock in a big cluster with hundreds of volumes.
content/docs/archives/1.1.2/deploy/upgrade/_index.md:26:> See more details [here](https://github.com/longhorn/longhorn/issues/2697).
content/docs/archives/1.1.2/deploy/upgrade/_index.md:27:> Longhorn v1.1.2 is shipped with a new instance manager image `v1_20210621` which fixed the deadlock
content/docs/archives/1.1.2/deploy/upgrade/_index.md:28:> but engine/replica processes of volumes don't migrate from the old instance manager to the new instance manager
content/docs/archives/1.1.2/deploy/upgrade/_index.md:29:> until the next time the volumes are detached/attached. Longhorn does it because we don't want to interrupt the
content/docs/archives/1.1.2/deploy/upgrade/_index.md:30:> data plane of the volumes.
content/docs/archives/1.1.2/deploy/upgrade/_index.md:31:>
content/docs/archives/1.1.2/deploy/upgrade/_index.md:32:> If you hit the deadlock in the old instance manager, please follow the recovering steps [here](https://github.com/longhorn/longhorn/issues/2697#issuecomment-879374809)
content/docs/archives/1.1.2/references/settings.md:61:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:69:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:75:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:86:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:92:> Default: `0`
content/docs/archives/1.1.2/references/settings.md:100:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:114:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.1.2/references/settings.md:120:> Default: `disabled`
content/docs/archives/1.1.2/references/settings.md:135:> Default: `/var/lib/longhorn/`
content/docs/archives/1.1.2/references/settings.md:153:> Default: `longhorn-static`
content/docs/archives/1.1.2/references/settings.md:159:> Default: `3`
content/docs/archives/1.1.2/references/settings.md:171:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:177:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:189:> Default: `do-nothing`
content/docs/archives/1.1.2/references/settings.md:204:> Default: `600`
content/docs/archives/1.1.2/references/settings.md:212:> Default: `if-not-present`
content/docs/archives/1.1.2/references/settings.md:227:> Default: `60`
content/docs/archives/1.1.2/references/settings.md:235:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:243:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.1.2/references/settings.md:249:> Example: `s3-secret`
content/docs/archives/1.1.2/references/settings.md:255:> Default: `300`
content/docs/archives/1.1.2/references/settings.md:265:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:269:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.1.2/references/settings.md:273:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:281:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:289:> Default: `true`
content/docs/archives/1.1.2/references/settings.md:295:> **Note:** Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.1.2/references/settings.md:299:> Default: `25`
content/docs/archives/1.1.2/references/settings.md:307:> Default: `200`
content/docs/archives/1.1.2/references/settings.md:319:> Default: `false`
content/docs/archives/1.1.2/references/settings.md:325:> Default: `12`
content/docs/archives/1.1.2/references/settings.md:337:> **Warning:**
content/docs/archives/1.1.2/references/settings.md:338:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.1.2/references/settings.md:339:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.1.2/references/settings.md:340:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.2/references/settings.md:341:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.1.2/references/settings.md:342:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.2/references/settings.md:346:> Default: `12`
content/docs/archives/1.1.2/references/settings.md:358:> **Warning:**
content/docs/archives/1.1.2/references/settings.md:359:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.1.2/references/settings.md:360:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.1.2/references/settings.md:361:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.2/references/settings.md:362:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.1.2/references/settings.md:363:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.2/references/settings.md:368:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.1.2/references/settings.md:387:> Example: `high-priority`
content/docs/archives/1.1.2/references/settings.md:396:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.1.2/references/settings.md:402:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.1.2/references/settings.md:409:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.2/references/settings.md:417:> Default:
content/docs/archives/1.1.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> CSI snapshot support is available for Kubernetes versions >= **1.17**.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:12:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:13:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:36:> **Prerequisite**
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:37:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:41:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:42:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.1.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:22:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't new data for a long time, Longhorn does the following:
content/docs/archives/1.1.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:23:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.1.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:24:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.1.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.1.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.1.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.1.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.1.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.1.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.1.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.1.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.1.2/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.1.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.1.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.1.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.3/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.1.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.1.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.1.3/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.1.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.1.3/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.1.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.1.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.1.3/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.1.3/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.1.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:110:> GET / HTTP/1.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:111:> Host: 97.107.142.125
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:112:> User-Agent: curl/7.64.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:113:> Accept: */*
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:114:>
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:138:> GET / HTTP/1.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:139:> Host: 97.107.142.125
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:140:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:141:> User-Agent: curl/7.64.1
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> Accept: */*
content/docs/archives/1.1.3/deploy/accessing-the-ui/longhorn-ingress.md:143:>
content/docs/archives/1.1.3/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.1.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:14:> **Note:**
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:15:> There is a bug in the instance manager image `v1_20201216` shipped in Longhorn v1.1.0 and v1.1.1
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:16:> which can lead to a deadlock in a big cluster with hundreds of volumes.
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:17:> See more details [here](https://github.com/longhorn/longhorn/issues/2697).
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:18:> Longhorn v1.1.2 is shipped with a new instance manager image `v1_20210621` which fixed the deadlock
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:19:> but engine/replica processes of volumes don't migrate from old instance manager to new instance manager
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:20:> until the next time the volumes are detached/attached because Longhorn doesn't want to interrupt the
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:21:> data plane of the volumes.
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:22:>
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:23:> To reduce the chance of hitting the deadlock while the engine/replica processes are still in the old instance manager,
content/docs/archives/1.1.3/deploy/upgrade/upgrade-engine.md:24:> you should upgrade engine for volumes in small batches, e.g., upgrading 2 or 3 volumes at a time.
content/docs/archives/1.1.3/deploy/upgrade/_index.md:23:> **Note:**
content/docs/archives/1.1.3/deploy/upgrade/_index.md:24:> There is a bug in the instance manager image `v1_20201216` shipped in Longhorn v1.1.0 and v1.1.1
content/docs/archives/1.1.3/deploy/upgrade/_index.md:25:> which can lead to a deadlock in a big cluster with hundreds of volumes.
content/docs/archives/1.1.3/deploy/upgrade/_index.md:26:> See more details [here](https://github.com/longhorn/longhorn/issues/2697).
content/docs/archives/1.1.3/deploy/upgrade/_index.md:27:> Longhorn v1.1.2 is shipped with a new instance manager image `v1_20210621` which fixed the deadlock
content/docs/archives/1.1.3/deploy/upgrade/_index.md:28:> but engine/replica processes of volumes don't migrate from the old instance manager to the new instance manager
content/docs/archives/1.1.3/deploy/upgrade/_index.md:29:> until the next time the volumes are detached/attached. Longhorn does it because we don't want to interrupt the
content/docs/archives/1.1.3/deploy/upgrade/_index.md:30:> data plane of the volumes.
content/docs/archives/1.1.3/deploy/upgrade/_index.md:31:>
content/docs/archives/1.1.3/deploy/upgrade/_index.md:32:> If you hit the deadlock in the old instance manager, please follow the recovering steps [here](https://github.com/longhorn/longhorn/issues/2697#issuecomment-879374809)
content/docs/archives/1.1.3/references/settings.md:61:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:69:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:75:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:86:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:92:> Default: `0`
content/docs/archives/1.1.3/references/settings.md:100:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:114:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.1.3/references/settings.md:120:> Default: `disabled`
content/docs/archives/1.1.3/references/settings.md:135:> Default: `/var/lib/longhorn/`
content/docs/archives/1.1.3/references/settings.md:153:> Default: `longhorn-static`
content/docs/archives/1.1.3/references/settings.md:159:> Default: `3`
content/docs/archives/1.1.3/references/settings.md:171:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:177:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:189:> Default: `do-nothing`
content/docs/archives/1.1.3/references/settings.md:204:> Default: `600`
content/docs/archives/1.1.3/references/settings.md:212:> Default: `if-not-present`
content/docs/archives/1.1.3/references/settings.md:227:> Default: `60`
content/docs/archives/1.1.3/references/settings.md:235:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:243:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.1.3/references/settings.md:249:> Example: `s3-secret`
content/docs/archives/1.1.3/references/settings.md:255:> Default: `300`
content/docs/archives/1.1.3/references/settings.md:265:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:269:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.1.3/references/settings.md:273:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:281:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:289:> Default: `true`
content/docs/archives/1.1.3/references/settings.md:295:> **Note:**
content/docs/archives/1.1.3/references/settings.md:296:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.1.3/references/settings.md:297:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.1.3/references/settings.md:301:> Default: `25`
content/docs/archives/1.1.3/references/settings.md:309:> Default: `200`
content/docs/archives/1.1.3/references/settings.md:321:> Default: `false`
content/docs/archives/1.1.3/references/settings.md:327:> Default: `12`
content/docs/archives/1.1.3/references/settings.md:339:> **Warning:**
content/docs/archives/1.1.3/references/settings.md:340:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.1.3/references/settings.md:341:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.1.3/references/settings.md:342:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.3/references/settings.md:343:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.1.3/references/settings.md:344:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.3/references/settings.md:348:> Default: `12`
content/docs/archives/1.1.3/references/settings.md:360:> **Warning:**
content/docs/archives/1.1.3/references/settings.md:361:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.1.3/references/settings.md:362:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.1.3/references/settings.md:363:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.1.3/references/settings.md:364:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.1.3/references/settings.md:365:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.1.3/references/settings.md:370:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.1.3/references/settings.md:389:> Example: `high-priority`
content/docs/archives/1.1.3/references/settings.md:398:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.1.3/references/settings.md:404:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.1.3/references/settings.md:411:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.1.3/references/settings.md:419:> Default:
content/docs/archives/1.1.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> CSI snapshot support is available for Kubernetes versions >= **1.17**.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:12:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:13:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:36:> **Prerequisite**
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:37:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:38:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:39:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:40:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:41:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:42:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.1.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.1.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:22:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't new data for a long time, Longhorn does the following:
content/docs/archives/1.1.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:23:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.1.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:24:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.1.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.1.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.1.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.1.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.1.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.1.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.1.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.1.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.1.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.1.3/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.0/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.0/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.0/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.0/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.0/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.0/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.2.0/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.2.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.0/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.0/references/settings.md:62:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:70:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:76:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:87:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:93:> Default: `0`
content/docs/archives/1.2.0/references/settings.md:101:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:115:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.0/references/settings.md:121:> Default: `disabled`
content/docs/archives/1.2.0/references/settings.md:136:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.0/references/settings.md:154:> Default: `longhorn-static`
content/docs/archives/1.2.0/references/settings.md:160:> Default: `3`
content/docs/archives/1.2.0/references/settings.md:172:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:178:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:190:> Default: `do-nothing`
content/docs/archives/1.2.0/references/settings.md:205:> Default: `600`
content/docs/archives/1.2.0/references/settings.md:213:> Default: `if-not-present`
content/docs/archives/1.2.0/references/settings.md:228:> Default: `60`
content/docs/archives/1.2.0/references/settings.md:236:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:244:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.0/references/settings.md:250:> Example: `s3-secret`
content/docs/archives/1.2.0/references/settings.md:256:> Default: `300`
content/docs/archives/1.2.0/references/settings.md:266:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:270:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.0/references/settings.md:274:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:282:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:290:> Default: `true`
content/docs/archives/1.2.0/references/settings.md:296:> **Note:**
content/docs/archives/1.2.0/references/settings.md:297:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.0/references/settings.md:298:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.0/references/settings.md:302:> Default: `disabled`
content/docs/archives/1.2.0/references/settings.md:318:> Default: `ignored`
content/docs/archives/1.2.0/references/settings.md:332:> Default: `25`
content/docs/archives/1.2.0/references/settings.md:340:> Default: `200`
content/docs/archives/1.2.0/references/settings.md:352:> Default: `false`
content/docs/archives/1.2.0/references/settings.md:358:> Default: `12`
content/docs/archives/1.2.0/references/settings.md:370:> **Warning:**
content/docs/archives/1.2.0/references/settings.md:371:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.0/references/settings.md:372:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.0/references/settings.md:373:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.0/references/settings.md:374:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.0/references/settings.md:375:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.0/references/settings.md:379:> Default: `12`
content/docs/archives/1.2.0/references/settings.md:391:> **Warning:**
content/docs/archives/1.2.0/references/settings.md:392:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.0/references/settings.md:393:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.0/references/settings.md:394:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.0/references/settings.md:395:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.0/references/settings.md:396:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.0/references/settings.md:401:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.0/references/settings.md:420:> Example: `high-priority`
content/docs/archives/1.2.0/references/settings.md:429:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.0/references/settings.md:435:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.0/references/settings.md:442:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.0/references/settings.md:450:> Default:
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.0/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.0/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.1/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.1/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.1/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.2.1/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.2.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.1/references/settings.md:64:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:72:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:78:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:89:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:95:> Default: `0`
content/docs/archives/1.2.1/references/settings.md:103:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:117:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.1/references/settings.md:123:> Default: `disabled`
content/docs/archives/1.2.1/references/settings.md:138:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.1/references/settings.md:156:> Default: `longhorn-static`
content/docs/archives/1.2.1/references/settings.md:162:> Default: `3`
content/docs/archives/1.2.1/references/settings.md:174:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:180:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:192:> Default: `do-nothing`
content/docs/archives/1.2.1/references/settings.md:207:> Default: `600`
content/docs/archives/1.2.1/references/settings.md:215:> Default: `if-not-present`
content/docs/archives/1.2.1/references/settings.md:230:> Default: `60`
content/docs/archives/1.2.1/references/settings.md:235:> Default: `300`
content/docs/archives/1.2.1/references/settings.md:238:> **Note:**
content/docs/archives/1.2.1/references/settings.md:239:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.1/references/settings.md:240:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.1/references/settings.md:246:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:254:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.1/references/settings.md:260:> Example: `s3-secret`
content/docs/archives/1.2.1/references/settings.md:266:> Default: `300`
content/docs/archives/1.2.1/references/settings.md:276:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:280:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.1/references/settings.md:284:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:292:> Default: `false`
content/docs/archives/1.2.1/references/settings.md:300:> Default: `true`
content/docs/archives/1.2.1/references/settings.md:306:> **Note:**
content/docs/archives/1.2.1/references/settings.md:307:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.1/references/settings.md:308:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.1/references/settings.md:312:> Default: `disabled`
content/docs/archives/1.2.1/references/settings.md:328:> Default: `ignored`
content/docs/archives/1.2.1/references/settings.md:342:> Default: `25`
content/docs/archives/1.2.1/references/settings.md:350:> Default: `200`
content/docs/archives/1.2.1/references/settings.md:362:> Default: `5`
content/docs/archives/1.2.1/references/settings.md:368:> **WARNING:**
content/docs/archives/1.2.1/references/settings.md:369:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.1/references/settings.md:370:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.1/references/settings.md:371:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.1/references/settings.md:375:> Default: `12`
content/docs/archives/1.2.1/references/settings.md:387:> **Warning:**
content/docs/archives/1.2.1/references/settings.md:388:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.1/references/settings.md:389:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.1/references/settings.md:390:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.1/references/settings.md:391:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.1/references/settings.md:392:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.1/references/settings.md:396:> Default: `12`
content/docs/archives/1.2.1/references/settings.md:408:> **Warning:**
content/docs/archives/1.2.1/references/settings.md:409:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.1/references/settings.md:410:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.1/references/settings.md:411:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.1/references/settings.md:412:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.1/references/settings.md:413:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.1/references/settings.md:418:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.1/references/settings.md:437:> Example: `high-priority`
content/docs/archives/1.2.1/references/settings.md:446:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.1/references/settings.md:452:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.1/references/settings.md:459:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.1/references/settings.md:467:> Default:
content/docs/archives/1.2.1/references/settings.md:473:> Default: `false`
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.1/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.1/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.2/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.2/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.2/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.2/best-practices.md:101:> Recommend: `false`
content/docs/archives/1.2.2/best-practices.md:106:> Recommend: `false`
content/docs/archives/1.2.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.2/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.2/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.2/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.2/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.2/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.2/references/settings.md:64:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:72:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:78:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:89:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:95:> Default: `0`
content/docs/archives/1.2.2/references/settings.md:103:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:117:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.2/references/settings.md:123:> Default: `disabled`
content/docs/archives/1.2.2/references/settings.md:138:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.2/references/settings.md:156:> Default: `longhorn-static`
content/docs/archives/1.2.2/references/settings.md:162:> Default: `3`
content/docs/archives/1.2.2/references/settings.md:174:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:180:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:192:> Default: `do-nothing`
content/docs/archives/1.2.2/references/settings.md:207:> Default: `600`
content/docs/archives/1.2.2/references/settings.md:215:> Default: `if-not-present`
content/docs/archives/1.2.2/references/settings.md:231:> Default: `60`
content/docs/archives/1.2.2/references/settings.md:236:> Default: `300`
content/docs/archives/1.2.2/references/settings.md:239:> **Note:**
content/docs/archives/1.2.2/references/settings.md:240:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.2/references/settings.md:241:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.2/references/settings.md:247:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:255:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.2/references/settings.md:261:> Example: `s3-secret`
content/docs/archives/1.2.2/references/settings.md:267:> Default: `300`
content/docs/archives/1.2.2/references/settings.md:277:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:281:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.2/references/settings.md:285:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:293:> Default: `false`
content/docs/archives/1.2.2/references/settings.md:301:> Default: `true`
content/docs/archives/1.2.2/references/settings.md:307:> **Note:**
content/docs/archives/1.2.2/references/settings.md:308:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.2/references/settings.md:309:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.2/references/settings.md:313:> Default: `disabled`
content/docs/archives/1.2.2/references/settings.md:329:> Default: `ignored`
content/docs/archives/1.2.2/references/settings.md:343:> Default: `25`
content/docs/archives/1.2.2/references/settings.md:351:> Default: `200`
content/docs/archives/1.2.2/references/settings.md:363:> Default: `5`
content/docs/archives/1.2.2/references/settings.md:369:> **WARNING:**
content/docs/archives/1.2.2/references/settings.md:370:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.2/references/settings.md:371:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.2/references/settings.md:372:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.2/references/settings.md:376:> Default: `12`
content/docs/archives/1.2.2/references/settings.md:388:> **Warning:**
content/docs/archives/1.2.2/references/settings.md:389:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.2/references/settings.md:390:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.2/references/settings.md:391:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.2/references/settings.md:392:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.2/references/settings.md:393:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.2/references/settings.md:397:> Default: `12`
content/docs/archives/1.2.2/references/settings.md:409:> **Warning:**
content/docs/archives/1.2.2/references/settings.md:410:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.2/references/settings.md:411:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.2/references/settings.md:412:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.2/references/settings.md:413:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.2/references/settings.md:414:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.2/references/settings.md:419:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.2/references/settings.md:438:> Example: `high-priority`
content/docs/archives/1.2.2/references/settings.md:447:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.2/references/settings.md:453:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.2/references/settings.md:460:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.2/references/settings.md:468:> Default:
content/docs/archives/1.2.2/references/settings.md:474:> Default: `false`
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.2/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.2/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.2/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.2/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.2/volumes-and-nodes/volume-size.md:27:> The illustration presents the the file organazation of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.3/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.2.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.3/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.3/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.3/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.3/best-practices.md:107:> Recommend: `false`
content/docs/archives/1.2.3/best-practices.md:112:> Recommend: `false`
content/docs/archives/1.2.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.3/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.3/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.3/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.3/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.3/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.3/references/settings.md:64:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:72:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:78:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:89:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:95:> Default: `0`
content/docs/archives/1.2.3/references/settings.md:103:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:117:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.3/references/settings.md:123:> Default: `disabled`
content/docs/archives/1.2.3/references/settings.md:138:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.3/references/settings.md:156:> Default: `longhorn-static`
content/docs/archives/1.2.3/references/settings.md:162:> Default: `3`
content/docs/archives/1.2.3/references/settings.md:174:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:180:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:192:> Default: `do-nothing`
content/docs/archives/1.2.3/references/settings.md:207:> Default: `600`
content/docs/archives/1.2.3/references/settings.md:215:> Default: `if-not-present`
content/docs/archives/1.2.3/references/settings.md:231:> Default: `60`
content/docs/archives/1.2.3/references/settings.md:236:> Default: `300`
content/docs/archives/1.2.3/references/settings.md:239:> **Note:**
content/docs/archives/1.2.3/references/settings.md:240:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.3/references/settings.md:241:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.3/references/settings.md:247:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:255:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.3/references/settings.md:261:> Example: `s3-secret`
content/docs/archives/1.2.3/references/settings.md:267:> Default: `300`
content/docs/archives/1.2.3/references/settings.md:277:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:281:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.3/references/settings.md:285:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:293:> Default: `false`
content/docs/archives/1.2.3/references/settings.md:301:> Default: `true`
content/docs/archives/1.2.3/references/settings.md:307:> **Note:**
content/docs/archives/1.2.3/references/settings.md:308:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.3/references/settings.md:309:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.3/references/settings.md:313:> Default: `disabled`
content/docs/archives/1.2.3/references/settings.md:329:> Default: `ignored`
content/docs/archives/1.2.3/references/settings.md:343:> Default: `25`
content/docs/archives/1.2.3/references/settings.md:351:> Default: `200`
content/docs/archives/1.2.3/references/settings.md:363:> Default: `5`
content/docs/archives/1.2.3/references/settings.md:369:> **WARNING:**
content/docs/archives/1.2.3/references/settings.md:370:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.3/references/settings.md:371:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.3/references/settings.md:372:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.3/references/settings.md:376:> Default: `12`
content/docs/archives/1.2.3/references/settings.md:388:> **Warning:**
content/docs/archives/1.2.3/references/settings.md:389:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.3/references/settings.md:390:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.3/references/settings.md:391:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.3/references/settings.md:392:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.3/references/settings.md:393:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.3/references/settings.md:397:> Default: `12`
content/docs/archives/1.2.3/references/settings.md:409:> **Warning:**
content/docs/archives/1.2.3/references/settings.md:410:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.3/references/settings.md:411:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.3/references/settings.md:412:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.3/references/settings.md:413:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.3/references/settings.md:414:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.3/references/settings.md:419:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.3/references/settings.md:438:> Example: `high-priority`
content/docs/archives/1.2.3/references/settings.md:447:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.3/references/settings.md:453:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.3/references/settings.md:460:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.3/references/settings.md:468:> Default:
content/docs/archives/1.2.3/references/settings.md:474:> Default: `false`
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.3/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.3/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.3/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.3/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.3/volumes-and-nodes/volume-size.md:27:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.4/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.2.4/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.4/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.4/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.4/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.4/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.4/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.4/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.4/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.4/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.4/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.4/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.4/best-practices.md:109:> Recommend: `false`
content/docs/archives/1.2.4/best-practices.md:114:> Recommend: `false`
content/docs/archives/1.2.4/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.4/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.4/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.4/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.4/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.4/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.4/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.4/references/settings.md:64:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:72:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:78:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:89:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:95:> Default: `0`
content/docs/archives/1.2.4/references/settings.md:103:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:117:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.4/references/settings.md:123:> Default: `disabled`
content/docs/archives/1.2.4/references/settings.md:138:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.4/references/settings.md:156:> Default: `longhorn-static`
content/docs/archives/1.2.4/references/settings.md:162:> Default: `3`
content/docs/archives/1.2.4/references/settings.md:174:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:180:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:192:> Default: `do-nothing`
content/docs/archives/1.2.4/references/settings.md:207:> Default: `600`
content/docs/archives/1.2.4/references/settings.md:215:> Default: `if-not-present`
content/docs/archives/1.2.4/references/settings.md:231:> Default: `60`
content/docs/archives/1.2.4/references/settings.md:236:> Default: `300`
content/docs/archives/1.2.4/references/settings.md:239:> **Note:**
content/docs/archives/1.2.4/references/settings.md:240:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.4/references/settings.md:241:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.4/references/settings.md:247:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:255:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.4/references/settings.md:261:> Example: `s3-secret`
content/docs/archives/1.2.4/references/settings.md:267:> Default: `300`
content/docs/archives/1.2.4/references/settings.md:277:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:281:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.4/references/settings.md:285:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:293:> Default: `false`
content/docs/archives/1.2.4/references/settings.md:301:> Default: `true`
content/docs/archives/1.2.4/references/settings.md:307:> **Note:**
content/docs/archives/1.2.4/references/settings.md:308:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.4/references/settings.md:309:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.4/references/settings.md:313:> Default: `disabled`
content/docs/archives/1.2.4/references/settings.md:329:> Default: `ignored`
content/docs/archives/1.2.4/references/settings.md:343:> Default: `25`
content/docs/archives/1.2.4/references/settings.md:351:> Default: `200`
content/docs/archives/1.2.4/references/settings.md:363:> Default: `5`
content/docs/archives/1.2.4/references/settings.md:369:> **WARNING:**
content/docs/archives/1.2.4/references/settings.md:370:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.4/references/settings.md:371:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.4/references/settings.md:372:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.4/references/settings.md:376:> Default: `12`
content/docs/archives/1.2.4/references/settings.md:388:> **Warning:**
content/docs/archives/1.2.4/references/settings.md:389:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.4/references/settings.md:390:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.4/references/settings.md:391:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.4/references/settings.md:392:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.4/references/settings.md:393:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.4/references/settings.md:397:> Default: `12`
content/docs/archives/1.2.4/references/settings.md:409:> **Warning:**
content/docs/archives/1.2.4/references/settings.md:410:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.4/references/settings.md:411:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.4/references/settings.md:412:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.4/references/settings.md:413:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.4/references/settings.md:414:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.4/references/settings.md:419:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.4/references/settings.md:438:> Example: `high-priority`
content/docs/archives/1.2.4/references/settings.md:447:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.4/references/settings.md:453:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.4/references/settings.md:460:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.4/references/settings.md:468:> Default:
content/docs/archives/1.2.4/references/settings.md:474:> Default: `false`
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.4/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.4/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.4/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.4/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.4/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.4/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.4/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.4/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.4/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.4/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.4/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.4/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.4/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.5/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.2.5/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.5/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.5/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.5/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.5/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.5/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.5/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.5/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.5/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.5/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.5/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.5/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.5/best-practices.md:107:> Recommend: `false`
content/docs/archives/1.2.5/best-practices.md:112:> Recommend: `false`
content/docs/archives/1.2.5/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.5/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.5/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.5/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.5/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.5/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.5/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.5/references/settings.md:65:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:73:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:79:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:90:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:96:> Default: `0`
content/docs/archives/1.2.5/references/settings.md:104:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:118:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.5/references/settings.md:124:> Default: `disabled`
content/docs/archives/1.2.5/references/settings.md:139:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.5/references/settings.md:157:> Default: `longhorn-static`
content/docs/archives/1.2.5/references/settings.md:163:> Default: `3`
content/docs/archives/1.2.5/references/settings.md:175:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:181:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:193:> Default: `do-nothing`
content/docs/archives/1.2.5/references/settings.md:208:> Default: `600`
content/docs/archives/1.2.5/references/settings.md:216:> Default: `if-not-present`
content/docs/archives/1.2.5/references/settings.md:232:> Default: `60`
content/docs/archives/1.2.5/references/settings.md:237:> Default: `300`
content/docs/archives/1.2.5/references/settings.md:240:> **Note:**
content/docs/archives/1.2.5/references/settings.md:241:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.5/references/settings.md:242:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.5/references/settings.md:248:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:256:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.5/references/settings.md:262:> Example: `s3-secret`
content/docs/archives/1.2.5/references/settings.md:268:> Default: `300`
content/docs/archives/1.2.5/references/settings.md:276:> Default: `1440`
content/docs/archives/1.2.5/references/settings.md:286:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:290:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.5/references/settings.md:294:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:302:> Default: `false`
content/docs/archives/1.2.5/references/settings.md:310:> Default: `true`
content/docs/archives/1.2.5/references/settings.md:316:> **Note:**
content/docs/archives/1.2.5/references/settings.md:317:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.5/references/settings.md:318:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.5/references/settings.md:322:> Default: `disabled`
content/docs/archives/1.2.5/references/settings.md:338:> Default: `ignored`
content/docs/archives/1.2.5/references/settings.md:352:> Default: `25`
content/docs/archives/1.2.5/references/settings.md:360:> Default: `200`
content/docs/archives/1.2.5/references/settings.md:372:> Default: `5`
content/docs/archives/1.2.5/references/settings.md:378:> **WARNING:**
content/docs/archives/1.2.5/references/settings.md:379:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.5/references/settings.md:380:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.5/references/settings.md:381:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.5/references/settings.md:385:> Default: `12`
content/docs/archives/1.2.5/references/settings.md:397:> **Warning:**
content/docs/archives/1.2.5/references/settings.md:398:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.5/references/settings.md:399:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.5/references/settings.md:400:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.5/references/settings.md:401:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.5/references/settings.md:402:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.5/references/settings.md:406:> Default: `12`
content/docs/archives/1.2.5/references/settings.md:418:> **Warning:**
content/docs/archives/1.2.5/references/settings.md:419:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.5/references/settings.md:420:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.5/references/settings.md:421:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.5/references/settings.md:422:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.5/references/settings.md:423:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.5/references/settings.md:428:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.5/references/settings.md:447:> Example: `high-priority`
content/docs/archives/1.2.5/references/settings.md:456:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.5/references/settings.md:462:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.5/references/settings.md:469:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.5/references/settings.md:477:> Default:
content/docs/archives/1.2.5/references/settings.md:483:> Default: `false`
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.5/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.5/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.5/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.5/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.5/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.5/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.5/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.5/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.5/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.5/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.5/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.5/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.5/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.5/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.5/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.5/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.2.6/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.2.6/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.2.6/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.2.6/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.6/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.6/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.2.6/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.2.6/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.2.6/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.2.6/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.2.6/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.2.6/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.2.6/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.2.6/best-practices.md:107:> Recommend: `false`
content/docs/archives/1.2.6/best-practices.md:112:> Recommend: `false`
content/docs/archives/1.2.6/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.2.6/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.2.6/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.2.6/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.2.6/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.2.6/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.2.6/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.2.6/references/settings.md:65:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:73:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:79:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:90:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:96:> Default: `0`
content/docs/archives/1.2.6/references/settings.md:104:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:118:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.2.6/references/settings.md:124:> Default: `disabled`
content/docs/archives/1.2.6/references/settings.md:139:> Default: `/var/lib/longhorn/`
content/docs/archives/1.2.6/references/settings.md:157:> Default: `longhorn-static`
content/docs/archives/1.2.6/references/settings.md:163:> Default: `3`
content/docs/archives/1.2.6/references/settings.md:175:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:181:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:193:> Default: `do-nothing`
content/docs/archives/1.2.6/references/settings.md:208:> Default: `600`
content/docs/archives/1.2.6/references/settings.md:216:> Default: `if-not-present`
content/docs/archives/1.2.6/references/settings.md:232:> Default: `60`
content/docs/archives/1.2.6/references/settings.md:237:> Default: `300`
content/docs/archives/1.2.6/references/settings.md:240:> **Note:**
content/docs/archives/1.2.6/references/settings.md:241:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.2.6/references/settings.md:242:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.2.6/references/settings.md:248:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:256:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.2.6/references/settings.md:262:> Example: `s3-secret`
content/docs/archives/1.2.6/references/settings.md:268:> Default: `300`
content/docs/archives/1.2.6/references/settings.md:276:> Default: `1440`
content/docs/archives/1.2.6/references/settings.md:286:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:290:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.2.6/references/settings.md:294:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:302:> Default: `false`
content/docs/archives/1.2.6/references/settings.md:310:> Default: `true`
content/docs/archives/1.2.6/references/settings.md:316:> **Note:**
content/docs/archives/1.2.6/references/settings.md:317:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.2.6/references/settings.md:318:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.2.6/references/settings.md:322:> Default: `disabled`
content/docs/archives/1.2.6/references/settings.md:338:> Default: `ignored`
content/docs/archives/1.2.6/references/settings.md:352:> Default: `25`
content/docs/archives/1.2.6/references/settings.md:360:> Default: `200`
content/docs/archives/1.2.6/references/settings.md:372:> Default: `5`
content/docs/archives/1.2.6/references/settings.md:378:> **WARNING:**
content/docs/archives/1.2.6/references/settings.md:379:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.2.6/references/settings.md:380:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.2.6/references/settings.md:381:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.2.6/references/settings.md:385:> Default: `12`
content/docs/archives/1.2.6/references/settings.md:397:> **Warning:**
content/docs/archives/1.2.6/references/settings.md:398:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.2.6/references/settings.md:399:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.2.6/references/settings.md:400:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.6/references/settings.md:401:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.2.6/references/settings.md:402:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.6/references/settings.md:406:> Default: `12`
content/docs/archives/1.2.6/references/settings.md:418:> **Warning:**
content/docs/archives/1.2.6/references/settings.md:419:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.2.6/references/settings.md:420:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.2.6/references/settings.md:421:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.2.6/references/settings.md:422:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.2.6/references/settings.md:423:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.2.6/references/settings.md:428:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.2.6/references/settings.md:447:> Example: `high-priority`
content/docs/archives/1.2.6/references/settings.md:456:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.2.6/references/settings.md:462:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.2.6/references/settings.md:469:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.2.6/references/settings.md:477:> Default:
content/docs/archives/1.2.6/references/settings.md:483:> Default: `false`
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.2.6/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/create-a-backup-via-csi.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:13:> **Prerequisite**
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:14:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:15:> CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:16:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:17:> If your Kubernetes distribution does not provide the Kubernetes snapshot controller as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:18:>
content/docs/archives/1.2.6/snapshots-and-backups/csi-snapshot-support/restore-a-backup-via-csi.md:19:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.2.6/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.2.6/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.2.6/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.2.6/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.2.6/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.2.6/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.2.6/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.2.6/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.2.6/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.2.6/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.2.6/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.2.6/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.2.6/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.2.6/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.0/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.0/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.0/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.0/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.0/advanced-resources/deploy/customizing-default-settings.md:26:> **NOTE:** When using Longhorn Deployment YAML file or Helm for installation, updating or upgrading, if the value of a default setting is an empty string and valid, the default setting will be cleaned up in Longhorn. If not, Longhorn will ignore the invalid values and will not update the default values.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.0/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.0/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.0/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.0/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.0/advanced-resources/deploy/rancher_windows_cluster.md:23:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.0/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.0/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.0/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.0/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.0/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.0/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.0/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.0/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.0/best-practices.md:121:> Recommend: `false`
content/docs/archives/1.3.0/best-practices.md:127:> Recommend: `false`
content/docs/archives/1.3.0/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.0/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.0/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.0/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.0/deploy/install/install-with-helm.md:15:> [This script](https://github.com/longhorn/longhorn/blob/v{{< current-version >}}/scripts/environment_check.sh) can be used to check the Longhorn environment for potential issues.
content/docs/archives/1.3.0/deploy/install/install-with-helm.md:20:> **Note**: The initial settings for Longhorn can be found in [customized using Helm options or by editing the deployment configuration file.](../../../advanced-resources/deploy/customizing-default-settings/#using-helm)
content/docs/archives/1.3.0/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.0/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.0/deploy/upgrade/longhorn-manager.md:36:> Note: Because the Cluster Manager (old UI) is being deprecated, we provided the instruction to migrate the existing Longhorn installation to the Longhorn chart in the Cluster Explorer (new UI) [here](https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/)
content/docs/archives/1.3.0/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.0/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.0/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.0/references/settings.md:68:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:76:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:82:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:93:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:99:> Default: `0`
content/docs/archives/1.3.0/references/settings.md:107:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:121:> Default: `longhorn.io/v1beta2`
content/docs/archives/1.3.0/references/settings.md:127:> Default: `disabled`
content/docs/archives/1.3.0/references/settings.md:142:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.0/references/settings.md:160:> Default: `longhorn-static`
content/docs/archives/1.3.0/references/settings.md:166:> Default: `3`
content/docs/archives/1.3.0/references/settings.md:178:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:184:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:196:> Default: `do-nothing`
content/docs/archives/1.3.0/references/settings.md:211:> Default: `600`
content/docs/archives/1.3.0/references/settings.md:219:> Default: `if-not-present`
content/docs/archives/1.3.0/references/settings.md:235:> Default: `60`
content/docs/archives/1.3.0/references/settings.md:240:> Default: `300`
content/docs/archives/1.3.0/references/settings.md:243:> **Note:**
content/docs/archives/1.3.0/references/settings.md:244:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.0/references/settings.md:245:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.0/references/settings.md:250:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:258:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:266:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.0/references/settings.md:272:> Example: `s3-secret`
content/docs/archives/1.3.0/references/settings.md:278:> Default: `300`
content/docs/archives/1.3.0/references/settings.md:288:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:292:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.0/references/settings.md:296:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:304:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:312:> Default: `true`
content/docs/archives/1.3.0/references/settings.md:318:> **Note:**
content/docs/archives/1.3.0/references/settings.md:319:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.0/references/settings.md:320:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.0/references/settings.md:324:> Default: `disabled`
content/docs/archives/1.3.0/references/settings.md:340:> Default: `ignored`
content/docs/archives/1.3.0/references/settings.md:354:> Default: `25`
content/docs/archives/1.3.0/references/settings.md:362:> Default: `200`
content/docs/archives/1.3.0/references/settings.md:374:> Default: `5`
content/docs/archives/1.3.0/references/settings.md:380:> **WARNING:**
content/docs/archives/1.3.0/references/settings.md:381:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.0/references/settings.md:382:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.0/references/settings.md:383:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.0/references/settings.md:387:> Default: `12`
content/docs/archives/1.3.0/references/settings.md:399:> **Warning:**
content/docs/archives/1.3.0/references/settings.md:400:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.0/references/settings.md:401:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.0/references/settings.md:402:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.0/references/settings.md:403:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.0/references/settings.md:404:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.0/references/settings.md:408:> Default: `12`
content/docs/archives/1.3.0/references/settings.md:420:> **Warning:**
content/docs/archives/1.3.0/references/settings.md:421:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.0/references/settings.md:422:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.0/references/settings.md:423:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.0/references/settings.md:424:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.0/references/settings.md:425:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.0/references/settings.md:430:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.0/references/settings.md:449:> Example: `high-priority`
content/docs/archives/1.3.0/references/settings.md:458:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.0/references/settings.md:464:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.0/references/settings.md:471:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.0/references/settings.md:477:> Default: `false`
content/docs/archives/1.3.0/references/settings.md:483:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.0/references/settings.md:487:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.0/references/settings.md:491:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.0/references/settings.md:500:> Default:
content/docs/archives/1.3.0/references/settings.md:506:> Default: `false`
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.0/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.0/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.0/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.0/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.0/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.0/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.0/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.0/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.0/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.0/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.0/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.0/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.0/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.0/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.0/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.0/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.1/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.1/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.1/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.1/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.1/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.1/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.1/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.1/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.1/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.1/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.1/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.1/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.1/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.1/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.1/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.1/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.1/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.1/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.1/best-practices.md:118:> Recommend: `false`
content/docs/archives/1.3.1/best-practices.md:123:> Recommend: `false`
content/docs/archives/1.3.1/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.1/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.1/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.1/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.1/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.1/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.1/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.1/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.1/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.1/references/settings.md:68:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:76:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:82:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:93:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:99:> Default: `0`
content/docs/archives/1.3.1/references/settings.md:107:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:121:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.3.1/references/settings.md:127:> Default: `disabled`
content/docs/archives/1.3.1/references/settings.md:142:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.1/references/settings.md:160:> Default: `longhorn-static`
content/docs/archives/1.3.1/references/settings.md:166:> Default: `3`
content/docs/archives/1.3.1/references/settings.md:178:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:184:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:196:> Default: `do-nothing`
content/docs/archives/1.3.1/references/settings.md:211:> Default: `600`
content/docs/archives/1.3.1/references/settings.md:219:> Default: `if-not-present`
content/docs/archives/1.3.1/references/settings.md:235:> Default: `60`
content/docs/archives/1.3.1/references/settings.md:240:> Default: `300`
content/docs/archives/1.3.1/references/settings.md:243:> **Note:**
content/docs/archives/1.3.1/references/settings.md:244:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.1/references/settings.md:245:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.1/references/settings.md:250:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:258:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:266:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.1/references/settings.md:272:> Example: `s3-secret`
content/docs/archives/1.3.1/references/settings.md:278:> Default: `300`
content/docs/archives/1.3.1/references/settings.md:288:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:292:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.1/references/settings.md:296:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:304:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:312:> Default: `true`
content/docs/archives/1.3.1/references/settings.md:318:> **Note:**
content/docs/archives/1.3.1/references/settings.md:319:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.1/references/settings.md:320:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.1/references/settings.md:324:> Default: `disabled`
content/docs/archives/1.3.1/references/settings.md:340:> Default: `ignored`
content/docs/archives/1.3.1/references/settings.md:354:> Default: `25`
content/docs/archives/1.3.1/references/settings.md:362:> Default: `200`
content/docs/archives/1.3.1/references/settings.md:374:> Default: `5`
content/docs/archives/1.3.1/references/settings.md:380:> **WARNING:**
content/docs/archives/1.3.1/references/settings.md:381:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.1/references/settings.md:382:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.1/references/settings.md:383:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.1/references/settings.md:387:> Default: `12`
content/docs/archives/1.3.1/references/settings.md:399:> **Warning:**
content/docs/archives/1.3.1/references/settings.md:400:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.1/references/settings.md:401:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.1/references/settings.md:402:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.1/references/settings.md:403:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.1/references/settings.md:404:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.1/references/settings.md:408:> Default: `12`
content/docs/archives/1.3.1/references/settings.md:420:> **Warning:**
content/docs/archives/1.3.1/references/settings.md:421:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.1/references/settings.md:422:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.1/references/settings.md:423:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.1/references/settings.md:424:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.1/references/settings.md:425:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.1/references/settings.md:430:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.1/references/settings.md:449:> Example: `high-priority`
content/docs/archives/1.3.1/references/settings.md:458:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.1/references/settings.md:464:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.1/references/settings.md:471:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.1/references/settings.md:477:> Default: `false`
content/docs/archives/1.3.1/references/settings.md:483:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.1/references/settings.md:487:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.1/references/settings.md:491:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.1/references/settings.md:500:> Default:
content/docs/archives/1.3.1/references/settings.md:506:> Default: `false`
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.1/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.1/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.1/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.1/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.1/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.1/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.1/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.1/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.1/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.1/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.1/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.1/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.1/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.1/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.1/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.1/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.2/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.2/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.2/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.2/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.2/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.2/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.2/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.2/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.2/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.2/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.2/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.2/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.2/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.2/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.2/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.2/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.2/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.2/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.2/best-practices.md:118:> Recommend: `false`
content/docs/archives/1.3.2/best-practices.md:123:> Recommend: `false`
content/docs/archives/1.3.2/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.2/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.2/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.2/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.2/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.2/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.2/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.2/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.2/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.2/references/settings.md:69:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:77:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:83:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:94:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:100:> Default: `0`
content/docs/archives/1.3.2/references/settings.md:108:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:122:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.3.2/references/settings.md:128:> Default: `disabled`
content/docs/archives/1.3.2/references/settings.md:143:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.2/references/settings.md:161:> Default: `longhorn-static`
content/docs/archives/1.3.2/references/settings.md:167:> Default: `3`
content/docs/archives/1.3.2/references/settings.md:179:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:185:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:197:> Default: `do-nothing`
content/docs/archives/1.3.2/references/settings.md:212:> Default: `600`
content/docs/archives/1.3.2/references/settings.md:220:> Default: `if-not-present`
content/docs/archives/1.3.2/references/settings.md:236:> Default: `60`
content/docs/archives/1.3.2/references/settings.md:241:> Default: `300`
content/docs/archives/1.3.2/references/settings.md:244:> **Note:**
content/docs/archives/1.3.2/references/settings.md:245:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.2/references/settings.md:246:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.2/references/settings.md:251:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:259:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:267:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.2/references/settings.md:273:> Example: `s3-secret`
content/docs/archives/1.3.2/references/settings.md:279:> Default: `300`
content/docs/archives/1.3.2/references/settings.md:287:> Default: `1440`
content/docs/archives/1.3.2/references/settings.md:297:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:301:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.2/references/settings.md:305:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:313:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:321:> Default: `true`
content/docs/archives/1.3.2/references/settings.md:327:> **Note:**
content/docs/archives/1.3.2/references/settings.md:328:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.2/references/settings.md:329:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.2/references/settings.md:333:> Default: `disabled`
content/docs/archives/1.3.2/references/settings.md:349:> Default: `ignored`
content/docs/archives/1.3.2/references/settings.md:363:> Default: `25`
content/docs/archives/1.3.2/references/settings.md:371:> Default: `200`
content/docs/archives/1.3.2/references/settings.md:383:> Default: `5`
content/docs/archives/1.3.2/references/settings.md:389:> **WARNING:**
content/docs/archives/1.3.2/references/settings.md:390:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.2/references/settings.md:391:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.2/references/settings.md:392:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.2/references/settings.md:396:> Default: `12`
content/docs/archives/1.3.2/references/settings.md:408:> **Warning:**
content/docs/archives/1.3.2/references/settings.md:409:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.2/references/settings.md:410:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.2/references/settings.md:411:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.2/references/settings.md:412:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.2/references/settings.md:413:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.2/references/settings.md:417:> Default: `12`
content/docs/archives/1.3.2/references/settings.md:429:> **Warning:**
content/docs/archives/1.3.2/references/settings.md:430:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.2/references/settings.md:431:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.2/references/settings.md:432:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.2/references/settings.md:433:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.2/references/settings.md:434:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.2/references/settings.md:439:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.2/references/settings.md:458:> Example: `high-priority`
content/docs/archives/1.3.2/references/settings.md:467:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.2/references/settings.md:473:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.2/references/settings.md:480:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.2/references/settings.md:486:> Default: `false`
content/docs/archives/1.3.2/references/settings.md:492:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.2/references/settings.md:496:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.2/references/settings.md:500:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.2/references/settings.md:509:> Default:
content/docs/archives/1.3.2/references/settings.md:515:> Default: `false`
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.2/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.2/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.2/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.2/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.2/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.2/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.2/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.2/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.2/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.2/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.2/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.2/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.2/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.2/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.2/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.2/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.3/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.3/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.3/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.3/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.3/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.3/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.3/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.3/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.3/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.3/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.3/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.3/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.3/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.3/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.3/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.3/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.3/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.3/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.3/best-practices.md:118:> Recommend: `false`
content/docs/archives/1.3.3/best-practices.md:123:> Recommend: `false`
content/docs/archives/1.3.3/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.3/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.3/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.3/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.3/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.3/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.3/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.3/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.3/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.3/references/settings.md:70:> Default: `block-if-contains-last-replica`
content/docs/archives/1.3.3/references/settings.md:81:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:87:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:98:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:104:> Default: `0`
content/docs/archives/1.3.3/references/settings.md:112:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:126:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.3.3/references/settings.md:132:> Default: `disabled`
content/docs/archives/1.3.3/references/settings.md:147:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.3/references/settings.md:165:> Default: `longhorn-static`
content/docs/archives/1.3.3/references/settings.md:171:> Default: `3`
content/docs/archives/1.3.3/references/settings.md:183:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:189:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:201:> Default: `do-nothing`
content/docs/archives/1.3.3/references/settings.md:216:> Default: `600`
content/docs/archives/1.3.3/references/settings.md:224:> Default: `if-not-present`
content/docs/archives/1.3.3/references/settings.md:240:> Default: `60`
content/docs/archives/1.3.3/references/settings.md:245:> Default: `300`
content/docs/archives/1.3.3/references/settings.md:248:> **Note:**
content/docs/archives/1.3.3/references/settings.md:249:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.3/references/settings.md:250:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.3/references/settings.md:255:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:263:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:271:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.3/references/settings.md:277:> Example: `s3-secret`
content/docs/archives/1.3.3/references/settings.md:283:> Default: `300`
content/docs/archives/1.3.3/references/settings.md:291:> Default: `1440`
content/docs/archives/1.3.3/references/settings.md:301:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:305:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.3/references/settings.md:309:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:317:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:325:> Default: `true`
content/docs/archives/1.3.3/references/settings.md:331:> **Note:**
content/docs/archives/1.3.3/references/settings.md:332:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.3/references/settings.md:333:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.3/references/settings.md:337:> Default: `disabled`
content/docs/archives/1.3.3/references/settings.md:353:> Default: `ignored`
content/docs/archives/1.3.3/references/settings.md:367:> Default: `25`
content/docs/archives/1.3.3/references/settings.md:375:> Default: `200`
content/docs/archives/1.3.3/references/settings.md:387:> Default: `5`
content/docs/archives/1.3.3/references/settings.md:393:> **WARNING:**
content/docs/archives/1.3.3/references/settings.md:394:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.3/references/settings.md:395:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.3/references/settings.md:396:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.3/references/settings.md:400:> Default: `12`
content/docs/archives/1.3.3/references/settings.md:412:> **Warning:**
content/docs/archives/1.3.3/references/settings.md:413:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.3/references/settings.md:414:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.3/references/settings.md:415:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.3/references/settings.md:416:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.3/references/settings.md:417:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.3/references/settings.md:421:> Default: `12`
content/docs/archives/1.3.3/references/settings.md:433:> **Warning:**
content/docs/archives/1.3.3/references/settings.md:434:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.3/references/settings.md:435:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.3/references/settings.md:436:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.3/references/settings.md:437:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.3/references/settings.md:438:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.3/references/settings.md:443:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.3/references/settings.md:462:> Example: `high-priority`
content/docs/archives/1.3.3/references/settings.md:471:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.3/references/settings.md:477:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.3/references/settings.md:484:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.3/references/settings.md:490:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:496:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.3/references/settings.md:500:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.3/references/settings.md:504:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.3/references/settings.md:513:> Default:
content/docs/archives/1.3.3/references/settings.md:519:> Default: `false`
content/docs/archives/1.3.3/references/settings.md:525:> Default: `false`
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.3/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.3/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.3/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.3/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.3/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.3/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.3/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.3/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.3/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.3/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.3/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.3/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.3/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.3/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.3/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/docs/archives/1.3.4/advanced-resources/cluster-restore/restore-to-a-new-cluster-using-velero.md:8:> **Note:** Need to use [Velero CSI plugin](https://github.com/vmware-tanzu/velero-plugin-for-csi) >= 0.4 to ensure restoring PersistentVolumeClaim successfully. Visit [here](/kb/troubleshooting-restore-pvc-stuck-using-velero-csi-plugin-version-lower-than-0.4) to get more information.
content/docs/archives/1.3.4/advanced-resources/data-recovery/export-from-replica.md:73:> To avoid accidental change of the volume content, it's recommended to use `mount -o ro` to mount the directory as `readonly`.
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:31:> **Result:** If the node tag list was originally empty, Longhorn updates the node with the tag list, and you will see the tags for that node updated according to the annotation. If the node already had tags, you will see no change to the tag list.
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:51:> **Prerequisite:** The Longhorn setting **Create Default Disk on Labeled Nodes** must be enabled.
content/docs/archives/1.3.4/advanced-resources/default-disk-and-node-config.md:91:> **Result:** The disks will be updated according to the annotation.
content/docs/archives/1.3.4/advanced-resources/deploy/airgap.md:175:> **Note:** Once you set registryUrl to your private registry, Longhorn tries to pull images from the registry exclusively. Make sure all Longhorn components' images are in the registry otherwise Longhorn will fail to pull images.
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:49:> **Warning**:
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:50:> * Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:51:> * Make sure all Longhorn volumes are `detached`. If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.4/advanced-resources/deploy/node-selector.md:52:> * Don't operate the Longhorn system while node selector settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.4/advanced-resources/deploy/priority-class.md:17:> **Warning:** Longhorn will not start if the Priority Class setting is invalid (such as the Priority Class not existing).
content/docs/archives/1.3.4/advanced-resources/deploy/priority-class.md:18:> You can see if this is the case by checking the status of the longhorn-manager DaemonSet with `kubectl -n longhorn-system describe daemonset.apps/longhorn-manager`.
content/docs/archives/1.3.4/advanced-resources/deploy/priority-class.md:19:> You will need to uninstall Longhorn and restart the installation if this is the case.
content/docs/archives/1.3.4/advanced-resources/deploy/rancher_windows_cluster.md:15:> **Note**: After Longhorn is deployed, you can launch workloads that use Longhorn volumes only on Linux nodes.
content/docs/archives/1.3.4/advanced-resources/deploy/revision_counter.md:16:> **Note:** 'Salvage' is Longhorn trying to recover a volume in a faulted state. A volume is in a faulted state when the Longhorn Engine loses the connection to all the replicas, and all replicas are marked as being in an error state.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:57:> **Warning**:
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:58:>
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:59:> Before modifying the toleration settings, users should make sure all Longhorn volumes are `detached`.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:60:>
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:61:> Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:62:> If there are running Longhorn volumes in the system, this means the Longhorn system cannot restart its components and the request will be rejected.
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:63:>
content/docs/archives/1.3.4/advanced-resources/deploy/taint-toleration.md:64:> Don't operate the Longhorn system while toleration settings are updated and Longhorn components are being restarted.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:15:> **Warning**: Do not change this setting with volumes attached.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:16:>
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:17:> Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:30:> **Warning:** Longhorn instance-manager will not start if the Storage Network setting is invalid.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:31:>
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:32:> You can check the events of the instance-manager Pod to see if it is related to an invalid NetworkAttachmentDefinition with `kubectl -n longhorn-system describe pods -l longhorn.io/component=instance-manager`.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:33:>
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:34:> If this is the case, provide a valid `NetworkAttachmentDefinition` and re-run Longhorn install.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:40:> **Warning:** Do not modify the NetworkAttachmentDefinition custom resource after applying it to the setting.
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:41:>
content/docs/archives/1.3.4/advanced-resources/deploy/storage-network.md:42:> Longhorn is not aware of the updates. Hence this will cause malfunctioning and error. Instead, you can create a new NetworkAttachmentDefinition custom resource and update it to the setting.
content/docs/archives/1.3.4/advanced-resources/migrating-flexvolume.md:8:> Note that the volumes created and used through one driver won't be recognized by Kubernetes using the other driver. So please don't switch the driver (e.g. during an upgrade) if you have existing volumes created using the old driver.
content/docs/archives/1.3.4/advanced-resources/os-distro-specific/csi-on-rke-and-coreos.md:29:> This step is only required for For CoreOS + and Kubernetes v1.11. It is not needed for Kubernetes v1.12+.
content/docs/archives/1.3.4/advanced-resources/security/mtls-support.md:53:> Note: Make sure to use `echo -n` when generating the base64 encoding,
content/docs/archives/1.3.4/advanced-resources/security/mtls-support.md:54:> otherwise a new line will be added at the end of the string
content/docs/archives/1.3.4/advanced-resources/security/mtls-support.md:55:> which will cause an error during loading of the certificates.
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks.md:24:> **Note:** If you choose to expand by [create additional volume](#create-additional-volume), the disks need to be manually added to the disk list of the nodes after the EKS cluster upgrade.
content/docs/archives/1.3.4/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks.md:10:> **Note:** If you have created [addition disks](../manage-node-group-on-eks#create-additional-volume) for Longhorn, you will need to manually add the path of the mounted disk into the disk list of the upgraded nodes.
content/docs/archives/1.3.4/best-practices.md:118:> Recommend: `false`
content/docs/archives/1.3.4/best-practices.md:123:> Recommend: `false`
content/docs/archives/1.3.4/concepts.md:14:> This section assumes familiarity with Kubernetes persistent storage concepts. For more information on these concepts, refer to the [appendix.](#appendix-how-persistent-storage-works-in-kubernetes) For help with the terminology used in this page, refer to [this section.](../terminology)
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:114:> GET / HTTP/1.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:115:> Host: 97.107.142.125
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:116:> User-Agent: curl/7.64.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:117:> Accept: */*
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:118:>
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:142:> GET / HTTP/1.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:143:> Host: 97.107.142.125
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:144:> Authorization: Basic Zm9vOmJhcg==
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:145:> User-Agent: curl/7.64.1
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:146:> Accept: */*
content/docs/archives/1.3.4/deploy/accessing-the-ui/longhorn-ingress.md:147:>
content/docs/archives/1.3.4/deploy/install/_index.md:7:> **Note**: This quick installation guide uses some configurations which are not for production usage.
content/docs/archives/1.3.4/deploy/install/_index.md:8:> Please see [Best Practices](../../best-practices/) for how to configure Longhorn for production usage.
content/docs/archives/1.3.4/deploy/uninstall/_index.md:61:> **Tip:** If you try `kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/v{{< current-version >}}/deploy/longhorn.yaml` first and get stuck there,
content/docs/archives/1.3.4/deploy/upgrade/longhorn-manager.md:30:> **Prerequisite:** Always back up volumes before upgrading. If anything goes wrong, you can restore the volume using the backup.
content/docs/archives/1.3.4/high-availability/k8s-cluster-autoscaler.md:20:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.4/monitoring/prometheus-and-grafana-setup.md:39:> **NOTE:** You may need to choose a release that is compatible with the Kubernetes version of the cluster.
content/docs/archives/1.3.4/references/networking.md:15:> Note: If you are writing network policies, please revisit this page before upgrading Longhorn to make the necessary adjustments to your network policies.
content/docs/archives/1.3.4/references/settings.md:70:> Default: `block-if-contains-last-replica`
content/docs/archives/1.3.4/references/settings.md:81:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:87:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:98:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:104:> Default: `0`
content/docs/archives/1.3.4/references/settings.md:112:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:126:> Default: `longhorn.io/v1beta1`
content/docs/archives/1.3.4/references/settings.md:132:> Default: `disabled`
content/docs/archives/1.3.4/references/settings.md:147:> Default: `/var/lib/longhorn/`
content/docs/archives/1.3.4/references/settings.md:165:> Default: `longhorn-static`
content/docs/archives/1.3.4/references/settings.md:171:> Default: `3`
content/docs/archives/1.3.4/references/settings.md:183:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:189:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:201:> Default: `do-nothing`
content/docs/archives/1.3.4/references/settings.md:216:> Default: `600`
content/docs/archives/1.3.4/references/settings.md:224:> Default: `if-not-present`
content/docs/archives/1.3.4/references/settings.md:240:> Default: `60`
content/docs/archives/1.3.4/references/settings.md:245:> Default: `300`
content/docs/archives/1.3.4/references/settings.md:248:> **Note:**
content/docs/archives/1.3.4/references/settings.md:249:>  - This recovery only works for the backing image of which the creation type is `download`.
content/docs/archives/1.3.4/references/settings.md:250:>  - File state `unknown` means the related manager pods on the pod is not running or the node itself is down/disconnected.
content/docs/archives/1.3.4/references/settings.md:255:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:263:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:271:> Example: `s3://backupbucket@us-east-1/backupstore`
content/docs/archives/1.3.4/references/settings.md:277:> Example: `s3-secret`
content/docs/archives/1.3.4/references/settings.md:283:> Default: `300`
content/docs/archives/1.3.4/references/settings.md:291:> Default: `1440`
content/docs/archives/1.3.4/references/settings.md:301:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:305:> **Note:** It's recommended to disable this setting when using Longhorn in the production environment. See [Best Practices](../../best-practices/) for details.
content/docs/archives/1.3.4/references/settings.md:309:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:317:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:325:> Default: `true`
content/docs/archives/1.3.4/references/settings.md:331:> **Note:**
content/docs/archives/1.3.4/references/settings.md:332:>   - Nodes that don't belong to any zone will be treated as if they belong to the same zone.
content/docs/archives/1.3.4/references/settings.md:333:>   - Longhorn relies on label `topology.kubernetes.io/zone=<Zone name of the node>` in the Kubernetes node object to identify the zone.
content/docs/archives/1.3.4/references/settings.md:337:> Default: `disabled`
content/docs/archives/1.3.4/references/settings.md:353:> Default: `ignored`
content/docs/archives/1.3.4/references/settings.md:367:> Default: `25`
content/docs/archives/1.3.4/references/settings.md:375:> Default: `200`
content/docs/archives/1.3.4/references/settings.md:387:> Default: `5`
content/docs/archives/1.3.4/references/settings.md:393:> **WARNING:**
content/docs/archives/1.3.4/references/settings.md:394:>  - The old setting "Disable Replica Rebuild" is replaced by this setting.
content/docs/archives/1.3.4/references/settings.md:395:>  - Different from relying on replica starting delay to limit the concurrent rebuilding, if the rebuilding is disabled, replica object replenishment will be directly skipped.
content/docs/archives/1.3.4/references/settings.md:396:>  - When the value is 0, the eviction and data locality feature won't work. But this shouldn't have any impact to any current replica rebuild and backup restore.
content/docs/archives/1.3.4/references/settings.md:400:> Default: `12`
content/docs/archives/1.3.4/references/settings.md:412:> **Warning:**
content/docs/archives/1.3.4/references/settings.md:413:>  - Value 0 means removing the CPU requests from spec of engine manager pods.
content/docs/archives/1.3.4/references/settings.md:414:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Replica Manager CPU' should not be greater than 40.
content/docs/archives/1.3.4/references/settings.md:415:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.4/references/settings.md:416:>  - This global setting will be ignored for a node if the field "EngineManagerCPURequest" on the node is set.
content/docs/archives/1.3.4/references/settings.md:417:>  - After this setting is changed, all engine manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.4/references/settings.md:421:> Default: `12`
content/docs/archives/1.3.4/references/settings.md:433:> **Warning:**
content/docs/archives/1.3.4/references/settings.md:434:>  - Value 0 means removing the CPU requests from specs of replica manager pods.
content/docs/archives/1.3.4/references/settings.md:435:>  - Considering the possible number of new instance manager pods in a further system upgrade, this integer value ranges from 0 to 40. And the total combined with the setting 'Guaranteed Engine Manager CPU' should not be greater than 40.
content/docs/archives/1.3.4/references/settings.md:436:>  - One more set of instance manager pods may need to be deployed when the Longhorn system is upgraded. If current available CPUs of the nodes are not enough for the new instance manager pods, you need to detach the volumes using the oldest instance manager pods so that Longhorn can clean up the old pods automatically and release the CPU resources. And the new pods with the latest instance manager image will be launched then.
content/docs/archives/1.3.4/references/settings.md:437:>  - This global setting will be ignored for a node if the field "ReplicaManagerCPURequest" on the node is set.
content/docs/archives/1.3.4/references/settings.md:438:>  - After this setting is changed, all replica manager pods using this global setting on all the nodes will be automatically restarted. In other words, DO NOT CHANGE THIS SETTING WITH ATTACHED VOLUMES.
content/docs/archives/1.3.4/references/settings.md:443:> Example: `nodetype=storage:NoSchedule`
content/docs/archives/1.3.4/references/settings.md:462:> Example: `high-priority`
content/docs/archives/1.3.4/references/settings.md:471:> **Warning:** This setting should only be changed after detaching all Longhorn volumes, as the Longhorn system components will be restarted to apply the setting. The Priority Class update will take a while, and users cannot operate Longhorn system during the update. Hence, it's recommended to set the Priority Class during Longhorn deployment.
content/docs/archives/1.3.4/references/settings.md:477:> Example: `label-key1:label-value1;label-key2:label-value2`
content/docs/archives/1.3.4/references/settings.md:484:> **Warning:**  Since all Longhorn components will be restarted, the Longhorn system is unavailable temporarily.
content/docs/archives/1.3.4/references/settings.md:490:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:496:> **Warning:** Replica rebuilding could be expensive because nodes with reusable replicas could get removed by the Kubernetes Cluster Autoscaler.
content/docs/archives/1.3.4/references/settings.md:500:> Example: `kube-system/demo-192-168-0-0`
content/docs/archives/1.3.4/references/settings.md:504:> **Warning:** This setting should change after detaching all Longhorn volumes, as some of the Longhorn system component pods will get recreated to apply the setting. Longhorn will try to block this setting update when there are attached volumes.
content/docs/archives/1.3.4/references/settings.md:513:> Default:
content/docs/archives/1.3.4/references/settings.md:519:> Default: `false`
content/docs/archives/1.3.4/references/settings.md:525:> Default: `false`
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:6:> Before v1.2.0, Longhorn uses a blocking way for communication with the remote backup target, so there will be some potential voluntary or involuntary factors (ex: network latency) impacting the functions relying on remote backup target like listing backups or even causing further cascading problems after the backup target operation.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:8:> Since v1.2.0, Longhorn starts using an asynchronous way to do backup operations to resolve the abovementioned issues in the previous versions.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:9:> - Create backup cluster custom resources first, and then perform the following snapshot and backup operations to the remote backup target.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:10:> - Once the backup creation is completed, asynchronously pull the state of backup volumes and backups from the remote backup target, then update the status of the corresponding cluster custom resources.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:11:>
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:12:> Besides, this enhancement is scalable for the backup query to solve the costly resources (even query timeout) caused by the original blocking way because all backups are saved as custom resources instead of querying from the remote target directly.
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:13:>
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/_index.md:14:> Please note that: after the Longhorn upgrade, if a volume hasn’t been upgraded to the latest longhorn engine (>=v1.2.0). When creating a backup, it will have the intermediate transition state of the name of the created backup (due to the different backup name handling in the latest longhorn version >= v1.2.0). However, in the end, Longhorn will ensure the backup is synced with the remote backup target and the backup will be updated to the final correct state as the remote backup target is the single source of truth. To upgrade the Longhorn engine, please refer to [Manually Upgrade Longhorn Engine](../../deploy/upgrade/upgrade-engine) or [Automatically Upgrade Longhorn Engine](../../deploy/upgrade/auto-upgrade-engine).
content/docs/archives/1.3.4/snapshots-and-backups/backup-and-restore/create-a-backup.md:8:> **Prerequisite:** A backup target must be set up. For more information, see [Set the BackupTarget](../set-backup-target). If the BackupTarget has not been set, you'll be presented with an error.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-backup.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:10:> **Prerequisite:** CSI snapshot support needs to be enabled on your cluster.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:11:> If your kubernetes distribution does not provide the kubernetes snapshot controller
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:12:> as well as the snapshot related custom resource definitions, you need to manually deploy them.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/csi-volume-snapshot-associated-with-longhorn-snapshot.md:13:> For more information, see [Enable CSI Snapshot Support](../enable-csi-snapshot-support).
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:7:> **Prerequisite**
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:8:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:9:> It is the responsibility of the Kubernetes distribution to deploy the snapshot controller as well as the related custom resource definitions.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:10:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:11:> For more information, see [CSI Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/).
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:19:> **Prerequisite**
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:20:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:21:> Please install the same release version of snapshot CRDs and snapshot controller to ensure that the CRD version is compatible with the snapshot controller.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:22:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:23:> For general use, update the snapshot controller YAMLs with an appropriate **namespace** prior to installing.
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:24:>
content/docs/archives/1.3.4/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support.md:25:> For example, on a vanilla Kubernetes cluster, update the namespace from `default` to `kube-system` prior to issuing the `kubectl create` command.
content/docs/archives/1.3.4/snapshots-and-backups/csi-volume-clone.md:40:> Note:
content/docs/archives/1.3.4/snapshots-and-backups/csi-volume-clone.md:41:> In addition to the requirements listed at [CSI volume cloning](https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/),
content/docs/archives/1.3.4/snapshots-and-backups/csi-volume-clone.md:42:> the `cloned-pvc` must have the same `resources.requests.storage` as the `source-pvc`.
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:33:> Note: To avoid the problem that recurring jobs may overwrite the old backups/snapshots with identical backups and empty snapshots when the volume doesn't have new data for a long time, Longhorn does the following:
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:34:> 1. Recurring backup job only takes a new backup when the volume has new data since the last backup.
content/docs/archives/1.3.4/snapshots-and-backups/scheduling-backups-and-snapshots.md:35:> 1. Recurring snapshot job only takes a new snapshot when the volume has new data in the volume head (the live data).
content/docs/archives/1.3.4/snapshots-and-backups/setup-disaster-recovery-volumes.md:17:> **Prerequisites:** Set up two Kubernetes clusters. These will be called cluster A and cluster B. Install Longhorn on both clusters, and set the same backup target on both clusters. For help setting the backup target, refer to [this page.](../backup-and-restore/set-backup-target)
content/docs/archives/1.3.4/volumes-and-nodes/create-volumes.md:8:> This section assumes that you understand how Kubernetes persistent storage works. For more information, see the [Kubernetes documentation.](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
content/docs/archives/1.3.4/volumes-and-nodes/delete-volumes.md:8:> **Note:** This method only works if the volume was provisioned by a StorageClass and the PersistentVolume for the Longhorn volume has its Reclaim Policy set to Delete.
content/docs/archives/1.3.4/volumes-and-nodes/delete-volumes.md:19:> **Note:** If Longhorn detects that a volume is tied to a PersistentVolume or PersistentVolumeClaim, then these resources will also be deleted once you delete the volume. You will be warned in the UI about this before proceeding with deletion. Longhorn will also warn you when deleting an attached volume, since it may be in use.
content/docs/archives/1.3.4/volumes-and-nodes/disks-or-nodes-eviction.md:8:> **Note:** This eviction feature can only be enabled when the selected disks or nodes have scheduling disabled. And during the eviction time, the selected disks or nodes cannot be re-enabled for scheduling.
content/docs/archives/1.3.4/volumes-and-nodes/disks-or-nodes-eviction.md:10:> **Note:** This eviction feature works for volumes that are `Attached` and `Detached`. If the volume is 'Detached', Longhorn will automatically attach it before the eviction and automatically detach it once eviction is done.
content/docs/archives/1.3.4/volumes-and-nodes/multidisk.md:24:> **Note**:
content/docs/archives/1.3.4/volumes-and-nodes/multidisk.md:25:> Since Longhorn uses filesystem ID to detect duplicate mounts of the same filesystem, you cannot add a disk that has the same filesystem ID as an existing disk on the same node.
content/docs/archives/1.3.4/volumes-and-nodes/multidisk.md:26:> See more details at https://github.com/longhorn/longhorn/issues/2477
content/docs/archives/1.3.4/volumes-and-nodes/volume-size.md:26:> The illustration presents the file organization of **one replica**. The volume head and snapshots are actually sparse files, which we mentioned above.
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:19:> For every CronJob, the CronJob Controller checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:20:>
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:21:>```
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:22:>Cannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
content/kb/troubleshooting-recurring-job-stuck-after-detach-attach.md:23:>```
content/kb/troubleshooting-volume-with-multipath.md:101:> The default configurations for multipath blacklist section is preventing the following device names by default
content/kb/troubleshooting-volume-with-multipath.md:102:> ^(ram|raw|loop|fd|md|dm-|sr|scd|st|dcssblk)[0-9]
content/kb/troubleshooting-volume-with-multipath.md:103:> ^(td|hd|vd)[a-z]
content/kb/troubleshooting-volume-filesystem-corruption.md:44:> **Warning**
content/kb/troubleshooting-volume-filesystem-corruption.md:45:> When a file system check tool fixes errors, it modifies the filesystem metadata and brings the filesystem to a
content/kb/troubleshooting-volume-filesystem-corruption.md:83:> **Warning**
content/kb/troubleshooting-volume-filesystem-corruption.md:84:> When a file system check tool fixes errors, it modifies the filesystem metadata and brings the filesystem to a
